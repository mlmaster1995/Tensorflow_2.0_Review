{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow 2.0.0 Basics Review\n",
    "## - tf variables\n",
    "## - tf operations\n",
    "## - tf keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow GPU Version: 2.0.0\n",
      "Eager Execution is: True\n",
      "Keras Version: 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- check tensorflow and keras version\n",
    "'''\n",
    "print(f'Tensorflow GPU Version: {tf.__version__}')\n",
    "print(f'Eager Execution is: {tf.executing_eagerly()}')\n",
    "print(f'Keras Version: {tf.keras.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- check GPU \n",
    "'''\n",
    "var = tf.Variable([3,3])\n",
    "if tf.test.is_gpu_available():\n",
    "    print('Running on GPU')\n",
    "else:\n",
    "    print('Runing on CPU')\n",
    "\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf1:  <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.6>\n",
      "tf2:  <tf.Variable 'Variable:0' shape=(3, 3) dtype=int32, numpy=\n",
      "array([[0, 4, 5],\n",
      "       [4, 2, 7],\n",
      "       [7, 8, 9]], dtype=int32)>\n",
      "tf1 iwth Numpy: 5.599999904632568\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- Declare variables\n",
    "'''\n",
    "var = 56\n",
    "tf1 = tf.Variable(var, dtype=tf.float32)\n",
    "tf1.assign(5.6)\n",
    "tf2 = tf.Variable([[0,4,5],[4,2,7],[7,8,9]])\n",
    "print('tf1: ',tf1)\n",
    "print('tf2: ',tf2)\n",
    "print(f'tf1 iwth Numpy: {tf1.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(568, shape=(), dtype=int16)\n",
      "568\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- Declare Constants\n",
    "'''\n",
    "constantVar = tf.constant(568, dtype = tf.int16)\n",
    "print(constantVar)\n",
    "print(constantVar.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var shape: (3, 4)\n",
      "var1 shape: (2, 6)\n",
      "var2 shape: (1, 12)\n",
      "var3 shape: (4, 3)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- Reshape a tensor\n",
    "'''\n",
    "var = tf.Variable([[2,3,4,5],[5,2,9,0],[3,1,2,4]], dtype=tf.float32)\n",
    "print('var shape:',var.shape)\n",
    "var1 = tf.reshape(var,(2,6))\n",
    "print('var1 shape:',var1.shape)\n",
    "var2 = tf.reshape(var, (1,12))\n",
    "print('var2 shape:',var2.shape)\n",
    "var3 = tf.reshape(var, (4,3))\n",
    "print('var3 shape:',var3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var rank: tf.Tensor(3, shape=(), dtype=int32)\n",
      "var2 slice: tf.Tensor(\n",
      "[[[ 2.  3.  4.  5.]\n",
      "  [ 5.  2.  9.  0.]\n",
      "  [ 3.  1.  2.  4.]]\n",
      "\n",
      " [[ 0. 30. 40. 50.]\n",
      "  [ 5.  2.  9.  0.]\n",
      "  [ 3.  1.  2.  4.]]], shape=(2, 3, 4), dtype=float32)\n",
      "var2 rank: 3\n",
      "var2 size: 24\n",
      "var2 dtype: <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tensor dimention\n",
    "- tensor slice\n",
    "- tensor cast numpy\n",
    "- tensor slice\n",
    "- tensor dtype\n",
    "'''\n",
    "var = tf.Variable([[[2,3,4,5],[5,2,9,0],[3,1,2,4]],[[0,30,40,50],[5,2,9,0],[3,1,2,4]],[[12,13,14,15],[5,2,9,0],[3,1,2,4]]], dtype=tf.float32)\n",
    "print('var rank:',tf.rank(var))\n",
    "var2 = var[0:2]\n",
    "print('var2 slice:',var2)\n",
    "print('var2 rank:',tf.rank(var2).numpy())\n",
    "print('var2 size:',tf.size(var2).numpy())\n",
    "print('var2 dtype:', var2.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var tensor:\n",
      " tf.Tensor(\n",
      "[[[  4.   6.   8.  10.]\n",
      "  [ 10.   4.  18.   0.]\n",
      "  [  6.   2.   4.   8.]]\n",
      "\n",
      " [[  0.  60.  80. 100.]\n",
      "  [ 10.   4.  18.   0.]\n",
      "  [  6.   2.   4.   8.]]\n",
      "\n",
      " [[ 24.  26.  28.  30.]\n",
      "  [ 10.   4.  18.   0.]\n",
      "  [  6.   2.   4.   8.]]], shape=(3, 3, 4), dtype=float32)\n",
      "\n",
      "var numpy cast:\n",
      " [[[  4.   6.   8.  10.]\n",
      "  [ 10.   4.  18.   0.]\n",
      "  [  6.   2.   4.   8.]]\n",
      "\n",
      " [[  0.  60.  80. 100.]\n",
      "  [ 10.   4.  18.   0.]\n",
      "  [  6.   2.   4.   8.]]\n",
      "\n",
      " [[ 24.  26.  28.  30.]\n",
      "  [ 10.   4.  18.   0.]\n",
      "  [  6.   2.   4.   8.]]]\n",
      "\n",
      "var*4:\n",
      " tf.Tensor(\n",
      "[[[ 16.  24.  32.  40.]\n",
      "  [ 40.  16.  72.   0.]\n",
      "  [ 24.   8.  16.  32.]]\n",
      "\n",
      " [[  0. 240. 320. 400.]\n",
      "  [ 40.  16.  72.   0.]\n",
      "  [ 24.   8.  16.  32.]]\n",
      "\n",
      " [[ 96. 104. 112. 120.]\n",
      "  [ 40.  16.  72.   0.]\n",
      "  [ 24.   8.  16.  32.]]], shape=(3, 3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tensor element-wise primitive tensor operations\n",
    "- tensor broadcasting\n",
    "'''\n",
    "var1 = tf.Variable([[[2,3,4,5],[5,2,9,0],[3,1,2,4]],[[0,30,40,50],[5,2,9,0],[3,1,2,4]],[[12,13,14,15],[5,2,9,0],[3,1,2,4]]], dtype=tf.float32)\n",
    "var2 = tf.Variable([[[2,2,2,2],[2,2,2,2],[2,2,2,2]],[[2,2,2,2],[2,2,2,2],[2,2,2,2]],[[2,2,2,2],[2,2,2,2],[2,2,2,2]]], dtype=tf.float32)\n",
    "var = var1 * var2\n",
    "print('var tensor:\\n',var)\n",
    "print('\\nvar numpy cast:\\n',var.numpy())\n",
    "print('\\nvar*4:\\n',var*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var:\n",
      " tf.Tensor(\n",
      "[[14 23]\n",
      " [23 50]], shape=(2, 2), dtype=int32)\n",
      "\n",
      "var dtype: <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tensor multiplication\n",
    "- tensor constant\n",
    "- tensor constant cast\n",
    "'''\n",
    "var1 = tf.constant([[1,2,3],[4,5,3]])\n",
    "var2 = tf.constant([[1,2,3],[4,5,3]])\n",
    "var = tf.matmul(var1,tf.transpose(var2))\n",
    "print('var:\\n',var)\n",
    "var = tf.cast(var, dtype=tf.float32)\n",
    "print('\\nvar dtype:', var.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[1.0, 2.0, 4.0], [], [3.0, 4.0], [1.0]]>\n",
      "tf.Tensor([1. 2. 4.], shape=(3,), dtype=float32)\n",
      "tf.Tensor([1.], shape=(1,), dtype=float32)\n",
      "<tf.RaggedTensor [[1, 2, 4], [], [5, 6], [7], [8]]>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- ragged tensors by constant\n",
    "- ragged tensors by split_row\n",
    "'''\n",
    "ragged_tensor = tf.ragged.constant([[1,2,4],[],[3,4],[1]], dtype=tf.float32, name='ragged_tensor')\n",
    "print(ragged_tensor)\n",
    "print(ragged_tensor[0])\n",
    "print(ragged_tensor[3])\n",
    "ragged_tensor2 = tf.RaggedTensor.from_row_splits(values=[1,2,4,5,6,7,8], row_splits=[0,3,3,5,6,7])\n",
    "print(ragged_tensor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_t1:\n",
      " tf.Tensor(\n",
      "[[0.23302452]\n",
      " [0.33036613]\n",
      " [0.07002912]], shape=(3, 1), dtype=float32)\n",
      "_t2:\n",
      " tf.Tensor(\n",
      "[[-1.1006709 ]\n",
      " [-0.40500164]\n",
      " [-1.2100329 ]], shape=(3, 1), dtype=float32)\n",
      "_t:\n",
      " tf.Tensor(\n",
      "[[1.7787435 ]\n",
      " [0.54076576]\n",
      " [1.638559  ]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- calculate square differences\n",
    "'''\n",
    "t1 = tf.random.normal((3,1))\n",
    "t2 = tf.random.normal((3,1))\n",
    "t = tf.math.squared_difference(t1,t2,name='square_difference')\n",
    "print('_t1:\\n',t1)\n",
    "print('_t2:\\n',t2)\n",
    "print('_t:\\n',t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_var:\n",
      " tf.Tensor(\n",
      "[[8.844145   8.424911   5.7227373 ]\n",
      " [4.715581   8.562659   7.411667  ]\n",
      " [0.57933927 8.567917   4.0922523 ]], shape=(3, 3), dtype=float32)\n",
      "\n",
      "_cross_mean:\n",
      " tf.Tensor(6.324579, shape=(), dtype=float32)\n",
      "\n",
      "_x_mean:\n",
      " tf.Tensor([4.7130218 8.518496  5.7422185], shape=(3,), dtype=float32)\n",
      "\n",
      "_x_mean_same_dim:\n",
      " tf.Tensor([[4.7130218 8.518496  5.7422185]], shape=(1, 3), dtype=float32)\n",
      "\n",
      "_y_mean:\n",
      " tf.Tensor([7.6639304 6.8966355 4.4131694], shape=(3,), dtype=float32)\n",
      "\n",
      "_y_mean_same_dim:\n",
      " tf.Tensor(\n",
      "[[7.6639304]\n",
      " [6.8966355]\n",
      " [4.4131694]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- calculate tensor mean\n",
    "'''\n",
    "var = tf.constant(tf.random.uniform((3,3),minval=0, maxval= 10, dtype=tf.float32))\n",
    "cross_mean = tf.reduce_mean(var, axis=None)\n",
    "x_mean = tf.reduce_mean(var, axis=0)\n",
    "y_mean = tf.reduce_mean(var, axis=1)\n",
    "x_mean_dim = tf.reduce_mean(var, axis=0,keepdims=True)\n",
    "y_mean_dim = tf.reduce_mean(var, axis=1, keepdims=True)\n",
    "print('_var:\\n',var)\n",
    "print('\\n_cross_mean:\\n',cross_mean)\n",
    "print('\\n_x_mean:\\n',x_mean)\n",
    "print('\\n_x_mean_same_dim:\\n',x_mean_dim)\n",
    "print('\\n_y_mean:\\n',y_mean)\n",
    "print('\\n_y_mean_same_dim:\\n',y_mean_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_random_var1:\n",
      " tf.Tensor(\n",
      "[[ 0.43616885]\n",
      " [-1.9093795 ]\n",
      " [ 1.3789066 ]\n",
      " [-1.0405852 ]], shape=(4, 1), dtype=float32)\n",
      "_random_var2:\n",
      " tf.Tensor(\n",
      "[[ 4.6377807]\n",
      " [14.660629 ]\n",
      " [-6.065405 ]\n",
      " [ 7.9940577]], shape=(4, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tensors with random normal values\n",
    "'''\n",
    "tf.random.set_seed(2)\n",
    "var_random1 = tf.random.normal((4,1),mean=0,stddev=1)\n",
    "var_random2 = tf.random.normal((4,1),mean=5,stddev=10)\n",
    "print('_random_var1:\\n',var_random1)\n",
    "print('_random_var2:\\n',var_random2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_var1: tf.Tensor(\n",
      "[[3]\n",
      " [1]\n",
      " [3]\n",
      " [1]], shape=(4, 1), dtype=int32)\n",
      "_var2: tf.Tensor(\n",
      "[[7]\n",
      " [5]\n",
      " [7]\n",
      " [7]], shape=(4, 1), dtype=int32)\n",
      "_var_concat_x: tf.Tensor(\n",
      "[[3]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [7]\n",
      " [5]\n",
      " [7]\n",
      " [7]], shape=(8, 1), dtype=int32)\n",
      "_var_concat_y: tf.Tensor(\n",
      "[[3 7]\n",
      " [1 5]\n",
      " [3 7]\n",
      " [1 7]], shape=(4, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tensors with random uniform values\n",
    "'''\n",
    "tf.random.set_seed(2)\n",
    "var1 = tf.random.uniform((4,1),minval=1, maxval=4, dtype=tf.int32)\n",
    "var2 = tf.random.uniform((4,1),minval=5, maxval=8, dtype=tf.int32)\n",
    "var_concat_x = tf.concat(values=[var1,var2],axis=0)\n",
    "var_concat_y = tf.concat(values=[var1,var2],axis=1)\n",
    "print('_var1:',var1)\n",
    "print('_var2:',var2)\n",
    "print('_var_concat_x:',var_concat_x)\n",
    "print('_var_concat_y:',var_concat_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_var:\n",
      " tf.Tensor(\n",
      "[[ 4  7  1  3  4  8  8  5  2  0]\n",
      " [ 6  4  0  9  4  3 -2  2  5 -1]\n",
      " [ 3  4  8  0 -1  5  0  8  4  7]\n",
      " [ 8  2  9  7  0  5  7  8  1  2]\n",
      " [ 3  1 -1  9 -2  5  2  7 -2  0]\n",
      " [ 3  8 -1 -2  8  1  5  7  5  6]\n",
      " [ 0  9 -2  3  8  3  4  0  4  0]\n",
      " [ 0  9  2  3  8  1  1  3  1  4]\n",
      " [-2  5  4  6 -2 -1  0  2  0  7]\n",
      " [-2  1  8  6 -2  4  3  6  4  0]], shape=(10, 10), dtype=int32)\n",
      "_max_index_x:\n",
      " [3 6 3 1 5 0 0 2 1 2]\n",
      "_min_index_y:\n",
      " [9 6 4 4 4 3 2 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- indexing tensor values\n",
    "- defualt axis=None, it's across cols to search max or min\n",
    "'''\n",
    "tf.random.set_seed(1)\n",
    "var = tf.constant(tf.random.uniform((10,10), minval=-2, maxval=10, dtype=tf.int32))\n",
    "_max_index_x = tf.argmax(input = var, axis=0, output_type=tf.int32)\n",
    "_min_index_y = tf.argmin(input = var, axis=1, output_type=tf.int32)\n",
    "print('_var:\\n', var)\n",
    "print('_max_index_x:\\n',_max_index_x.numpy())\n",
    "print('_min_index_y:\\n',_min_index_y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7ff7aaa56d10>"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "-?????????????????????//\n",
    "- Checkpoint to restore and save tensors\n",
    "- Checkpoint's constructor accepts keyword arguments whose values are types that contain trackable state, such as \n",
    "  ~`tf.keras.optimizers.Optimizer` implementations, \n",
    "  ~`tf.Variable`, \n",
    "  ~`tf.keras.Layer` implementations,\n",
    "  ~`tf.keras.Model` implementations. \n",
    "  It saves these values with a checkpoint and maintains a `save_counter` for numbering checkpoints\n",
    "'''\n",
    "var = tf.Variable([[1,2],[3,4]])\n",
    "checkpoint = tf.train.Checkpoint(var=var)\n",
    "path = checkpoint.save('./tf_ckpts/')\n",
    "var = var.assign([[3,3,],[5,5]])\n",
    "checkpoint.restore(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(23, shape=(), dtype=int32)\n",
      "tf.Tensor(22, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tf.function(\n",
    "    func=None, input_signature=None, autograph=True, experimental_implements=None,\n",
    "    experimental_autograph_options=None, experimental_relax_shapes=False,\n",
    "    experimental_compile=None\n",
    ")\n",
    "\n",
    "'''\n",
    "def calc(x,y):\n",
    "    return x**2*5+y\n",
    "f1 = tf.function(test)\n",
    "print(f1(2,3))\n",
    "\n",
    "@tf.function\n",
    "def calc_2(x,y):\n",
    "    return x*6+y\n",
    "print(calc_2(3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack_x: \n",
      " tf.Tensor(\n",
      "[[1 2 3 4]\n",
      " [1 2 3 4]\n",
      " [1 2 3 4]], shape=(3, 4), dtype=int32)\n",
      "\n",
      "stack_y: \n",
      " tf.Tensor(\n",
      "[[1 1 1]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [4 4 4]], shape=(4, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tf.stack\n",
    "- axis=0, take each item in the list and stack from top to bottom\n",
    "- axis=1, take values of each item in the list and make a row and stack rows from top to bottom \n",
    "'''\n",
    "var1 = tf.constant([1,2,3,4])\n",
    "var2 = tf.constant([1,2,3,4])\n",
    "var3 = tf.constant([1,2,3,4])\n",
    "stack_x = tf.stack([var1,var2,var3], axis=0)\n",
    "stack_y = tf.stack([var1,var2,var3], axis=1)\n",
    "print('stack_x: \\n', stack_x)\n",
    "print('\\nstack_y: \\n',stack_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf keras modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=4445, shape=(2, 2), dtype=float16, numpy=\n",
       "array([[1., 2.],\n",
       "       [3., 4.]], dtype=float16)>"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- declare a variable with backend\n",
    "- Keras is a model-level library, providing high-level building blocks for developing deep learning models. It does not handle low-level operations such as tensor products, convolutions and so on itself. \n",
    "- Instead, it relies on a specialized, well optimized tensor manipulation library to do so, serving as the \"backend engine\" of Keras. Rather than picking one single tensor library and making the \n",
    "implementation of Keras tied to that library, Keras handles the problem in a modular way, and several different backend engines can be plugged seamlessly into Keras.\n",
    "'''\n",
    "var = K.constant([[1,2],[3,4]],dtype=tf.float16)\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- load minist data\n",
    "'''\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_x, train_y),(test_x, test_y)=mnist.load_data()\n",
    "train_x = train_x.astype('float32')/255\n",
    "test_x =test_x.astype('float32')/255\n",
    "train_x = train_x.reshape(train_x.shape[0],28,28,1)\n",
    "test_x = test_x.reshape(test_x.shape[0],28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 6, 6, 64)          18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 87,562\n",
      "Trainable params: 87,370\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tf.keras Functional API\n",
    "'''\n",
    "ipt = tf.keras.Input(shape=(28,28,1))\n",
    "opt = tf.keras.layers.Conv2D(32,3)(ipt)\n",
    "opt = tf.keras.layers.LeakyReLU()(opt)\n",
    "opt = tf.keras.layers.BatchNormalization()(opt)\n",
    "opt = tf.keras.layers.MaxPool2D((3,3))(opt)\n",
    "\n",
    "opt = tf.keras.layers.Conv2D(64,3)(opt)\n",
    "opt = tf.keras.layers.LeakyReLU()(opt)\n",
    "opt = tf.keras.layers.BatchNormalization()(opt)\n",
    "opt = tf.keras.layers.MaxPool2D((3,3))(opt)\n",
    "\n",
    "opt = tf.keras.layers.Flatten()(opt)\n",
    "opt = tf.keras.layers.Dense(256, activation='relu')(opt)\n",
    "opt = tf.keras.layers.Dense(10, activation='softmax')(opt)\n",
    "\n",
    "model = tf.keras.models.Model(ipt, opt)\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=['acc']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.1646 - acc: 0.9532 - val_loss: 0.2478 - val_acc: 0.9220\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.0489 - acc: 0.9844 - val_loss: 0.0399 - val_acc: 0.9858\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0335 - acc: 0.9891 - val_loss: 0.0390 - val_acc: 0.9874\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0295 - acc: 0.9905 - val_loss: 0.0350 - val_acc: 0.9889\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0227 - acc: 0.9929 - val_loss: 0.0549 - val_acc: 0.9819\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0202 - acc: 0.9931 - val_loss: 0.0441 - val_acc: 0.9860\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.0162 - acc: 0.9948 - val_loss: 0.0338 - val_acc: 0.9894\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0144 - acc: 0.9952 - val_loss: 0.0409 - val_acc: 0.9887\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.0131 - acc: 0.9958 - val_loss: 0.0397 - val_acc: 0.9893\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0121 - acc: 0.9959 - val_loss: 0.0354 - val_acc: 0.9905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f67f063e350>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_x, \n",
    "    train_y,\n",
    "    batch_size = 128,\n",
    "    epochs = 10,\n",
    "    validation_data=(test_x,test_y),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- tf.keras.Model class\n",
    "- tf.keras.callbacks.Callback class\n",
    "- tf.keras.callbacks.EarlyStopping class\n",
    "'''\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32,(3,3), padding='same')\n",
    "        self.act1 = tf.keras.layers.LeakyReLU()\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.pool1 = tf.keras.layers.MaxPooling2D((3,3))\n",
    "        \n",
    "        self.conv2 = tf.keras.layers.Conv2D(32,(3,3), padding='same')\n",
    "        self.act2 = tf.keras.layers.LeakyReLU()\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.pool2 = tf.keras.layers.MaxPooling2D((3,3))\n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(512, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.act1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "    \n",
    "class customCallback(tf.keras.callbacks.Callback):\n",
    "    # constructor\n",
    "    def __init__(self):\n",
    "        # parent constructor\n",
    "        super(customCallback,self).__init__()\n",
    "        \n",
    "    # call at the training end    \n",
    "    def on_epoch_end(self, batch, logs=None):\n",
    "        print('\\n train_acc-val_acc:',logs['acc']-logs['val_acc'])\n",
    "        print(self.params)\n",
    "\n",
    "'''\n",
    "ValAccEarlyStopping Class:\n",
    "- val_acc_base is to define the expected val_acc at the end of each epoch traning\n",
    "- if val_acc >= val_acc_base, model will stop training\n",
    "- if early stopping is not triggered by the end of training, the model with best \n",
    "val_acc will be restored\n",
    "'''\n",
    "class ValAccEarlyStopping(tf.keras.callbacks.EarlyStopping):\n",
    "    # constructor\n",
    "    def __init__(self, val_acc_base):\n",
    "        # parent constructor\n",
    "        super(ValAccEarlyStopping,self).__init__(monitor='val_acc', verbose=1, baseline=val_acc_base, restore_best_weights=True)\n",
    "        # fields\n",
    "        self.__best_weights=None\n",
    "        self.__bestWeightEpoch=None\n",
    "        self.__weights =[]\n",
    "        self.__val_acc=[]\n",
    "        \n",
    "    # early stopping method\n",
    "    def on_epoch_end(self,epoch,logs=None): \n",
    "        # restore best model weights\n",
    "        if self.restore_best_weights:\n",
    "            # save weights & val_acc for each epoch\n",
    "            self.__weights.append(self.model.get_weights())\n",
    "            self.__val_acc.append(logs['val_acc'])\n",
    "            \n",
    "            # update the best weights\n",
    "            self.__bestWeightEpoch = self.__val_acc.index(max(self.__val_acc))\n",
    "            self.__best_weights = self.__weights[self.__bestWeightEpoch]           \n",
    "        \n",
    "        # early stopping check\n",
    "        if logs[self.monitor]>=self.baseline:\n",
    "            self.model.stop_training = True\n",
    "            self.stopped_epoch = epoch+1\n",
    "        \n",
    "    # update early stopping training end method         \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0 and self.verbose > 0:\n",
    "            print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))\n",
    "        else:\n",
    "            self.model.set_weights(self.__best_weights)\n",
    "            print(f'Early stopping is not triggered, but best model is restored at epoch {self.__bestWeightEpoch+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.1619 - acc: 0.9514 - val_loss: 0.1679 - val_acc: 0.9587\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.0498 - acc: 0.9843 - val_loss: 0.0428 - val_acc: 0.9859\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6614d91290>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- build model \n",
    "- compile model\n",
    "- train model with custom early stopping class\n",
    "'''\n",
    "model = MyModel()\n",
    "model.compile(\n",
    "    optimizer= tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics= ['acc'],\n",
    ")\n",
    "model.fit(\n",
    "    train_x, \n",
    "    train_y,\n",
    "    batch_size = 128,\n",
    "    epochs = 10,\n",
    "    validation_data = (test_x,test_y),\n",
    "    callbacks = [ValAccEarlyStopping(val_acc_base=0.98)],\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf data pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (60000, 28, 28, 1) , x_train data type:  float32\n",
      "x_test shape:  (10000, 28, 28, 1) , x_test data type:  float32\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "- load fashin_minist dataset\n",
    "- 10 classes 0-9\n",
    "'''\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0],28,28,1).astype('float32')/255\n",
    "x_test = x_test.reshape(x_test.shape[0],28,28,1).astype('float32')/255\n",
    "print('x_train shape: ',x_train.shape,', x_train data type: ', x_train.dtype)\n",
    "print('x_test shape: ',x_test.shape,', x_test data type: ',x_test.dtype)\n",
    " \n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(x_train[258].reshape((28,28)).astype('float32'))\n",
    "plt.title('fashion_mnist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- data pipeline from Numpy Arrays\n",
    "- tf.data.Dataset.from_tensor_slices\n",
    "'''\n",
    "batch_size = 128\n",
    "shuffle_buffer_size = 10000\n",
    "tf.random.set_seed(5)\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(batch_size).shuffle(shuffle_buffer_size)\n",
    "train_data = train_data.repeat()\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test,y_test)).batch(batch_size).shuffle(shuffle_buffer_size)\n",
    "test_data = test_data.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (128, 28, 28, 1)\n",
      "y shape: (128,)\n",
      "x shape: (128, 28, 28, 1)\n",
      "y shape: (128,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- data pipline from Numpy with iterator\n",
    "- tf.data.Dataset.from_tensor_slices\n",
    "- tf.compat.v1.data.make_one_shot_iteraor\n",
    "'''\n",
    "batch_size = 128\n",
    "shuffle_buffer_size = 10000\n",
    "tf.random.set_seed(5)\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(batch_size).shuffle(shuffle_buffer_size)\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test,y_test)).batch(batch_size).shuffle(shuffle_buffer_size)\n",
    "train_iterator = tf.compat.v1.data.make_one_shot_iterator(train_data)\n",
    "test_iterator = tf.compat.v1.data.make_one_shot_iterator(test_data)\n",
    "\n",
    "tr_x,tr_y = train_iterator.get_next()\n",
    "print('x shape:',tr_x.shape)\n",
    "print('y shape:',tr_y.shape)\n",
    "te_x,te_y = test_iterator.get_next()\n",
    "print('x shape:',te_x.shape)\n",
    "print('y shape:',te_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- data pipline from CSV file \n",
    "- tf.data.experimental.make_csv_dataset()\n",
    "'''\n",
    "TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
    "TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n",
    "\n",
    "train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\n",
    "test_file_path = tf.keras.utils.get_file(\"eval.csv\", TEST_DATA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 627 entries, 0 to 626\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   survived            627 non-null    int64  \n",
      " 1   sex                 627 non-null    object \n",
      " 2   age                 627 non-null    float64\n",
      " 3   n_siblings_spouses  627 non-null    int64  \n",
      " 4   parch               627 non-null    int64  \n",
      " 5   fare                627 non-null    float64\n",
      " 6   class               627 non-null    object \n",
      " 7   deck                627 non-null    object \n",
      " 8   embark_town         627 non-null    object \n",
      " 9   alone               627 non-null    object \n",
      "dtypes: float64(2), int64(3), object(5)\n",
      "memory usage: 49.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train_dt = pd.read_csv(train_file_path)\n",
    "train_dt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 264 entries, 0 to 263\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   survived            264 non-null    int64  \n",
      " 1   sex                 264 non-null    object \n",
      " 2   age                 264 non-null    float64\n",
      " 3   n_siblings_spouses  264 non-null    int64  \n",
      " 4   parch               264 non-null    int64  \n",
      " 5   fare                264 non-null    float64\n",
      " 6   class               264 non-null    object \n",
      " 7   deck                264 non-null    object \n",
      " 8   embark_town         264 non-null    object \n",
      " 9   alone               264 non-null    object \n",
      "dtypes: float64(2), int64(3), object(5)\n",
      "memory usage: 20.8+ KB\n"
     ]
    }
   ],
   "source": [
    "test_dt = pd.read_csv(test_file_path)\n",
    "test_dt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMN = 'survived'\n",
    "LABELS = [0,1]\n",
    "\n",
    "def get_csv_data(file_path, batch=3, **kwargs):\n",
    "    csvData = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern = file_path,\n",
    "        batch_size = batch,\n",
    "        label_name = LABEL_COLUMN,\n",
    "        na_value = \"?\",\n",
    "        num_epochs= 1,\n",
    "        ignore_errors = True,\n",
    "        **kwargs\n",
    "    )\n",
    "    return csvData\n",
    "\n",
    "# load csv data\n",
    "train_data = get_csv_data(train_file_path)\n",
    "test_data = get_csv_data(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'male' b'female' b'female']\n",
      "age                 : [32. 28. 39.]\n",
      "n_siblings_spouses  : [0 1 1]\n",
      "parch               : [0 0 1]\n",
      "fare                : [ 56.4958 133.65    83.1583]\n",
      "class               : [b'Third' b'First' b'First']\n",
      "deck                : [b'unknown' b'unknown' b'E']\n",
      "embark_town         : [b'Southampton' b'Southampton' b'Cherbourg']\n",
      "alone               : [b'y' b'n' b'n']\n",
      "\n",
      "labels              : [1 1 1]\n"
     ]
    }
   ],
   "source": [
    "def show_batch(dataset):\n",
    "    '''\n",
    "    - take only one batch of data for the display -> dataset.take(1)\n",
    "    - two batches data will be taken -> dataset.take(2)\n",
    "    - {:20s} -> 20 spaces \n",
    "    ''' \n",
    "    for batch, label in dataset.take(1):\n",
    "        for key, value in batch.items():\n",
    "            print(\"{:20s}: {}\".format(key,value.numpy()))\n",
    "        print(\"\\n{:20s}: {}\".format('labels',label.numpy()))\n",
    "            \n",
    "show_batch(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_names example data: \n",
      "sex                 : [b'male' b'female' b'female']\n",
      "age                 : [24. 28. 45.]\n",
      "n_siblings_spouses  : [0 3 1]\n",
      "parch               : [0 1 1]\n",
      "fare                : [ 13.      25.4667 164.8667]\n",
      "class               : [b'Second' b'Third' b'First']\n",
      "deck                : [b'unknown' b'unknown' b'unknown']\n",
      "embark_town         : [b'Southampton' b'Southampton' b'Southampton']\n",
      "alone               : [b'y' b'n' b'n']\n",
      "\n",
      "labels              : [0 0 1]\n",
      "\n",
      "select_columns example data: \n",
      "age                 : [28. 40. 33.]\n",
      "parch               : [0 1 0]\n",
      "\n",
      "labels              : [0 1 0]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- column_names -> to name the columes if the dataset has no column names \n",
    "- select_columns -> to select which column of data will be extraced from csv\n",
    "'''\n",
    "temp_data = get_csv_data(train_file_path, column_names = ['survived', 'sex', 'age', 'n_siblings_spouses', 'parch', 'fare', 'class', 'deck', 'embark_town', 'alone'])\n",
    "print('column_names example data: ')\n",
    "show_batch(temp_data)\n",
    "\n",
    "temp_data = get_csv_data(train_file_path, select_columns = ['survived','age','parch'])\n",
    "print('\\nselect_columns example data: ')\n",
    "show_batch(temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                 : [ 3. 28. 29. 29. 28. 28. 22. 22. 45. 18.]\n",
      "n_siblings_spouses  : [4. 1. 1. 1. 0. 0. 0. 0. 1. 0.]\n",
      "parch               : [2. 2. 1. 0. 0. 0. 0. 1. 1. 1.]\n",
      "fare                : [ 31.3875  23.45    10.4625  26.       7.75     8.05     7.2292  55.\n",
      " 164.8667  23.    ]\n",
      "\n",
      "labels              : [1 0 0 1 1 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- load selected column data in one batch size\n",
    "'''\n",
    "SELECT_COLUMNS = ['survived', 'age', 'n_siblings_spouses', 'parch', 'fare']\n",
    "DEFAULTS = [0, 0.0, 0.0, 0.0, 0.0]\n",
    "temp_dataset = get_csv_data(train_file_path, batch=10, select_columns=SELECT_COLUMNS, column_defaults = DEFAULTS)\n",
    "\n",
    "show_batch(temp_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one batch data and labels:\n",
      "age            : [32. 28. 21. 18. 18. 29.  4.  1. 40. 18.]\n",
      "n_siblings_spouses: [0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "parch          : [0. 0. 0. 0. 0. 0. 2. 2. 0. 1.]\n",
      "fare           : [ 8.3625  7.8792 77.9583  7.775  13.      9.5    22.025  20.575  31.\n",
      "  7.8542]\n",
      "labels              : [0 1 1 0 0 1 1 1 1 0]\n",
      "\n",
      "Stack data rows:\n",
      "[[32.     28.     21.     18.     18.     29.      4.      1.     40.\n",
      "  18.    ]\n",
      " [ 0.      0.      0.      0.      0.      0.      0.      1.      0.\n",
      "   1.    ]\n",
      " [ 0.      0.      0.      0.      0.      0.      2.      2.      0.\n",
      "   1.    ]\n",
      " [ 8.3625  7.8792 77.9583  7.775  13.      9.5    22.025  20.575  31.\n",
      "   7.8542]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- take one example batch\n",
    "- The Python iter() function returns an iterator for the given object.\n",
    "- iter(temp_datset) returns a tensorflow iterator\n",
    "- example_batch -> tensor dictionary\n",
    "- labels_batch -> tensor\n",
    "'''\n",
    "features, labels = next(iter(temp_dataset)) \n",
    "print('one batch data and labels:')\n",
    "for key,values in features.items():\n",
    "     print(\"{:15s}: {}\".format(key,values.numpy()))\n",
    "print(\"{:20s}: {}\".format('labels',labels.numpy()))\n",
    "\n",
    "'''\n",
    "- stack rows from top to bottom to make a matrix \n",
    "'''\n",
    "print('\\nStack data rows:')\n",
    "print(tf.stack(list(features.values()),axis=0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 48.      22.      35.      26.      28.      31.      17.      40.\n",
      "   30.      19.    ]\n",
      " [  1.       0.       0.       0.       1.       1.       0.       0.\n",
      "    0.       0.    ]\n",
      " [  0.       0.       0.       0.       0.       0.       0.       0.\n",
      "    0.       2.    ]\n",
      " [ 39.6      9.35     7.05    56.4958  15.5    113.275    8.6625  31.\n",
      "   10.5     26.2833]]\n",
      "\n",
      "[1 0 0 1 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- define pack function \n",
    "- take features and labels in one batch of data\n",
    "- return a stacked matrix and label pair\n",
    "'''\n",
    "def pack_rows (features, labels):\n",
    "    return tf.stack(list(features.values()),axis=0),labels\n",
    "\n",
    "packed_dataset = temp_dataset.map(pack_rows)\n",
    "\n",
    "for features, labels in packed_dataset.take(1):\n",
    "    print(features.numpy())\n",
    "    print()\n",
    "    print(labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- put all together into one class for the data pipeline process\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- categorical data processing\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=8413, shape=(60000, 10), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- one hot encoding\n",
    "'''\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "y_train = tf.one_hot(y_train, depth=10)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.keras.layers.Layer and activation functions\n",
    "## - tf.keras.layers.Layer custom definition (https://www.tensorflow.org/guide/keras/custom_layers_and_models#layers_encapsulate_a_state_weights_and_some_computation) use row matrix multiplication;\n",
    "## - row[...] * mat [...] = row[...]\n",
    "## - weight matrix shape is input_shape * units and this cannot be changed in the custom layer as it has to be consistentcy for further calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial weights:  tf.Tensor(\n",
      "[[ 0.40749726  0.02219561 -2.200683   -0.8212301 ]\n",
      " [-0.36420798 -2.177051   -0.8709116  -0.40788257]\n",
      " [ 0.37394255 -0.49891925 -0.00245201 -0.14541708]\n",
      " [ 0.56160647 -0.88500816 -0.12318895  0.7797051 ]], shape=(4, 4), dtype=float32)\n",
      "initial zeros:  tf.Tensor(\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]], shape=(4, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tf.random_normal_initializer\n",
    "- tf.zeros_initializer\n",
    "'''\n",
    "tf.random.set_seed(15)\n",
    "input_dim = (4,4)\n",
    "init = tf.random_normal_initializer(mean=0.0, stddev=1)\n",
    "init_zeros = tf.zeros_initializer()\n",
    "print('initial weights: ',init(shape=input_dim, dtype=tf.float32))\n",
    "print('initial zeros: ', init_zeros(shape=input_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- single linear combination layer with self-defined weights, units -> output channels, input_shape -> input shape\n",
    "- computation is max[...]*col[...] = col[...]\n",
    "- **kwargs-> allowed_kwargs = {\n",
    "                                'input_shape',\n",
    "                                'batch_input_shape',\n",
    "                                'batch_size',\n",
    "                                'weights',\n",
    "                                'activity_regularizer',\n",
    "                                'autocast'\n",
    "                            }\n",
    "- \n",
    "'''\n",
    "class customLayer(tf.keras.layers.Layer):\n",
    "    #constructor\n",
    "    def __init__(self, units = 3, input_shape=5, trainable=True, name=None, dtype=tf.float32, **kwargs):\n",
    "        # parent constructor\n",
    "        super(customLayer, self).__init__(name=name, dtype= dtype, trainable=trainable, **kwargs)\n",
    "        # fields\n",
    "        init_w = tf.random_normal_initializer(mean=0, stddev=1)\n",
    "        init_b = tf.zeros_initializer()\n",
    "        self.w = tf.Variable(init_w(shape=(units, input_shape),dtype=dtype), trainable=trainable)\n",
    "        self.b = tf.Variable(init_b(shape=(units,)),dtype=dtype, trainable=trainable)       \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        if tf.rank(inputs)==1:\n",
    "            inputs = tf.keras.backend.expand_dims(inputs, axis=1)\n",
    "        return tf.matmul(self.w, inputs)+self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape is at 4 * 1: \n",
      "[1. 1. 1. 1.]\n",
      "\n",
      "layer1 trainable weights is at 5*4: \n",
      "  [[ 2.1618998   0.07567143  1.8947576   1.5536673 ]\n",
      " [-1.2869931  -0.81316155  0.6367824  -1.7842115 ]\n",
      " [ 0.32601485 -0.3001915  -0.73505306  0.06373952]\n",
      " [-0.6815776  -0.5778277   1.0000818   0.6779952 ]\n",
      " [ 0.38209513 -0.7653013  -0.3519354   0.03938751]]\n",
      "\n",
      "output channels is at 5*1: \n",
      " [[ 5.685996    5.685996    5.685996    5.685996    5.685996  ]\n",
      " [-3.2475839  -3.2475839  -3.2475839  -3.2475839  -3.2475839 ]\n",
      " [-0.64549017 -0.64549017 -0.64549017 -0.64549017 -0.64549017]\n",
      " [ 0.4186716   0.4186716   0.4186716   0.4186716   0.4186716 ]\n",
      " [-0.695754   -0.695754   -0.695754   -0.695754   -0.695754  ]]\n",
      "\n",
      "layer1 trainable bias: \n",
      " [0. 0. 0. 0. 0.]\n",
      "\n",
      "layer1 non-trainable weights:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "input_shape=4\n",
    "units= 5\n",
    "ipt = tf.ones((input_shape))\n",
    "layer1 = customLayer(units=units, input_shape=input_shape, name='test')\n",
    "print(f'input shape is at {input_shape} * 1: \\n{ipt}')\n",
    "print(f'\\nlayer1 trainable weights is at {units}*{input_shape}: \\n  {layer1.weights[0].numpy()}',)\n",
    "print(f'\\noutput channels is at {units}*1: \\n {layer1(ipt).numpy()}')\n",
    "print('\\nlayer1 trainable bias: \\n', layer1.weights[1].numpy())\n",
    "print('\\nlayer1 non-trainable weights:\\n', layer1.non_trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- Multi-layer Perceptron(MLP) layer\n",
    "- subclass of tf.keras.layers.Layer class\n",
    "- this layer has no batch size feature\n",
    "- this layer use build() to identify the input shape on the runtime\n",
    "- this layer use row multiplication caculation on weights and inputs\n",
    "- add_weight(\n",
    "        name=None, shape=None, dtype=None, initializer=None, regularizer=None,\n",
    "        trainable=None, constraint=None, partitioner=None, use_resource=None,\n",
    "        synchronization=tf.VariableSynchronization.AUTO,\n",
    "        aggregation=tf.compat.v1.VariableAggregation.NONE, **kwargs\n",
    "    )\n",
    "- **kwargs: `getter`, 'collections`, `experimental_autocast` and `caching_device`.\n",
    "\n",
    "'''\n",
    "class MLP(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=3, activation=None, trainable=True, name=None, dtype=tf.float32, **kwargs):\n",
    "        super(MLP, self).__init__( name=name, trainable = trainable, dtype=dtype, **kwargs)\n",
    "        self.units = units;\n",
    "        self.__activation_name = activation\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),initializer='random_normal')\n",
    "        self.b = self.add_weight(shape=(self.units,),initializer='random_normal')\n",
    "    \n",
    "    def get_config(self):\n",
    "        config_dic ={\n",
    "            'units':self.units,\n",
    "            'activation': self.__activation_name, \n",
    "            'trainable_weights & bias':self.trainable_weights,\n",
    "            'non-trainable_weights & bias':self.non_trainable_weights,\n",
    "        }\n",
    "        config = super(MLP,self).get_config()\n",
    "        config.update(config_dic)\n",
    "        return config\n",
    "    \n",
    "    def call(self, inputs, training = None):\n",
    "        if tf.rank(inputs)==1:\n",
    "            inputs = tf.keras.backend.expand_dims(inputs, axis=0)\n",
    "        linear_combination = tf.matmul(inputs, self.w)+self.b \n",
    "        return self.activation(linear_combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'mlp_single_layer',\n",
       " 'trainable': True,\n",
       " 'dtype': 'float32',\n",
       " 'units': 5,\n",
       " 'activation': None,\n",
       " 'trainable_weights & bias': [<tf.Variable 'mlp_single_layer/Variable:0' shape=(4, 5) dtype=float32, numpy=\n",
       "  array([[-0.04268041, -0.0240161 , -0.00559446,  0.00667279,  0.05542039],\n",
       "         [ 0.0149584 ,  0.04895762,  0.00866133, -0.03208294, -0.02934596],\n",
       "         [ 0.12082821, -0.01217135,  0.06881073, -0.04737529, -0.01472751],\n",
       "         [ 0.09234401, -0.05215368,  0.03285414, -0.01964135,  0.09214985]],\n",
       "        dtype=float32)>,\n",
       "  <tf.Variable 'mlp_single_layer/Variable:0' shape=(5,) dtype=float32, numpy=\n",
       "  array([ 0.06843295,  0.02584788,  0.0653073 , -0.0641969 , -0.0261974 ],\n",
       "        dtype=float32)>],\n",
       " 'non-trainable_weights & bias': []}"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- get_config() function\n",
    "- override get_config() in the tf.keras.layers.Layer\n",
    "- \"input_shape\" in kwards is to generate \"batch_input_shape\" and it should be avoided in the custom layer\n",
    "- instead, use build() to get the real input shape in the custom layer\n",
    "'''\n",
    "input_shape=4\n",
    "units= 5\n",
    "ipt = tf.ones((input_shape))\n",
    "mlp = MLP(units=units, activation=None, name='mlp_single_layer', trainable=True)\n",
    "opt = mlp(ipt)\n",
    "mlp.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape is at 4: \n",
      "[1. 1. 1. 1.]\n",
      "\n",
      "layer1 trainable weights is at 4*5: \n",
      "  [[-0.02856542  0.03515685 -0.06862309  0.07264601 -0.02461529]\n",
      " [-0.00546568 -0.04800047  0.15303433 -0.06815054  0.03404353]\n",
      " [-0.02024032  0.07377522  0.01931716  0.03163869  0.01544571]\n",
      " [-0.05890731 -0.13744293 -0.08588818 -0.01069717 -0.04543935]]\n",
      "\n",
      "layer1 trainable bias is at 5: \n",
      "  [ 0.06411647  0.01416687  0.05103198 -0.03926305 -0.04594219]\n",
      "\n",
      "output channels is at 5: \n",
      " [[-0.04906226 -0.06234445  0.06887219 -0.01382606 -0.06650759]]\n",
      "\n",
      "layer1 non-trainable weights:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- single MLP layer computation\n",
    "- without activation functions\n",
    "- with build() to acquire input_shape\n",
    "'''\n",
    "input_shape=4\n",
    "units= 5\n",
    "ipt = tf.ones((input_shape))\n",
    "mlp = MLP(units=units,activation=None, name='mlp_single_layer')\n",
    "opt = mlp(ipt)\n",
    "print(f'input shape is at {input_shape}: \\n{ipt}')\n",
    "print(f'\\nlayer1 trainable weights is at {input_shape}*{units}: \\n  {mlp.weights[0].numpy()}',)\n",
    "print(f'\\nlayer1 trainable bias is at {units}: \\n  {mlp.weights[1].numpy()}',)\n",
    "print(f'\\noutput channels is at {units}: \\n {mlp(ipt).numpy()}')\n",
    "print('\\nlayer1 non-trainable weights:\\n', mlp.non_trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: [ 0.40749726  0.02219561 -2.200683   -0.8212301  -0.36420798]\n",
      "\n",
      "final output:  [[0.00949545]]\n",
      "\n",
      "mlp1 layer weights and bias: \n",
      " [array([[-0.05752216, -0.01777615, -0.0443813 , -0.07142202,  0.03639229],\n",
      "       [ 0.05079982,  0.01979304, -0.01862826, -0.01179774,  0.03318491],\n",
      "       [-0.0077181 , -0.0696383 , -0.09101368, -0.00102651,  0.03367503],\n",
      "       [-0.07211521,  0.02843316, -0.05534288, -0.08601289,  0.10155316],\n",
      "       [-0.00349029, -0.05710344,  0.02195667,  0.02711194, -0.05647339]],\n",
      "      dtype=float32), array([ 0.01557087,  0.06733529, -0.02465851, -0.09773596,  0.00385595],\n",
      "      dtype=float32)]\n",
      "\n",
      "mlp2 layer weights and bias: \n",
      " [array([[-0.02640516],\n",
      "       [ 0.02764253],\n",
      "       [-0.0596687 ],\n",
      "       [-0.02553965],\n",
      "       [ 0.04818435]], dtype=float32), array([0.02116098], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- multiple MLP layers computation\n",
    "- without activation functions\n",
    "- with input_shape acquired by build() function\n",
    "'''\n",
    "tf.random.set_seed(15)\n",
    "input_shape=5\n",
    "ipt = tf.random.normal(shape=(input_shape,))\n",
    "mlp1 = MLP(units=5, activation= None, name='mlp_layer1')\n",
    "mlp2 = MLP(units=1, activation=None,  name='mlp_layer2')\n",
    "opt = mlp1(ipt)\n",
    "opt = mlp2(opt)\n",
    "print('input:', ipt.numpy())\n",
    "print('\\nfinal output: ', opt.numpy())\n",
    "print('\\nmlp1 layer weights and bias: \\n',[val.numpy() for val in mlp1.weights])\n",
    "print('\\nmlp2 layer weights and bias: \\n',[val.numpy() for val in mlp2.weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: \n",
      " [[0.08908382 0.09991569 0.10659099 0.10593536 0.09802915 0.08910162\n",
      "  0.09953129 0.11073688 0.09931216 0.10176301]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- multiple MLP with 'sigmoid' activation function\n",
    "'''\n",
    "tf.random.set_seed(15)\n",
    "input_shape=5\n",
    "ipt = tf.random.normal(shape=(input_shape,))\n",
    "opt = MLP(units = 256, activation = 'relu')(ipt)\n",
    "opt = MLP(units = 128, activation = 'relu')(ipt)\n",
    "opt = MLP(units =10, activation= 'softmax')(opt)\n",
    "print('output: \\n',opt.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input is: \n",
      " [[ 0.19024122 -0.4606145  -0.13381146 -2.562121   -0.72492903  0.613187\n",
      "  -0.15034355 -1.5807154   0.6264738   0.9457944 ]]\n",
      "\n",
      "default linear activation output: \n",
      " [[ 0.19024122 -0.4606145  -0.13381146 -2.562121   -0.72492903  0.613187\n",
      "  -0.15034355 -1.5807154   0.6264738   0.9457944 ]]\n",
      "\n",
      "sigmoid activation output: \n",
      " [[0.5474174  0.38684008 0.46659696 0.07161635 0.32630855 0.64866745\n",
      "  0.46248475 0.17069417 0.65168947 0.72026867]]\n",
      "\n",
      "tanh activation output: \n",
      " [[ 0.18797891 -0.43058494 -0.1330185  -0.98816895 -0.61995316  0.54636663\n",
      "  -0.14922096 -0.91871357  0.5556194   0.7378732 ]]\n",
      "\n",
      "relu activation output: \n",
      " [[0.19024122 0.         0.         0.         0.         0.613187\n",
      "  0.         0.         0.6264738  0.9457944 ]]\n",
      "\n",
      "softmax activation output: \n",
      " [[0.11373109 0.05932205 0.08225171 0.00725343 0.04554344 0.17360501\n",
      "  0.08090309 0.01935363 0.17592703 0.24210948]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tf activation functions\n",
    "- tf.keras.activations module\n",
    "'''\n",
    "ipt = tf.random.normal(shape=(1,10))\n",
    "print('input is: \\n', ipt.numpy())\n",
    "\n",
    "linear_res = tf.keras.activations.get(None)(ipt)\n",
    "print('\\ndefault linear activation output: \\n',linear_res.numpy())\n",
    "\n",
    "sigmoid_res = tf.keras.activations.get('sigmoid')(ipt)\n",
    "print('\\nsigmoid activation output: \\n',sigmoid_res.numpy())\n",
    "\n",
    "tanh_res = tf.keras.activations.tanh(ipt)\n",
    "print('\\ntanh activation output: \\n',tanh_res.numpy())\n",
    "\n",
    "relu_res = tf.keras.activations.relu(ipt)\n",
    "print('\\nrelu activation output: \\n',relu_res.numpy())\n",
    "\n",
    "softmax_res = tf.keras.activations.softmax(ipt)\n",
    "print('\\nsoftmax activation output: \\n',softmax_res.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf gradients and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
