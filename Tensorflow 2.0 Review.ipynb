{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow 2.0.0 Basics Review\n",
    "## - tf variables\n",
    "## - tf operations\n",
    "## - tf keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow GPU Version: 2.0.0\n",
      "Eager Execution is: True\n",
      "Keras Version: 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- check tensorflow and keras version\n",
    "'''\n",
    "print(f'Tensorflow GPU Version: {tf.__version__}')\n",
    "print(f'Eager Execution is: {tf.executing_eagerly()}')\n",
    "print(f'Keras Version: {tf.keras.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- check GPU \n",
    "'''\n",
    "var = tf.Variable([3,3])\n",
    "if tf.test.is_gpu_available():\n",
    "    print('Running on GPU')\n",
    "else:\n",
    "    print('Runing on CPU')\n",
    "\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf1:  <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.6>\n",
      "tf2:  <tf.Variable 'Variable:0' shape=(3, 3) dtype=int32, numpy=\n",
      "array([[0, 4, 5],\n",
      "       [4, 2, 7],\n",
      "       [7, 8, 9]], dtype=int32)>\n",
      "tf1 iwth Numpy: 5.599999904632568\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- Declare variables\n",
    "'''\n",
    "var = 56\n",
    "tf1 = tf.Variable(var, dtype=tf.float32)\n",
    "tf1.assign(5.6)\n",
    "tf2 = tf.Variable([[0,4,5],[4,2,7],[7,8,9]])\n",
    "print('tf1: ',tf1)\n",
    "print('tf2: ',tf2)\n",
    "print(f'tf1 iwth Numpy: {tf1.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(568, shape=(), dtype=int16)\n",
      "568\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- Declare Constants\n",
    "'''\n",
    "constantVar = tf.constant(568, dtype = tf.int16)\n",
    "print(constantVar)\n",
    "print(constantVar.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var shape: (3, 4)\n",
      "var1 shape: (2, 6)\n",
      "var2 shape: (1, 12)\n",
      "var3 shape: (4, 3)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- Reshape a tensor\n",
    "'''\n",
    "var = tf.Variable([[2,3,4,5],[5,2,9,0],[3,1,2,4]], dtype=tf.float32)\n",
    "print('var shape:',var.shape)\n",
    "var1 = tf.reshape(var,(2,6))\n",
    "print('var1 shape:',var1.shape)\n",
    "var2 = tf.reshape(var, (1,12))\n",
    "print('var2 shape:',var2.shape)\n",
    "var3 = tf.reshape(var, (4,3))\n",
    "print('var3 shape:',var3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var rank: tf.Tensor(3, shape=(), dtype=int32)\n",
      "var2 slice: tf.Tensor(\n",
      "[[[ 2.  3.  4.  5.]\n",
      "  [ 5.  2.  9.  0.]\n",
      "  [ 3.  1.  2.  4.]]\n",
      "\n",
      " [[ 0. 30. 40. 50.]\n",
      "  [ 5.  2.  9.  0.]\n",
      "  [ 3.  1.  2.  4.]]], shape=(2, 3, 4), dtype=float32)\n",
      "var2 rank: 3\n",
      "var2 size: 24\n",
      "var2 dtype: <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tensor dimention\n",
    "- tensor slice\n",
    "- tensor cast numpy\n",
    "- tensor slice\n",
    "- tensor dtype\n",
    "'''\n",
    "var = tf.Variable([[[2,3,4,5],[5,2,9,0],[3,1,2,4]],[[0,30,40,50],[5,2,9,0],[3,1,2,4]],[[12,13,14,15],[5,2,9,0],[3,1,2,4]]], dtype=tf.float32)\n",
    "print('var rank:',tf.rank(var))\n",
    "var2 = var[0:2]\n",
    "print('var2 slice:',var2)\n",
    "print('var2 rank:',tf.rank(var2).numpy())\n",
    "print('var2 size:',tf.size(var2).numpy())\n",
    "print('var2 dtype:', var2.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var tensor:\n",
      " tf.Tensor(\n",
      "[[[  4.   6.   8.  10.]\n",
      "  [ 10.   4.  18.   0.]\n",
      "  [  6.   2.   4.   8.]]\n",
      "\n",
      " [[  0.  60.  80. 100.]\n",
      "  [ 10.   4.  18.   0.]\n",
      "  [  6.   2.   4.   8.]]\n",
      "\n",
      " [[ 24.  26.  28.  30.]\n",
      "  [ 10.   4.  18.   0.]\n",
      "  [  6.   2.   4.   8.]]], shape=(3, 3, 4), dtype=float32)\n",
      "\n",
      "var numpy cast:\n",
      " [[[  4.   6.   8.  10.]\n",
      "  [ 10.   4.  18.   0.]\n",
      "  [  6.   2.   4.   8.]]\n",
      "\n",
      " [[  0.  60.  80. 100.]\n",
      "  [ 10.   4.  18.   0.]\n",
      "  [  6.   2.   4.   8.]]\n",
      "\n",
      " [[ 24.  26.  28.  30.]\n",
      "  [ 10.   4.  18.   0.]\n",
      "  [  6.   2.   4.   8.]]]\n",
      "\n",
      "var*4:\n",
      " tf.Tensor(\n",
      "[[[ 16.  24.  32.  40.]\n",
      "  [ 40.  16.  72.   0.]\n",
      "  [ 24.   8.  16.  32.]]\n",
      "\n",
      " [[  0. 240. 320. 400.]\n",
      "  [ 40.  16.  72.   0.]\n",
      "  [ 24.   8.  16.  32.]]\n",
      "\n",
      " [[ 96. 104. 112. 120.]\n",
      "  [ 40.  16.  72.   0.]\n",
      "  [ 24.   8.  16.  32.]]], shape=(3, 3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tensor element-wise primitive tensor operations\n",
    "- tensor broadcasting\n",
    "'''\n",
    "var1 = tf.Variable([[[2,3,4,5],[5,2,9,0],[3,1,2,4]],[[0,30,40,50],[5,2,9,0],[3,1,2,4]],[[12,13,14,15],[5,2,9,0],[3,1,2,4]]], dtype=tf.float32)\n",
    "var2 = tf.Variable([[[2,2,2,2],[2,2,2,2],[2,2,2,2]],[[2,2,2,2],[2,2,2,2],[2,2,2,2]],[[2,2,2,2],[2,2,2,2],[2,2,2,2]]], dtype=tf.float32)\n",
    "var = var1 * var2\n",
    "print('var tensor:\\n',var)\n",
    "print('\\nvar numpy cast:\\n',var.numpy())\n",
    "print('\\nvar*4:\\n',var*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var:\n",
      " tf.Tensor(\n",
      "[[14 23]\n",
      " [23 50]], shape=(2, 2), dtype=int32)\n",
      "\n",
      "var dtype: <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tensor multiplication\n",
    "- tensor constant\n",
    "- tensor constant cast\n",
    "'''\n",
    "var1 = tf.constant([[1,2,3],[4,5,3]])\n",
    "var2 = tf.constant([[1,2,3],[4,5,3]])\n",
    "var = tf.matmul(var1,tf.transpose(var2))\n",
    "print('var:\\n',var)\n",
    "var = tf.cast(var, dtype=tf.float32)\n",
    "print('\\nvar dtype:', var.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[1.0, 2.0, 4.0], [], [3.0, 4.0], [1.0]]>\n",
      "tf.Tensor([1. 2. 4.], shape=(3,), dtype=float32)\n",
      "tf.Tensor([1.], shape=(1,), dtype=float32)\n",
      "<tf.RaggedTensor [[1, 2, 4], [], [5, 6], [7], [8]]>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- ragged tensors by constant\n",
    "- ragged tensors by split_row\n",
    "'''\n",
    "ragged_tensor = tf.ragged.constant([[1,2,4],[],[3,4],[1]], dtype=tf.float32, name='ragged_tensor')\n",
    "print(ragged_tensor)\n",
    "print(ragged_tensor[0])\n",
    "print(ragged_tensor[3])\n",
    "ragged_tensor2 = tf.RaggedTensor.from_row_splits(values=[1,2,4,5,6,7,8], row_splits=[0,3,3,5,6,7])\n",
    "print(ragged_tensor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_t1:\n",
      " tf.Tensor(\n",
      "[[0.23302452]\n",
      " [0.33036613]\n",
      " [0.07002912]], shape=(3, 1), dtype=float32)\n",
      "_t2:\n",
      " tf.Tensor(\n",
      "[[-1.1006709 ]\n",
      " [-0.40500164]\n",
      " [-1.2100329 ]], shape=(3, 1), dtype=float32)\n",
      "_t:\n",
      " tf.Tensor(\n",
      "[[1.7787435 ]\n",
      " [0.54076576]\n",
      " [1.638559  ]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- calculate square differences\n",
    "'''\n",
    "t1 = tf.random.normal((3,1))\n",
    "t2 = tf.random.normal((3,1))\n",
    "t = tf.math.squared_difference(t1,t2,name='square_difference')\n",
    "print('_t1:\\n',t1)\n",
    "print('_t2:\\n',t2)\n",
    "print('_t:\\n',t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_var:\n",
      " tf.Tensor(\n",
      "[[8.844145   8.424911   5.7227373 ]\n",
      " [4.715581   8.562659   7.411667  ]\n",
      " [0.57933927 8.567917   4.0922523 ]], shape=(3, 3), dtype=float32)\n",
      "\n",
      "_cross_mean:\n",
      " tf.Tensor(6.324579, shape=(), dtype=float32)\n",
      "\n",
      "_x_mean:\n",
      " tf.Tensor([4.7130218 8.518496  5.7422185], shape=(3,), dtype=float32)\n",
      "\n",
      "_x_mean_same_dim:\n",
      " tf.Tensor([[4.7130218 8.518496  5.7422185]], shape=(1, 3), dtype=float32)\n",
      "\n",
      "_y_mean:\n",
      " tf.Tensor([7.6639304 6.8966355 4.4131694], shape=(3,), dtype=float32)\n",
      "\n",
      "_y_mean_same_dim:\n",
      " tf.Tensor(\n",
      "[[7.6639304]\n",
      " [6.8966355]\n",
      " [4.4131694]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- calculate tensor mean\n",
    "'''\n",
    "var = tf.constant(tf.random.uniform((3,3),minval=0, maxval= 10, dtype=tf.float32))\n",
    "cross_mean = tf.reduce_mean(var, axis=None)\n",
    "x_mean = tf.reduce_mean(var, axis=0)\n",
    "y_mean = tf.reduce_mean(var, axis=1)\n",
    "x_mean_dim = tf.reduce_mean(var, axis=0,keepdims=True)\n",
    "y_mean_dim = tf.reduce_mean(var, axis=1, keepdims=True)\n",
    "print('_var:\\n',var)\n",
    "print('\\n_cross_mean:\\n',cross_mean)\n",
    "print('\\n_x_mean:\\n',x_mean)\n",
    "print('\\n_x_mean_same_dim:\\n',x_mean_dim)\n",
    "print('\\n_y_mean:\\n',y_mean)\n",
    "print('\\n_y_mean_same_dim:\\n',y_mean_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_random_var1:\n",
      " tf.Tensor(\n",
      "[[ 0.43616885]\n",
      " [-1.9093795 ]\n",
      " [ 1.3789066 ]\n",
      " [-1.0405852 ]], shape=(4, 1), dtype=float32)\n",
      "_random_var2:\n",
      " tf.Tensor(\n",
      "[[ 4.6377807]\n",
      " [14.660629 ]\n",
      " [-6.065405 ]\n",
      " [ 7.9940577]], shape=(4, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tensors with random normal values\n",
    "'''\n",
    "tf.random.set_seed(2)\n",
    "var_random1 = tf.random.normal((4,1),mean=0,stddev=1)\n",
    "var_random2 = tf.random.normal((4,1),mean=5,stddev=10)\n",
    "print('_random_var1:\\n',var_random1)\n",
    "print('_random_var2:\\n',var_random2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_var1: tf.Tensor(\n",
      "[[3]\n",
      " [1]\n",
      " [3]\n",
      " [1]], shape=(4, 1), dtype=int32)\n",
      "_var2: tf.Tensor(\n",
      "[[7]\n",
      " [5]\n",
      " [7]\n",
      " [7]], shape=(4, 1), dtype=int32)\n",
      "_var_concat_x: tf.Tensor(\n",
      "[[3]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [7]\n",
      " [5]\n",
      " [7]\n",
      " [7]], shape=(8, 1), dtype=int32)\n",
      "_var_concat_y: tf.Tensor(\n",
      "[[3 7]\n",
      " [1 5]\n",
      " [3 7]\n",
      " [1 7]], shape=(4, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tensors with random uniform values\n",
    "'''\n",
    "tf.random.set_seed(2)\n",
    "var1 = tf.random.uniform((4,1),minval=1, maxval=4, dtype=tf.int32)\n",
    "var2 = tf.random.uniform((4,1),minval=5, maxval=8, dtype=tf.int32)\n",
    "var_concat_x = tf.concat(values=[var1,var2],axis=0)\n",
    "var_concat_y = tf.concat(values=[var1,var2],axis=1)\n",
    "print('_var1:',var1)\n",
    "print('_var2:',var2)\n",
    "print('_var_concat_x:',var_concat_x)\n",
    "print('_var_concat_y:',var_concat_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_var:\n",
      " tf.Tensor(\n",
      "[[ 4  7  1  3  4  8  8  5  2  0]\n",
      " [ 6  4  0  9  4  3 -2  2  5 -1]\n",
      " [ 3  4  8  0 -1  5  0  8  4  7]\n",
      " [ 8  2  9  7  0  5  7  8  1  2]\n",
      " [ 3  1 -1  9 -2  5  2  7 -2  0]\n",
      " [ 3  8 -1 -2  8  1  5  7  5  6]\n",
      " [ 0  9 -2  3  8  3  4  0  4  0]\n",
      " [ 0  9  2  3  8  1  1  3  1  4]\n",
      " [-2  5  4  6 -2 -1  0  2  0  7]\n",
      " [-2  1  8  6 -2  4  3  6  4  0]], shape=(10, 10), dtype=int32)\n",
      "_max_index_x:\n",
      " [3 6 3 1 5 0 0 2 1 2]\n",
      "_min_index_y:\n",
      " [9 6 4 4 4 3 2 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- indexing tensor values\n",
    "- defualt axis=None, it's across cols to search max or min\n",
    "'''\n",
    "tf.random.set_seed(1)\n",
    "var = tf.constant(tf.random.uniform((10,10), minval=-2, maxval=10, dtype=tf.int32))\n",
    "_max_index_x = tf.argmax(input = var, axis=0, output_type=tf.int32)\n",
    "_min_index_y = tf.argmin(input = var, axis=1, output_type=tf.int32)\n",
    "print('_var:\\n', var)\n",
    "print('_max_index_x:\\n',_max_index_x.numpy())\n",
    "print('_min_index_y:\\n',_min_index_y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original var: \n",
      " [[4 5 6]\n",
      " [4 5 6]\n",
      " [4 5 6]]\n",
      "new var: \n",
      " [[0 0 6]\n",
      " [4 0 0]\n",
      " [0 5 0]]\n",
      "restore var: \n",
      " [[4 5 6]\n",
      " [4 5 6]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- Checkpoint to restore and save tensors\n",
    "- Checkpoint's constructor accepts keyword arguments whose values are types that contain trackable state, such as \n",
    "  ~`tf.keras.optimizers.Optimizer` implementations, \n",
    "  ~`tf.Variable`, \n",
    "  ~`tf.keras.Layer` implementations,\n",
    "  ~`tf.keras.Model` implementations. \n",
    "  It saves these values with a checkpoint and maintains a `save_counter` for numbering checkpoints\n",
    "'''\n",
    "var = tf.Variable([[4,5,6],[4,5,6],[4,5,6]])\n",
    "print('original var: \\n',var.numpy())\n",
    "savePoint = tf.train.Checkpoint(var=var)\n",
    "savePath = savePoint.save('./tk_ckpts/vars')\n",
    "var.assign([[0,0,6],[4,0,0],[0,5,0]])\n",
    "print('new var: \\n',var.numpy())\n",
    "savePoint.restore(savePath)\n",
    "print('restore var: \\n',var.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(23, shape=(), dtype=int32)\n",
      "tf.Tensor(22, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tf.function(\n",
    "    func=None, input_signature=None, autograph=True, experimental_implements=None,\n",
    "    experimental_autograph_options=None, experimental_relax_shapes=False,\n",
    "    experimental_compile=None\n",
    ")\n",
    "\n",
    "'''\n",
    "def calc(x,y):\n",
    "    return x**2*5+y\n",
    "f1 = tf.function(test)\n",
    "print(f1(2,3))\n",
    "\n",
    "@tf.function\n",
    "def calc_2(x,y):\n",
    "    return x*6+y\n",
    "print(calc_2(3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack_x: \n",
      " tf.Tensor(\n",
      "[[1 2 3 4]\n",
      " [1 2 3 4]\n",
      " [1 2 3 4]], shape=(3, 4), dtype=int32)\n",
      "\n",
      "stack_y: \n",
      " tf.Tensor(\n",
      "[[1 1 1]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [4 4 4]], shape=(4, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tf.stack\n",
    "- axis=0, take each item in the list and stack from top to bottom\n",
    "- axis=1, take values of each item in the list and make a row and stack rows from top to bottom \n",
    "'''\n",
    "var1 = tf.constant([1,2,3,4])\n",
    "var2 = tf.constant([1,2,3,4])\n",
    "var3 = tf.constant([1,2,3,4])\n",
    "stack_x = tf.stack([var1,var2,var3], axis=0)\n",
    "stack_y = tf.stack([var1,var2,var3], axis=1)\n",
    "print('stack_x: \\n', stack_x)\n",
    "print('\\nstack_y: \\n',stack_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf keras modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=4445, shape=(2, 2), dtype=float16, numpy=\n",
       "array([[1., 2.],\n",
       "       [3., 4.]], dtype=float16)>"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- declare a variable with backend\n",
    "- Keras is a model-level library, providing high-level building blocks for developing deep learning models. It does not handle low-level operations such as tensor products, convolutions and so on itself. \n",
    "- Instead, it relies on a specialized, well optimized tensor manipulation library to do so, serving as the \"backend engine\" of Keras. Rather than picking one single tensor library and making the \n",
    "implementation of Keras tied to that library, Keras handles the problem in a modular way, and several different backend engines can be plugged seamlessly into Keras.\n",
    "'''\n",
    "var = K.constant([[1,2],[3,4]],dtype=tf.float16)\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- load minist data\n",
    "'''\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_x, train_y),(test_x, test_y)=mnist.load_data()\n",
    "train_x = train_x.astype('float32')/255\n",
    "test_x =test_x.astype('float32')/255\n",
    "train_x = train_x.reshape(train_x.shape[0],28,28,1)\n",
    "test_x = test_x.reshape(test_x.shape[0],28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 6, 6, 64)          18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 87,562\n",
      "Trainable params: 87,370\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tf.keras Functional API\n",
    "'''\n",
    "ipt = tf.keras.Input(shape=(28,28,1))\n",
    "opt = tf.keras.layers.Conv2D(32,3)(ipt)\n",
    "opt = tf.keras.layers.LeakyReLU()(opt)\n",
    "opt = tf.keras.layers.BatchNormalization()(opt)\n",
    "opt = tf.keras.layers.MaxPool2D((3,3))(opt)\n",
    "\n",
    "opt = tf.keras.layers.Conv2D(64,3)(opt)\n",
    "opt = tf.keras.layers.LeakyReLU()(opt)\n",
    "opt = tf.keras.layers.BatchNormalization()(opt)\n",
    "opt = tf.keras.layers.MaxPool2D((3,3))(opt)\n",
    "\n",
    "opt = tf.keras.layers.Flatten()(opt)\n",
    "opt = tf.keras.layers.Dense(256, activation='relu')(opt)\n",
    "opt = tf.keras.layers.Dense(10, activation='softmax')(opt)\n",
    "\n",
    "model = tf.keras.models.Model(ipt, opt)\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=['acc']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.1646 - acc: 0.9532 - val_loss: 0.2478 - val_acc: 0.9220\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.0489 - acc: 0.9844 - val_loss: 0.0399 - val_acc: 0.9858\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0335 - acc: 0.9891 - val_loss: 0.0390 - val_acc: 0.9874\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0295 - acc: 0.9905 - val_loss: 0.0350 - val_acc: 0.9889\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0227 - acc: 0.9929 - val_loss: 0.0549 - val_acc: 0.9819\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0202 - acc: 0.9931 - val_loss: 0.0441 - val_acc: 0.9860\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.0162 - acc: 0.9948 - val_loss: 0.0338 - val_acc: 0.9894\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0144 - acc: 0.9952 - val_loss: 0.0409 - val_acc: 0.9887\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.0131 - acc: 0.9958 - val_loss: 0.0397 - val_acc: 0.9893\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0121 - acc: 0.9959 - val_loss: 0.0354 - val_acc: 0.9905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f67f063e350>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_x, \n",
    "    train_y,\n",
    "    batch_size = 128,\n",
    "    epochs = 10,\n",
    "    validation_data=(test_x,test_y),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- tf.keras.Model class\n",
    "- tf.keras.callbacks.Callback class\n",
    "- tf.keras.callbacks.EarlyStopping class\n",
    "'''\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32,(3,3), padding='same')\n",
    "        self.act1 = tf.keras.layers.LeakyReLU()\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.pool1 = tf.keras.layers.MaxPooling2D((3,3))\n",
    "        \n",
    "        self.conv2 = tf.keras.layers.Conv2D(32,(3,3), padding='same')\n",
    "        self.act2 = tf.keras.layers.LeakyReLU()\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.pool2 = tf.keras.layers.MaxPooling2D((3,3))\n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(512, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.act1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "    \n",
    "class customCallback(tf.keras.callbacks.Callback):\n",
    "    # constructor\n",
    "    def __init__(self):\n",
    "        # parent constructor\n",
    "        super(customCallback,self).__init__()\n",
    "        \n",
    "    # call at the training end    \n",
    "    def on_epoch_end(self, batch, logs=None):\n",
    "        print('\\n train_acc-val_acc:',logs['acc']-logs['val_acc'])\n",
    "        print(self.params)\n",
    "\n",
    "'''\n",
    "ValAccEarlyStopping Class:\n",
    "- val_acc_base is to define the expected val_acc at the end of each epoch traning\n",
    "- if val_acc >= val_acc_base, model will stop training\n",
    "- if early stopping is not triggered by the end of training, the model with best \n",
    "val_acc will be restored\n",
    "'''\n",
    "class ValAccEarlyStopping(tf.keras.callbacks.EarlyStopping):\n",
    "    # constructor\n",
    "    def __init__(self, val_acc_base):\n",
    "        # parent constructor\n",
    "        super(ValAccEarlyStopping,self).__init__(monitor='val_acc', verbose=1, baseline=val_acc_base, restore_best_weights=True)\n",
    "        # fields\n",
    "        self.__best_weights=None\n",
    "        self.__bestWeightEpoch=None\n",
    "        self.__weights =[]\n",
    "        self.__val_acc=[]\n",
    "        \n",
    "    # early stopping method\n",
    "    def on_epoch_end(self,epoch,logs=None): \n",
    "        # restore best model weights\n",
    "        if self.restore_best_weights:\n",
    "            # save weights & val_acc for each epoch\n",
    "            self.__weights.append(self.model.get_weights())\n",
    "            self.__val_acc.append(logs['val_acc'])\n",
    "            \n",
    "            # update the best weights\n",
    "            self.__bestWeightEpoch = self.__val_acc.index(max(self.__val_acc))\n",
    "            self.__best_weights = self.__weights[self.__bestWeightEpoch]           \n",
    "        \n",
    "        # early stopping check\n",
    "        if logs[self.monitor]>=self.baseline:\n",
    "            self.model.stop_training = True\n",
    "            self.stopped_epoch = epoch+1\n",
    "        \n",
    "    # update early stopping training end method         \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0 and self.verbose > 0:\n",
    "            print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))\n",
    "        else:\n",
    "            self.model.set_weights(self.__best_weights)\n",
    "            print(f'Early stopping is not triggered, but best model is restored at epoch {self.__bestWeightEpoch+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.1619 - acc: 0.9514 - val_loss: 0.1679 - val_acc: 0.9587\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.0498 - acc: 0.9843 - val_loss: 0.0428 - val_acc: 0.9859\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6614d91290>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- build model \n",
    "- compile model\n",
    "- train model with custom early stopping class\n",
    "'''\n",
    "model = MyModel()\n",
    "model.compile(\n",
    "    optimizer= tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics= ['acc'],\n",
    ")\n",
    "model.fit(\n",
    "    train_x, \n",
    "    train_y,\n",
    "    batch_size = 128,\n",
    "    epochs = 10,\n",
    "    validation_data = (test_x,test_y),\n",
    "    callbacks = [ValAccEarlyStopping(val_acc_base=0.98)],\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf data pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (60000, 28, 28, 1) , x_train data type:  float32\n",
      "x_test shape:  (10000, 28, 28, 1) , x_test data type:  float32\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "- load fashin_minist dataset\n",
    "- 10 classes 0-9\n",
    "'''\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0],28,28,1).astype('float32')/255\n",
    "x_test = x_test.reshape(x_test.shape[0],28,28,1).astype('float32')/255\n",
    "print('x_train shape: ',x_train.shape,', x_train data type: ', x_train.dtype)\n",
    "print('x_test shape: ',x_test.shape,', x_test data type: ',x_test.dtype)\n",
    " \n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(x_train[258].reshape((28,28)).astype('float32'))\n",
    "plt.title('fashion_mnist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- data pipeline from Numpy Arrays\n",
    "- tf.data.Dataset.from_tensor_slices\n",
    "'''\n",
    "batch_size = 128\n",
    "shuffle_buffer_size = 10000\n",
    "tf.random.set_seed(5)\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(batch_size).shuffle(shuffle_buffer_size)\n",
    "train_data = train_data.repeat()\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test,y_test)).batch(batch_size).shuffle(shuffle_buffer_size)\n",
    "test_data = test_data.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (128, 28, 28, 1)\n",
      "y shape: (128,)\n",
      "x shape: (128, 28, 28, 1)\n",
      "y shape: (128,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- data pipline from Numpy with iterator\n",
    "- tf.data.Dataset.from_tensor_slices\n",
    "- tf.compat.v1.data.make_one_shot_iteraor\n",
    "'''\n",
    "batch_size = 128\n",
    "shuffle_buffer_size = 10000\n",
    "tf.random.set_seed(5)\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(batch_size).shuffle(shuffle_buffer_size)\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test,y_test)).batch(batch_size).shuffle(shuffle_buffer_size)\n",
    "train_iterator = tf.compat.v1.data.make_one_shot_iterator(train_data)\n",
    "test_iterator = tf.compat.v1.data.make_one_shot_iterator(test_data)\n",
    "\n",
    "tr_x,tr_y = train_iterator.get_next()\n",
    "print('x shape:',tr_x.shape)\n",
    "print('y shape:',tr_y.shape)\n",
    "te_x,te_y = test_iterator.get_next()\n",
    "print('x shape:',te_x.shape)\n",
    "print('y shape:',te_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- data pipline from CSV file \n",
    "- tf.data.experimental.make_csv_dataset()\n",
    "'''\n",
    "TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
    "TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n",
    "\n",
    "train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\n",
    "test_file_path = tf.keras.utils.get_file(\"eval.csv\", TEST_DATA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 627 entries, 0 to 626\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   survived            627 non-null    int64  \n",
      " 1   sex                 627 non-null    object \n",
      " 2   age                 627 non-null    float64\n",
      " 3   n_siblings_spouses  627 non-null    int64  \n",
      " 4   parch               627 non-null    int64  \n",
      " 5   fare                627 non-null    float64\n",
      " 6   class               627 non-null    object \n",
      " 7   deck                627 non-null    object \n",
      " 8   embark_town         627 non-null    object \n",
      " 9   alone               627 non-null    object \n",
      "dtypes: float64(2), int64(3), object(5)\n",
      "memory usage: 49.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train_dt = pd.read_csv(train_file_path)\n",
    "train_dt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 264 entries, 0 to 263\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   survived            264 non-null    int64  \n",
      " 1   sex                 264 non-null    object \n",
      " 2   age                 264 non-null    float64\n",
      " 3   n_siblings_spouses  264 non-null    int64  \n",
      " 4   parch               264 non-null    int64  \n",
      " 5   fare                264 non-null    float64\n",
      " 6   class               264 non-null    object \n",
      " 7   deck                264 non-null    object \n",
      " 8   embark_town         264 non-null    object \n",
      " 9   alone               264 non-null    object \n",
      "dtypes: float64(2), int64(3), object(5)\n",
      "memory usage: 20.8+ KB\n"
     ]
    }
   ],
   "source": [
    "test_dt = pd.read_csv(test_file_path)\n",
    "test_dt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMN = 'survived'\n",
    "LABELS = [0,1]\n",
    "\n",
    "def get_csv_data(file_path, batch=3, **kwargs):\n",
    "    csvData = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern = file_path,\n",
    "        batch_size = batch,\n",
    "        label_name = LABEL_COLUMN,\n",
    "        na_value = \"?\",\n",
    "        num_epochs= 1,\n",
    "        ignore_errors = True,\n",
    "        **kwargs\n",
    "    )\n",
    "    return csvData\n",
    "\n",
    "# load csv data\n",
    "train_data = get_csv_data(train_file_path)\n",
    "test_data = get_csv_data(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'male' b'female' b'female']\n",
      "age                 : [32. 28. 39.]\n",
      "n_siblings_spouses  : [0 1 1]\n",
      "parch               : [0 0 1]\n",
      "fare                : [ 56.4958 133.65    83.1583]\n",
      "class               : [b'Third' b'First' b'First']\n",
      "deck                : [b'unknown' b'unknown' b'E']\n",
      "embark_town         : [b'Southampton' b'Southampton' b'Cherbourg']\n",
      "alone               : [b'y' b'n' b'n']\n",
      "\n",
      "labels              : [1 1 1]\n"
     ]
    }
   ],
   "source": [
    "def show_batch(dataset):\n",
    "    '''\n",
    "    - take only one batch of data for the display -> dataset.take(1)\n",
    "    - two batches data will be taken -> dataset.take(2)\n",
    "    - {:20s} -> 20 spaces \n",
    "    ''' \n",
    "    for batch, label in dataset.take(1):\n",
    "        for key, value in batch.items():\n",
    "            print(\"{:20s}: {}\".format(key,value.numpy()))\n",
    "        print(\"\\n{:20s}: {}\".format('labels',label.numpy()))\n",
    "            \n",
    "show_batch(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_names example data: \n",
      "sex                 : [b'male' b'female' b'female']\n",
      "age                 : [24. 28. 45.]\n",
      "n_siblings_spouses  : [0 3 1]\n",
      "parch               : [0 1 1]\n",
      "fare                : [ 13.      25.4667 164.8667]\n",
      "class               : [b'Second' b'Third' b'First']\n",
      "deck                : [b'unknown' b'unknown' b'unknown']\n",
      "embark_town         : [b'Southampton' b'Southampton' b'Southampton']\n",
      "alone               : [b'y' b'n' b'n']\n",
      "\n",
      "labels              : [0 0 1]\n",
      "\n",
      "select_columns example data: \n",
      "age                 : [28. 40. 33.]\n",
      "parch               : [0 1 0]\n",
      "\n",
      "labels              : [0 1 0]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- column_names -> to name the columes if the dataset has no column names \n",
    "- select_columns -> to select which column of data will be extraced from csv\n",
    "'''\n",
    "temp_data = get_csv_data(train_file_path, column_names = ['survived', 'sex', 'age', 'n_siblings_spouses', 'parch', 'fare', 'class', 'deck', 'embark_town', 'alone'])\n",
    "print('column_names example data: ')\n",
    "show_batch(temp_data)\n",
    "\n",
    "temp_data = get_csv_data(train_file_path, select_columns = ['survived','age','parch'])\n",
    "print('\\nselect_columns example data: ')\n",
    "show_batch(temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                 : [ 3. 28. 29. 29. 28. 28. 22. 22. 45. 18.]\n",
      "n_siblings_spouses  : [4. 1. 1. 1. 0. 0. 0. 0. 1. 0.]\n",
      "parch               : [2. 2. 1. 0. 0. 0. 0. 1. 1. 1.]\n",
      "fare                : [ 31.3875  23.45    10.4625  26.       7.75     8.05     7.2292  55.\n",
      " 164.8667  23.    ]\n",
      "\n",
      "labels              : [1 0 0 1 1 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- load selected column data in one batch size\n",
    "'''\n",
    "SELECT_COLUMNS = ['survived', 'age', 'n_siblings_spouses', 'parch', 'fare']\n",
    "DEFAULTS = [0, 0.0, 0.0, 0.0, 0.0]\n",
    "temp_dataset = get_csv_data(train_file_path, batch=10, select_columns=SELECT_COLUMNS, column_defaults = DEFAULTS)\n",
    "\n",
    "show_batch(temp_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one batch data and labels:\n",
      "age            : [32. 28. 21. 18. 18. 29.  4.  1. 40. 18.]\n",
      "n_siblings_spouses: [0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "parch          : [0. 0. 0. 0. 0. 0. 2. 2. 0. 1.]\n",
      "fare           : [ 8.3625  7.8792 77.9583  7.775  13.      9.5    22.025  20.575  31.\n",
      "  7.8542]\n",
      "labels              : [0 1 1 0 0 1 1 1 1 0]\n",
      "\n",
      "Stack data rows:\n",
      "[[32.     28.     21.     18.     18.     29.      4.      1.     40.\n",
      "  18.    ]\n",
      " [ 0.      0.      0.      0.      0.      0.      0.      1.      0.\n",
      "   1.    ]\n",
      " [ 0.      0.      0.      0.      0.      0.      2.      2.      0.\n",
      "   1.    ]\n",
      " [ 8.3625  7.8792 77.9583  7.775  13.      9.5    22.025  20.575  31.\n",
      "   7.8542]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- take one example batch\n",
    "- The Python iter() function returns an iterator for the given object.\n",
    "- iter(temp_datset) returns a tensorflow iterator\n",
    "- example_batch -> tensor dictionary\n",
    "- labels_batch -> tensor\n",
    "'''\n",
    "features, labels = next(iter(temp_dataset)) \n",
    "print('one batch data and labels:')\n",
    "for key,values in features.items():\n",
    "     print(\"{:15s}: {}\".format(key,values.numpy()))\n",
    "print(\"{:20s}: {}\".format('labels',labels.numpy()))\n",
    "\n",
    "'''\n",
    "- stack rows from top to bottom to make a matrix \n",
    "'''\n",
    "print('\\nStack data rows:')\n",
    "print(tf.stack(list(features.values()),axis=0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 48.      22.      35.      26.      28.      31.      17.      40.\n",
      "   30.      19.    ]\n",
      " [  1.       0.       0.       0.       1.       1.       0.       0.\n",
      "    0.       0.    ]\n",
      " [  0.       0.       0.       0.       0.       0.       0.       0.\n",
      "    0.       2.    ]\n",
      " [ 39.6      9.35     7.05    56.4958  15.5    113.275    8.6625  31.\n",
      "   10.5     26.2833]]\n",
      "\n",
      "[1 0 0 1 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- define pack function \n",
    "- take features and labels in one batch of data\n",
    "- return a stacked matrix and label pair\n",
    "'''\n",
    "def pack_rows (features, labels):\n",
    "    return tf.stack(list(features.values()),axis=1),labels\n",
    "\n",
    "packed_dataset = temp_dataset.map(pack_rows)\n",
    "\n",
    "for features, labels in packed_dataset.take(1):\n",
    "    print(features.numpy())\n",
    "    print()\n",
    "    print(labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- put all together into one class for the data pipeline process\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- categorical data processing\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=8413, shape=(60000, 10), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- one hot encoding\n",
    "'''\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "y_train = tf.one_hot(y_train, depth=10)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.keras.layers.Layer and activation functions\n",
    "## - tf.keras.layers.Layer custom definition (https://www.tensorflow.org/guide/keras/custom_layers_and_models#layers_encapsulate_a_state_weights_and_some_computation) use row matrix multiplication;\n",
    "## - row[...] * mat [...] = row[...]\n",
    "## - weight matrix shape is input_shape * units and this cannot be changed in the custom layer as it has to be consistentcy for further calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial weights:  tf.Tensor(\n",
      "[[ 0.40749726  0.02219561 -2.200683   -0.8212301 ]\n",
      " [-0.36420798 -2.177051   -0.8709116  -0.40788257]\n",
      " [ 0.37394255 -0.49891925 -0.00245201 -0.14541708]\n",
      " [ 0.56160647 -0.88500816 -0.12318895  0.7797051 ]], shape=(4, 4), dtype=float32)\n",
      "initial zeros:  tf.Tensor(\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]], shape=(4, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tf.random_normal_initializer\n",
    "- tf.zeros_initializer\n",
    "'''\n",
    "tf.random.set_seed(15)\n",
    "input_dim = (4,4)\n",
    "init = tf.random_normal_initializer(mean=0.0, stddev=1)\n",
    "init_zeros = tf.zeros_initializer()\n",
    "print('initial weights: ',init(shape=input_dim, dtype=tf.float32))\n",
    "print('initial zeros: ', init_zeros(shape=input_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- single linear combination layer with self-defined weights, units -> output channels, input_shape -> input shape\n",
    "- computation is max[...]*col[...] = col[...]\n",
    "- **kwargs-> allowed_kwargs = {\n",
    "                                'input_shape',\n",
    "                                'batch_input_shape',\n",
    "                                'batch_size',\n",
    "                                'weights',\n",
    "                                'activity_regularizer',\n",
    "                                'autocast'\n",
    "                            }\n",
    "- \n",
    "'''\n",
    "class customLayer(tf.keras.layers.Layer):\n",
    "    #constructor\n",
    "    def __init__(self, units = 3, input_shape=5, trainable=True, name=None, dtype=tf.float32, **kwargs):\n",
    "        # parent constructor\n",
    "        super(customLayer, self).__init__(name=name, dtype= dtype, trainable=trainable, **kwargs)\n",
    "        # fields\n",
    "        init_w = tf.random_normal_initializer(mean=0, stddev=1)\n",
    "        init_b = tf.zeros_initializer()\n",
    "        self.w = tf.Variable(init_w(shape=(units, input_shape),dtype=dtype), trainable=trainable)\n",
    "        self.b = tf.Variable(init_b(shape=(units,)),dtype=dtype, trainable=trainable)       \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        if tf.rank(inputs)==1:\n",
    "            inputs = tf.keras.backend.expand_dims(inputs, axis=1)\n",
    "        return tf.matmul(self.w, inputs)+self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape is at 4 * 1: \n",
      "[1. 1. 1. 1.]\n",
      "\n",
      "layer1 trainable weights is at 5*4: \n",
      "  [[ 2.1618998   0.07567143  1.8947576   1.5536673 ]\n",
      " [-1.2869931  -0.81316155  0.6367824  -1.7842115 ]\n",
      " [ 0.32601485 -0.3001915  -0.73505306  0.06373952]\n",
      " [-0.6815776  -0.5778277   1.0000818   0.6779952 ]\n",
      " [ 0.38209513 -0.7653013  -0.3519354   0.03938751]]\n",
      "\n",
      "output channels is at 5*1: \n",
      " [[ 5.685996    5.685996    5.685996    5.685996    5.685996  ]\n",
      " [-3.2475839  -3.2475839  -3.2475839  -3.2475839  -3.2475839 ]\n",
      " [-0.64549017 -0.64549017 -0.64549017 -0.64549017 -0.64549017]\n",
      " [ 0.4186716   0.4186716   0.4186716   0.4186716   0.4186716 ]\n",
      " [-0.695754   -0.695754   -0.695754   -0.695754   -0.695754  ]]\n",
      "\n",
      "layer1 trainable bias: \n",
      " [0. 0. 0. 0. 0.]\n",
      "\n",
      "layer1 non-trainable weights:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "input_shape=4\n",
    "units= 5\n",
    "ipt = tf.ones((input_shape))\n",
    "layer1 = customLayer(units=units, input_shape=input_shape, name='test')\n",
    "print(f'input shape is at {input_shape} * 1: \\n{ipt}')\n",
    "print(f'\\nlayer1 trainable weights is at {units}*{input_shape}: \\n  {layer1.weights[0].numpy()}',)\n",
    "print(f'\\noutput channels is at {units}*1: \\n {layer1(ipt).numpy()}')\n",
    "print('\\nlayer1 trainable bias: \\n', layer1.weights[1].numpy())\n",
    "print('\\nlayer1 non-trainable weights:\\n', layer1.non_trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- Multi-layer Perceptron(MLP) layer\n",
    "- subclass of tf.keras.layers.Layer class\n",
    "- this layer has no batch size feature\n",
    "- this layer use build() to identify the input shape on the runtime\n",
    "- this layer use row multiplication caculation on weights and inputs\n",
    "- add_weight(\n",
    "        name=None, shape=None, dtype=None, initializer=None, regularizer=None,\n",
    "        trainable=None, constraint=None, partitioner=None, use_resource=None,\n",
    "        synchronization=tf.VariableSynchronization.AUTO,\n",
    "        aggregation=tf.compat.v1.VariableAggregation.NONE, **kwargs\n",
    "    )\n",
    "- **kwargs: `getter`, 'collections`, `experimental_autocast` and `caching_device`.\n",
    "\n",
    "'''\n",
    "class MLP(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=3, activation=None, trainable=True, name=None, dtype=tf.float32, **kwargs):\n",
    "        super(MLP, self).__init__( name=name, trainable = trainable, dtype=dtype, **kwargs)\n",
    "        self.units = units;\n",
    "        self.__activation_name = activation\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),initializer='random_normal')\n",
    "        self.b = self.add_weight(shape=(self.units,),initializer='random_normal')\n",
    "    \n",
    "    def get_config(self):\n",
    "        config_dic ={\n",
    "            'units':self.units,\n",
    "            'activation': self.__activation_name, \n",
    "            'trainable_weights & bias':self.trainable_weights,\n",
    "            'non-trainable_weights & bias':self.non_trainable_weights,\n",
    "        }\n",
    "        config = super(MLP,self).get_config()\n",
    "        config.update(config_dic)\n",
    "        return config\n",
    "    \n",
    "    def call(self, inputs, training = None):\n",
    "        if tf.rank(inputs)==1:\n",
    "            inputs = tf.keras.backend.expand_dims(inputs, axis=0)\n",
    "        linear_combination = tf.matmul(inputs, self.w)+self.b \n",
    "        return self.activation(linear_combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'mlp_single_layer',\n",
       " 'trainable': True,\n",
       " 'dtype': 'float32',\n",
       " 'units': 5,\n",
       " 'activation': None,\n",
       " 'trainable_weights & bias': [<tf.Variable 'mlp_single_layer/Variable:0' shape=(4, 5) dtype=float32, numpy=\n",
       "  array([[-0.04268041, -0.0240161 , -0.00559446,  0.00667279,  0.05542039],\n",
       "         [ 0.0149584 ,  0.04895762,  0.00866133, -0.03208294, -0.02934596],\n",
       "         [ 0.12082821, -0.01217135,  0.06881073, -0.04737529, -0.01472751],\n",
       "         [ 0.09234401, -0.05215368,  0.03285414, -0.01964135,  0.09214985]],\n",
       "        dtype=float32)>,\n",
       "  <tf.Variable 'mlp_single_layer/Variable:0' shape=(5,) dtype=float32, numpy=\n",
       "  array([ 0.06843295,  0.02584788,  0.0653073 , -0.0641969 , -0.0261974 ],\n",
       "        dtype=float32)>],\n",
       " 'non-trainable_weights & bias': []}"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- get_config() function\n",
    "- override get_config() in the tf.keras.layers.Layer\n",
    "- \"input_shape\" in kwards is to generate \"batch_input_shape\" and it should be avoided in the custom layer\n",
    "- instead, use build() to get the real input shape in the custom layer\n",
    "'''\n",
    "input_shape=4\n",
    "units= 5\n",
    "ipt = tf.ones((input_shape))\n",
    "mlp = MLP(units=units, activation=None, name='mlp_single_layer', trainable=True)\n",
    "opt = mlp(ipt)\n",
    "mlp.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape is at 4: \n",
      "[1. 1. 1. 1.]\n",
      "\n",
      "layer1 trainable weights is at 4*5: \n",
      "  [[-0.02856542  0.03515685 -0.06862309  0.07264601 -0.02461529]\n",
      " [-0.00546568 -0.04800047  0.15303433 -0.06815054  0.03404353]\n",
      " [-0.02024032  0.07377522  0.01931716  0.03163869  0.01544571]\n",
      " [-0.05890731 -0.13744293 -0.08588818 -0.01069717 -0.04543935]]\n",
      "\n",
      "layer1 trainable bias is at 5: \n",
      "  [ 0.06411647  0.01416687  0.05103198 -0.03926305 -0.04594219]\n",
      "\n",
      "output channels is at 5: \n",
      " [[-0.04906226 -0.06234445  0.06887219 -0.01382606 -0.06650759]]\n",
      "\n",
      "layer1 non-trainable weights:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- single MLP layer computation\n",
    "- without activation functions\n",
    "- with build() to acquire input_shape\n",
    "'''\n",
    "input_shape=4\n",
    "units= 5\n",
    "ipt = tf.ones((input_shape))\n",
    "mlp = MLP(units=units,activation=None, name='mlp_single_layer')\n",
    "opt = mlp(ipt)\n",
    "print(f'input shape is at {input_shape}: \\n{ipt}')\n",
    "print(f'\\nlayer1 trainable weights is at {input_shape}*{units}: \\n  {mlp.weights[0].numpy()}',)\n",
    "print(f'\\nlayer1 trainable bias is at {units}: \\n  {mlp.weights[1].numpy()}',)\n",
    "print(f'\\noutput channels is at {units}: \\n {mlp(ipt).numpy()}')\n",
    "print('\\nlayer1 non-trainable weights:\\n', mlp.non_trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: [ 0.40749726  0.02219561 -2.200683   -0.8212301  -0.36420798]\n",
      "\n",
      "final output:  [[0.00949545]]\n",
      "\n",
      "mlp1 layer weights and bias: \n",
      " [array([[-0.05752216, -0.01777615, -0.0443813 , -0.07142202,  0.03639229],\n",
      "       [ 0.05079982,  0.01979304, -0.01862826, -0.01179774,  0.03318491],\n",
      "       [-0.0077181 , -0.0696383 , -0.09101368, -0.00102651,  0.03367503],\n",
      "       [-0.07211521,  0.02843316, -0.05534288, -0.08601289,  0.10155316],\n",
      "       [-0.00349029, -0.05710344,  0.02195667,  0.02711194, -0.05647339]],\n",
      "      dtype=float32), array([ 0.01557087,  0.06733529, -0.02465851, -0.09773596,  0.00385595],\n",
      "      dtype=float32)]\n",
      "\n",
      "mlp2 layer weights and bias: \n",
      " [array([[-0.02640516],\n",
      "       [ 0.02764253],\n",
      "       [-0.0596687 ],\n",
      "       [-0.02553965],\n",
      "       [ 0.04818435]], dtype=float32), array([0.02116098], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- multiple MLP layers computation\n",
    "- without activation functions\n",
    "- with input_shape acquired by build() function\n",
    "'''\n",
    "tf.random.set_seed(15)\n",
    "input_shape=5\n",
    "ipt = tf.random.normal(shape=(input_shape,))\n",
    "mlp1 = MLP(units=5, activation= None, name='mlp_layer1')\n",
    "mlp2 = MLP(units=1, activation=None,  name='mlp_layer2')\n",
    "opt = mlp1(ipt)\n",
    "opt = mlp2(opt)\n",
    "print('input:', ipt.numpy())\n",
    "print('\\nfinal output: ', opt.numpy())\n",
    "print('\\nmlp1 layer weights and bias: \\n',[val.numpy() for val in mlp1.weights])\n",
    "print('\\nmlp2 layer weights and bias: \\n',[val.numpy() for val in mlp2.weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: \n",
      " [[0.08908382 0.09991569 0.10659099 0.10593536 0.09802915 0.08910162\n",
      "  0.09953129 0.11073688 0.09931216 0.10176301]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- multiple MLP with 'sigmoid' activation function\n",
    "'''\n",
    "tf.random.set_seed(15)\n",
    "input_shape=5\n",
    "ipt = tf.random.normal(shape=(input_shape,))\n",
    "opt = MLP(units = 256, activation = 'relu')(ipt)\n",
    "opt = MLP(units = 128, activation = 'relu')(ipt)\n",
    "opt = MLP(units =10, activation= 'softmax')(opt)\n",
    "print('output: \\n',opt.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input is: \n",
      " [[ 0.19024122 -0.4606145  -0.13381146 -2.562121   -0.72492903  0.613187\n",
      "  -0.15034355 -1.5807154   0.6264738   0.9457944 ]]\n",
      "\n",
      "default linear activation output: \n",
      " [[ 0.19024122 -0.4606145  -0.13381146 -2.562121   -0.72492903  0.613187\n",
      "  -0.15034355 -1.5807154   0.6264738   0.9457944 ]]\n",
      "\n",
      "sigmoid activation output: \n",
      " [[0.5474174  0.38684008 0.46659696 0.07161635 0.32630855 0.64866745\n",
      "  0.46248475 0.17069417 0.65168947 0.72026867]]\n",
      "\n",
      "tanh activation output: \n",
      " [[ 0.18797891 -0.43058494 -0.1330185  -0.98816895 -0.61995316  0.54636663\n",
      "  -0.14922096 -0.91871357  0.5556194   0.7378732 ]]\n",
      "\n",
      "relu activation output: \n",
      " [[0.19024122 0.         0.         0.         0.         0.613187\n",
      "  0.         0.         0.6264738  0.9457944 ]]\n",
      "\n",
      "softmax activation output: \n",
      " [[0.11373109 0.05932205 0.08225171 0.00725343 0.04554344 0.17360501\n",
      "  0.08090309 0.01935363 0.17592703 0.24210948]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tf activation functions\n",
    "- tf.keras.activations module\n",
    "'''\n",
    "ipt = tf.random.normal(shape=(1,10))\n",
    "print('input is: \\n', ipt.numpy())\n",
    "\n",
    "linear_res = tf.keras.activations.get(None)(ipt)\n",
    "print('\\ndefault linear activation output: \\n',linear_res.numpy())\n",
    "\n",
    "sigmoid_res = tf.keras.activations.get('sigmoid')(ipt)\n",
    "print('\\nsigmoid activation output: \\n',sigmoid_res.numpy())\n",
    "\n",
    "tanh_res = tf.keras.activations.tanh(ipt)\n",
    "print('\\ntanh activation output: \\n',tanh_res.numpy())\n",
    "\n",
    "relu_res = tf.keras.activations.relu(ipt)\n",
    "print('\\nrelu activation output: \\n',relu_res.numpy())\n",
    "\n",
    "softmax_res = tf.keras.activations.softmax(ipt)\n",
    "print('\\nsoftmax activation output: \\n',softmax_res.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf gradients and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy_dw: 2.0,\n",
      "dy_db: 1.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- batch = 1\n",
    "- x is tf.constant and it is not trainable so tape.gradient() on x is not working, \n",
    "- weight and bias is tf.Variable and it is trainalbe\n",
    "'''\n",
    "x = tf.constant(2.0) \n",
    "weight = tf.Variable(2.0)\n",
    "bias = tf.Variable(2.0)\n",
    "with tf.GradientTape(persistent = True, watch_accessed_variables=True) as tape:\n",
    "    y = weight * x + bias\n",
    "    \n",
    "dy_dw = tape.gradient(y, weight)\n",
    "dy_db = tape.gradient(y, bias)\n",
    "    \n",
    "print(f'dy_dw: {dy_dw},\\ndy_db: {dy_db}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[6.]\n",
      " [6.]\n",
      " [6.]], shape=(3, 1), dtype=float32)\n",
      "dy_dw: 6.0,\n",
      "dy_db: 3.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- btach = 3\n",
    "- x is tf.constant and it is not trainable so tape.gradient() on x is not working, \n",
    "- weight and bias is tf.Variable and it is trainalbe\n",
    "'''\n",
    "x = tf.constant([[2],[2],[2]], dtype=tf.float32) \n",
    "weight = tf.Variable(2, dtype=tf.float32)\n",
    "bias = tf.Variable(2, dtype=tf.float32)\n",
    "with tf.GradientTape(persistent = True, watch_accessed_variables=True) as tape:\n",
    "    y = weight * x + bias\n",
    "    \n",
    "dy_dw = tape.gradient(y, weight)\n",
    "dy_db = tape.gradient(y, bias)\n",
    "\n",
    "print(y)\n",
    "print(f'dy_dw: {dy_dw},\\ndy_db: {dy_db}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy_dw: 48.0,\n",
      "dy_dw2:36.0,\n",
      "dy_db: 24.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- batch =1\n",
    "- gradients for nested calculation function \n",
    "'''\n",
    "x = tf.constant(2.0) \n",
    "weight = tf.Variable(2, dtype=tf.float32)\n",
    "weight2 = tf.Variable(2, dtype=tf.float32)\n",
    "bias = tf.Variable(2.0, dtype=tf.float32)\n",
    "\n",
    "def y1 (weight, bias, x):\n",
    "    return weight * x + bias\n",
    "\n",
    "def y1_square(x):\n",
    "    return weight2*x**2\n",
    "\n",
    "with tf.GradientTape(persistent = True, watch_accessed_variables=True) as tape:\n",
    "    y = y1_square(y1(weight, bias, x))   \n",
    "    \n",
    "dy_dw = tape.gradient(y, weight)\n",
    "dy_dw2 = tape.gradient(y, weight2)\n",
    "dy_db = tape.gradient(y, bias)\n",
    "    \n",
    "print(f'dy_dw: {dy_dw},\\ndy_dw2:{dy_dw2},\\ndy_db: {dy_db}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy_dw:\n",
      "[[1.]\n",
      " [2.]\n",
      " [3.]],\n",
      "dy_db:\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- btach = 1\n",
    "- gradients on linear combination function with 1-dim weights\n",
    "'''\n",
    "ipt = tf.Variable([[1,2,3]],dtype=tf.float32) \n",
    "w = tf.Variable([[1],[2],[3]],dtype=tf.float32)\n",
    "b = tf.Variable([1],dtype=tf.float32)\n",
    "\n",
    "def linear_combination(ipt,weight,bias):\n",
    "    return tf.matmul(ipt,weight)+bias\n",
    "\n",
    "with tf.GradientTape(persistent = True, watch_accessed_variables=True) as tape:\n",
    "    y = linear_combination(ipt,w,b)\n",
    "dy_dw = tape.gradient(y, w)\n",
    "dy_db = tape.gradient(y, b)\n",
    "    \n",
    "print(f'dy_dw:\\n{dy_dw},\\ndy_db:\\n{dy_db}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy_dw:\n",
      "[[ 4.0688735e-01  1.2480556e-02  1.3418210e-01]\n",
      " [-1.7811962e+00 -5.4635078e-02 -5.8739763e-01]\n",
      " [ 1.2863358e+00  3.9456099e-02  4.2420402e-01]\n",
      " [-9.7072709e-01 -2.9775353e-02 -3.2012349e-01]\n",
      " [-8.9167268e-04 -2.7350497e-05 -2.9405317e-04]],\n",
      "dy_db:\n",
      "[[0.9328666  0.02861405 0.30763796]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- multiple inputs on a logistic regression calculation with multiple outpus\n",
    "- binary_cross_entropy for the loss\n",
    "'''\n",
    "tf.random.set_seed(2)\n",
    "init = tf.random_normal_initializer(mean=0.0, stddev=1)\n",
    "ipt = tf.Variable(initial_value=init(shape=(1,5),dtype=tf.float32)) \n",
    "w = tf.Variable(initial_value= init(shape=(5,3),dtype=tf.float32))\n",
    "b = tf.Variable(initial_value= init(shape=(1,3),dtype=tf.float32))\n",
    "target = tf.Variable(initial_value = init(shape=(1,3),dtype= tf.float32))\n",
    "\n",
    "def logistic_regression(ipt,weight,bias):\n",
    "    return tf.keras.activations.sigmoid(tf.matmul(ipt,weight)+bias)\n",
    "\n",
    "def cross_entropy_loss(label,predict):\n",
    "    loss =  tf.keras.losses.BinaryCrossentropy()\n",
    "    return loss(label,predict)\n",
    "\n",
    "with tf.GradientTape(persistent = True, watch_accessed_variables=True) as tape:\n",
    "    loss = cross_entropy_loss(target,logistic_regression(ipt,w,b))\n",
    "dy_dw = tape.gradient(loss, w)\n",
    "dy_db = tape.gradient(loss, b)\n",
    "    \n",
    "print(f'dy_dw:\\n{dy_dw},\\ndy_db:\\n{dy_db}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.keras custom model training I\n",
    "### - training a small dataset with stochastic gradient decent\n",
    "### - loss function is mean least square\n",
    "### - MLP model to simulate a linear combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- MLP layer\n",
    "'''\n",
    "class MLP(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=3, activation=None, trainable=True, name=None, dtype=tf.float32, **kwargs):\n",
    "        super(MLP, self).__init__( name=name, trainable = trainable, dtype=dtype, **kwargs)\n",
    "        self.units = units\n",
    "        self.__activation_name = activation\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),initializer='random_normal',)\n",
    "        self.b = self.add_weight(shape=(self.units,),initializer='random_normal')\n",
    "        \n",
    "    def get_config(self):\n",
    "        config_dic ={\n",
    "            'units':self.units,\n",
    "            'activation': self.__activation_name, \n",
    "            'trainable_weights & bias':self.trainable_weights,\n",
    "            'non-trainable_weights & bias':self.non_trainable_weights,\n",
    "        }\n",
    "        config = super(MLP,self).get_config()\n",
    "        config.update(config_dic)\n",
    "        return config\n",
    "    \n",
    "    def call(self, inputs, training = None):\n",
    "        if tf.rank(inputs)==1:\n",
    "            inputs = tf.keras.backend.expand_dims(inputs, axis=0)\n",
    "        linear_combination = tf.matmul(inputs, self.w)+self.b \n",
    "        return self.activation(linear_combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- subclass model\n",
    "'''\n",
    "class MLP_Model(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MLP_Model, self).__init__(self, **kwargs)\n",
    "        self.mlp1 = MLP(5)\n",
    "        self.mlp2 = MLP(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        opt = self.mlp1(inputs)\n",
    "        opt = self.mlp2(opt)\n",
    "        return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "- generate random data \n",
    "'''\n",
    "# define random data\n",
    "w = 6\n",
    "b = 1\n",
    "batch = 1000\n",
    "inputs  = tf.random.normal(shape=(batch,1), mean=0, stddev=1)\n",
    "noise   = tf.random.normal(shape=(batch,1), mean=0, stddev=1)\n",
    "outputs = inputs * w + b + noise\n",
    "\n",
    "# plotting \n",
    "model = MLP_Model()\n",
    "plt.scatter(inputs,outputs, c='b')\n",
    "plt.scatter(inputs, model(inputs), c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final epoch: 50, loss: 0.9573383331298828\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "- custom model training with SGD and mean square loss function\n",
    "'''\n",
    "# define model\n",
    "model = MLP_Model()\n",
    "\n",
    "# define loss\n",
    "mean_square_loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# gradient function -> return loss -> return gradients\n",
    "def get_grad(target, model, data):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = mean_square_loss(target, model(data))\n",
    "    return loss, tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "# define optimizer\n",
    "SGD = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# training process\n",
    "num_epochs = 50\n",
    "for _ in range(num_epochs):\n",
    "    loss, grad = get_grad(outputs, model, inputs)\n",
    "    SGD.apply_gradients(zip(grad, model.trainable_variables))\n",
    "print(f'final epoch: {SGD.iterations.numpy()}, loss: {loss}')\n",
    "\n",
    "# plotting \n",
    "plt.scatter(inputs,outputs, c='b')\n",
    "plt.scatter(inputs, model(inputs), c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.keras custom model training II\n",
    "## - training model a formal dataset with batches \n",
    "## - loss function is categorial cross entropy\n",
    "## - optimizer is SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>120</th>\n",
       "      <th>4</th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   120    4  setosa  versicolor  virginica\n",
       "0  6.4  2.8     5.6         2.2          2\n",
       "1  5.0  2.3     3.3         1.0          1\n",
       "2  4.9  2.5     4.5         1.7          2\n",
       "3  4.9  3.1     1.5         0.1          0\n",
       "4  5.7  3.8     1.7         0.3          0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tensorflow built in dataset\n",
    "train_dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\"\n",
    "train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),\n",
    "                                           origin=train_dataset_url)\n",
    "\n",
    "data = pd.read_csv(train_dataset_url)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120 entries, 0 to 119\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   120         120 non-null    float64\n",
      " 1   4           120 non-null    float64\n",
      " 2   setosa      120 non-null    float64\n",
      " 3   versicolor  120 non-null    float64\n",
      " 4   virginica   120 non-null    int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 4.8 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "Label: species\n",
      "Label classes:['Iris setosa', 'Iris versicolor', 'Iris virginica']\n"
     ]
    }
   ],
   "source": [
    "# column order in CSV file\n",
    "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "class_names = ['Iris setosa', 'Iris versicolor', 'Iris virginica']\n",
    "\n",
    "feature_names = column_names[:-1]\n",
    "label_name = column_names[-1]\n",
    "\n",
    "print(\"Features: {}\".format(feature_names))\n",
    "print(\"Label: {}\".format(label_name))\n",
    "print(f\"Label classes:{class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  species\n",
       "0           6.4          2.8           5.6          2.2        2\n",
       "1           5.0          2.3           3.3          1.0        1\n",
       "2           4.9          2.5           4.5          1.7        2\n",
       "3           4.9          3.1           1.5          0.1        0\n",
       "4           5.7          3.8           1.7          0.3        0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename the dataset column name\n",
    "data = data.rename(columns={'120': \"sepal_length\", '4': \"sepal_width\", 'setosa': \"petal_length\",'versicolor':'petal_width','virginica':'species'})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAFHCAYAAADgNUZ2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de3yNd4LH8W+SIxchhARbsl0VhmFXy1yathJNEDWuiTaqZWsMBtXdWFSL0CUlqoMoKrQuSYtEjaaWBEFLO6gp4zJtdTvLugxy0iByJ9k/vHpWGsmJSH5Hjs/79fJ6nfNczvPN4/fK9zzPec4Tl9LS0lIBAABjXB0dAACABw3lCwCAYZQvAACGUb4AABhG+QIAYBjlCwCAYZQvYMfPfvYzXbx48a7WCQ0N1eHDh+9qnalTp2rZsmV3nHf58mVNmTJFwcHBCg8P18CBA7V58+Z7yni3SktLtWbNGoWHhyssLEw9e/bUrFmzdP369VrZXlFRkbZs2VIrrw04GuUL3Ofy8vI0bNgwtWzZUhkZGUpPT9f8+fP1zjvvKDk52ViOhQsXauvWrVq9erUyMjK0ZcsWFRQUaPTo0aqN2wX89a9/pXzhtChfoJoKCws1adIkhYeHKzQ0VHFxcWXmHzhwQAMHDlRISIji4+Nt0zMyMtSvXz/17t1bo0ePVmZmZqXb2bx5s3x9ffVv//ZvqlevniSpXbt2Wrp0qbp27Vpu+YSEBIWHh6tHjx4aM2aMrl27Jkk6deqUoqKi1KdPH/Xq1UtJSUmVTr9ddna21q5dq7i4OD300EOSJG9vb82aNUu/+93vVFpaqsLCQsXExCg8PFy9e/dWXFycbt68Kan8kfmPz//3f/9XTz31lJKSktS3b1899dRT2rp1q6xWq15++WUdPXpUQ4cOtft/AdQ1lC9QTevXr1d2dra2b9+uP/7xj9q8eXOZU81//etf9dFHH+mjjz7SunXr9N133+nSpUt67bXXtHjxYqWlpalr166aPXt2pdv58ssv1b1793LTO3TooDZt2pSZ9vXXX+u9997Tpk2btGPHDhUWFtrK9J133tGQIUO0bds2bdiwQX/6059UVFRU4fTbHTt2TC1atCi3PU9PT4WGhsrV1VVr167VxYsXtW3bNqWmpurw4cP65JNPKv3ZXF1dlZ2dLRcXF23dulXTpk3TokWL5Ofnp4kTJ+rRRx/Vhx9+WOlrAHUR5QtU00svvaTly5fL1dVVjRo1Utu2bXXu3Dnb/H79+snNzU1+fn7q2rWrjh49qn379qlz58565JFHJEnPP/+8du/eXelp25ycHDVt2rRKmTp06KDPPvtMDRs2lKurq7p06aKzZ89Kkpo1a6b09HSdOHFCvr6+Wrp0qdzd3Sucfrtr166pSZMmlW577969ioiIkJubm9zd3fXMM8/o888/t5v5xo0bGjRokCSpU6dOtf7ZNXA/sDg6AFBXff/994qLi9OZM2fk4uKiixcvKiIiwjbf19fX9rhhw4a6evWqSkpK9NVXX6l37962eQ0aNFB2dnaF2/H19dWlS5eqlOn69et68803dfToUZWUlOjKlSu2o+bJkydr2bJlmjhxovLz8zVu3Dg9//zzFU6/2wxZWVlq3Lix7XmjRo30ww8/2M3s5uam+vXrS5JcXFxUUlJSpZ8VqMs48gWq6Y033lBgYKC2b9+utLQ0dejQocz8q1evlnncqFEj+fn5KSgoSGlpabZ/Bw4cqPSo8pe//KXS09PLHR1/9dVXSk1NLTNtzZo1Onv2rFJSUpSWlqaoqCjbPA8PD0VHR2vHjh1asWKFFi9erDNnzlQ4/XaPPvqoMjMzdfz48TLTi4uLtXDhQuXn56tp06Zl3kRkZ2fLz89P0q3Tyz/mr62ro4G6hPIFqunatWvq2LGjXF1dtWfPHp05c0a5ubm2+Vu3blVJSYkuX76sr776Sl27dtWTTz6pw4cP28rt2LFjevPNNyvdzqBBg1RSUqK33nrL9lnsqVOnNHnyZLm4uJTL9Mgjj8jb21tnzpzRnj17bJnGjBmj7777TpL0yCOPqEGDBpVOv12DBg00ZswYTZ8+3XZqPTc3VzExMTpx4oS8vLz09NNP6+OPP1ZJSYny8/P1X//1XwoJCZEkNW/eXH/7298kSdu3by+X+04sFouuX79eK1dSA47GaWegCoYNGyY3Nzfb8zlz5mjs2LGaPXu24uPj9cwzz2j8+PGKj49Xp06ddPPmTXXu3FkRERHKzs7WqFGjbJ/zxsbGasKECSoqKlL9+vU1ffr0Srft4eGhxMRELViwQH379pWrq6saNmyoqVOnqmfPnmWWjYqK0oQJE/T000+rU6dOmjZtml5++WUlJibqxRdf1H/8x3+ouLhYLi4uevHFF/Xwww9XOP2nXn75ZTVp0kRjx47VjRs3VFJSorCwMM2aNUuSNHz4cJ09e1bPPPOMJKlPnz62x9HR0YqJiVGrVq0UFhYmHx8f25XQFenatasWLFigkJAQ7dmzp8z+B+o6F/6eLwAAZnHaGQAAwyhfAAAMo3wBADCM8gUAwDAjVztnZuaY2IxT8PWtr+zsPEfHgBNhTKEmMZ6qzt+/YYXzOPK9z1gsfJ0CNYsxhZrEeKoZlC8AAIZRvgAAGEb5AgBgGOULAIBhlC8AAIZRvgAAGEb5AgBgGOULAIBhlC8AAIZRvgAAGEb5AgBgmJE/rHC/+u283Y6OUCe8PzXU0RHqBMZT1TGm8KDjyBcAAMMoXwAADKN8AQAwjPIFAMAwyhcAAMMoXwAADKN8AQAwjPIFAMCwKpVvQUGBwsLCtHnzZmVlZWnkyJF67rnn9Morr6ioqKi2MwIA4FSqVL7Lly9X48aNJUnz589XZGSkkpOT1bJlS6WmptZqQAAAnI3d8v3+++/1/fffq3v37pKkQ4cOKTT01q3hwsLCtH///loNCACAs7FbvvPnz9fUqVNtz3Nzc+Xp6SlJatKkiaxWa+2lAwDACVX6hxW2bNmiX/ziF2rVqpVtWr169WyPS0tL5eLiYncjvr71ZbG43UNMOJK/f0NHR4CTYUzVbfz/3btKy3fv3r06d+6cdu7cqYsXL8rd3V0eHh7Kz8+Xl5eXrFarmjVrZncj2dl5NRYY5mVm5jg6ApwMY6ru8vdvyP9fFVX2JqXS8l20aJHt8ZIlS9SyZUudPHlSGRkZ6tu3r3bu3KmQkJCaSwoAwAPgrr/nO2bMGG3cuFGRkZG6cuWK+vTpUxu5AABwWpUe+d5uwoQJtseJiYm1EgYAgAcBd7gCAMAwyhcAAMMoXwAADKN8AQAwjPIFAMAwyhcAAMMoXwAADKN8AQAwjPIFAMAwyhcAAMMoXwAADKN8AQAwjPIFAMAwyhcAAMMoXwAADKN8AQAwjPIFAMAwyhcAAMMoXwAADLM4OgAA4M5+O2+3oyPUGe9PDXV0hLvCkS8AAIZRvgAAGGb3tHN+fr6mTp2qrKws5eXlafz48friiy905MgReXt7S5JGjhyp7t2713ZWAACcgt3y3b17tzp16qRRo0bp/Pnz+u1vf6suXbooNjZWHTp0MJERAACnYrd8f/Ob39geX7x4Uc2bN1dubm6thgIAwJlV+WrnZ599VlarVQkJCZo3b57i4+OVk5Oj5s2ba8aMGWrcuHGF6/r61pfF4lYjgWGev39DR0eAk2FMoabVtTFV5fJNSUnRyZMnNXHiRL388stq06aNAgMDlZCQoPj4eMXExFS4bnZ2Xo2EhWNkZuY4OgKcDGMKNe1+HFOVvSGwe7Xz8ePHdeHCBUlSx44dVVJSol/84hcKDAyUJIWFhenUqVM1FBUAAOdnt3yPHDmitWvXSpKsVqtyc3M1bdo0nTt3TpJ06NAhtW3btnZTAgDgROyedh4yZIhee+01DR06VEVFRZo5c6Y8PT0VHR0tDw8PeXt7a+7cuSayAgDgFOyWr7u7u95+++1y01NSUmolEAAAzo47XAEAYBjlCwCAYZQvAACGUb4AABhG+QIAYBjlCwCAYZQvAACGUb4AABhG+QIAYBjlCwCAYZQvAACGUb4AABhG+QIAYBjlCwCAYZQvAACGUb4AABhG+QIAYBjlCwCAYZQvAACGUb4AABhG+QIAYBjlCwCAYRZ7C+Tn52vq1KnKyspSXl6exo8fr0cffVRTpkxRTk6OWrRooQULFsjd3d1EXgAA6jy7R767d+9Wp06dlJSUpCVLlmj+/PmaP3++IiMjlZycrJYtWyo1NdVEVgAAnILd8v3Nb36jUaNGSZIuXryo5s2b69ChQwoNDZUkhYWFaf/+/bWbEgAAJ2L3tPOPnn32WVmtViUkJOiFF16Qp6enJKlJkyayWq2VruvrW18Wi9u9JYXD+Ps3dHQEOBnGFGpaXRtTVS7flJQUnTx5UhMnTpSb2/8XaWlpqVxcXCpdNzs7r/oJ4XCZmTmOjgAnw5hCTbsfx1RlbwjsnnY+fvy4Lly4IEnq2LGjSkpK5OXlpfz8fEmS1WpVs2bNaigqAADOz275HjlyRGvXrpV0q2hzc3P19NNPKyMjQ5K0c+dOhYSE1G5KAACciN3yHTJkiKxWq4YOHarf//73mjlzpsaMGaONGzcqMjJSV65cUZ8+fUxkBQDAKdj9zNfd3V1vv/12uemJiYm1EggAAGfHHa4AADCM8gUAwDDKFwAAwyhfAAAMo3wBADCM8gUAwDDKFwAAwyhfAAAMo3wBADCM8gUAwDDKFwAAwyhfAAAMo3wBADCM8gUAwDDKFwAAwyhfAAAMo3wBADCM8gUAwDDKFwAAwyhfAAAMo3wBADCM8gUAwDBLVRb6wx/+oIMHD6q4uFijRo3S4cOHdeTIEXl7e0uSRo4cqe7du9dmTgAAnIbd8v3yyy/19ddfa+PGjbpy5Yr69++vJ598UrGxserQoYOJjAAAOBW7p50fe+wxLVq0SJLk4+Oj4uJi5eTk1HowAACcld0jX4vFIovl1mIpKSkKCQlRZmam4uPjlZOTo+bNm2vGjBlq3Lhxha/h61tfFotbzaWGUf7+DR0dAU6GMYWaVtfGVJU+85WkXbt2KTk5WatXr9bBgwfVunVrBQYGKiEhQfHx8YqJialw3ezsvBoJC8fIzORMB2oWYwo17X4cU5W9IajS1c779u3TsmXLtGrVKvn4+Khnz54KDAyUJIWFhenUqVM1kxQAgAeA3fLNycnRvHnzlJCQIF9fX0nSuHHjdO7cOUnSoUOH1LZt29pNCQCAE7F72nnbtm26evWqoqOjbdMiIiIUHR0tDw8PeXt7a+7cubUaEgAAZ2K3fKOiohQVFVVu+qBBg2olEAAAzo47XAEAYBjlCwCAYZQvAACGUb4AABhG+QIAYBjlCwCAYZQvAACGUb4AABhG+QIAYBjlCwCAYZQvAACGUb4AABhG+QIAYBjlCwCAYZQvAACGUb4AABhG+QIAYBjlCwCAYZQvAACGUb4AABhG+QIAYBjlCwCAYZaqLPSHP/xBBw8eVHFxsUaNGqVf/epXmjJlinJyctSiRQstWLBA7u7utZ0VAACnYLd8v/zyS3399dfauHGjrly5ov79+ysoKEiRkZHq06eP4uLilJqaqsGDB5vICwBAnWf3tPNjjz2mRYsWSZJ8fHxUXFysAwcOKDQ0VJIUFham/fv3125KAACciN0jX4vFIovl1mIpKSkKCQnR7t275enpKUlq0qSJrFZrpa/h61tfFotbDcSFI/j7N3R0BDgZxhRqWl0bU1X6zFeSdu3apeTkZK1evVr79u2zTS8tLZWLi0ul62Zn51U/IRwuMzPH0RHgZBhTqGn345iq7A1Bla523rdvn5YtW6ZVq1bJx8dH3t7eys/PlyRZrVY1a9asZpICAPAAsFu+OTk5mjdvnhISEuTr6ytJ6tatmzIyMiRJO3fuVEhISO2mBADAidg97bxt2zZdvXpV0dHRtmnz5s3T1KlTtXr1arVu3Vp9+vSp1ZAAADgTu+UbFRWlqKioctMTExNrJRAAAM6OO1wBAGAY5QsAgGGULwAAhlG+AAAYRvkCAGAY5QsAgGGULwAAhlG+AAAYRvkCAGAY5QsAgGGULwAAhlG+AAAYRvkCAGAY5QsAgGGULwAAhlG+AAAYRvkCAGAY5QsAgGGULwAAhlG+AAAYRvkCAGAY5QsAgGFVKt9Tp06pR48eSkpKkiTNnj1bERERGjZsmIYNG6a9e/fWZkYAAJyKxd4CeXl5mj17toKCgspMi42NVYcOHWo1HAAAzsjuka+7u7tWrlypZs2a2abl5ubWaigAAJyZ3SNfi8Uii6XsYrm5uYqPj1dOTo6aN2+uGTNmqHHjxhW+hq9vfVksbveeFg7h79/Q0RHgZBhTqGl1bUzZLd87GTJkiFq3bq3AwEAlJCQoPj5eMTExFS6fnZ1X7YBwvMzMHEdHgJNhTKGm3Y9jqrI3BNW62rlnz54KDAyUJIWFhenUqVPVSwYAwAOoWuU7btw4nTt3TpJ06NAhtW3btkZDAQDgzOyedj5x4oTi4uJ0/vx5WSwWpaen68UXX1R0dLQ8PDzk7e2tuXPnmsgKAIBTsFu+nTp1UmJiYrnp4eHhtRIIAABnxx2uAAAwjPIFAMAwyhcAAMMoXwAADKN8AQAwjPIFAMAwyhcAAMMoXwAADKN8AQAwjPIFAMAwyhcAAMMoXwAADKN8AQAwjPIFAMAwyhcAAMMoXwAADKN8AQAwjPIFAMAwyhcAAMMoXwAADKN8AQAwrErle+rUKfXo0UNJSUmSpKysLI0cOVLPPfecXnnlFRUVFdVqSAAAnInd8s3Ly9Ps2bMVFBRkmzZ//nxFRkYqOTlZLVu2VGpqaq2GBADAmdgtX3d3d61cuVLNmjWzTTt06JBCQ0MlSWFhYdq/f3/tJQQAwMlY7C5gschiKbtYbm6uPD09JUlNmjSR1WqtnXQAADghu+V7J/Xq1bM9Li0tlYuLS6XL+/rWl8XiVp1N4T7g79/Q0RHgZBhTqGl1bUxVq3y9vb2Vn58vLy8vWa3WMqek7yQ7O69a4XB/yMzMcXQEOBnGFGra/TimKntDUK2vGnXr1k0ZGRmSpJ07dyokJKR6yQAAeADZPfI9ceKE4uLidP78eVksFqWnp2vBggWaNGmSVq9erdatW6tPnz4msgIA4BTslm+nTp2UmJhYbvqdpgEAAPu4wxUAAIZRvgAAGEb5AgBgGOULAIBhlC8AAIZRvgAAGEb5AgBgGOULAIBhlC8AAIZRvgAAGEb5AgBgGOULAIBhlC8AAIZRvgAAGEb5AgBgGOULAIBhlC8AAIZRvgAAGEb5AgBgGOULAIBhlC8AAIZRvgAAGGapzkonTpzQuHHj9PDDD0uS2rVrpxkzZtRoMAAAnFW1yjcvL0/h4eGaNm1aTecBAMDpVeu0c25ubk3nAADggVHtI98///nPGjFihIqLizV+/HgFBQVVuLyvb31ZLG7VDgnH8vdv6OgIcDKMKdS0ujamqlW+7du315gxYxQeHq4zZ87opZdeUnp6utzd3e+4fHZ23j2FhGNlZuY4OgKcDGMKNe1+HFOVvSGoVvm2adNGbdq0kSQ9/PDD8vPz06VLlxQQEFC9hAAAPECq9ZnvH//4R61Zs0aSlJWVpaysLDVv3rwmcwEA4LSqdeTbo0cPTZ48WTt27NCNGzc0c+bMCk85AwCAsqpVvg0bNtS7775b01kAAHggcIcrAAAMo3wBADCM8gUAwDDKFwAAwyhfAAAMo3wBADCM8gUAwDDKFwAAwyhfAAAMo3wBADCM8gUAwDDKFwAAwyhfAAAMo3wBADCM8gUAwDDKFwAAwyhfAAAMo3wBADCM8gUAwDDKFwAAwyhfAAAMo3wBADCs2uW7ePFiDRkyRBERETp+/HhNZgIAwKlVq3wPHDig48ePa8OGDZo3b57mzZtX07kAAHBa1SrfgwcPKiwsTJLUrl07Xb58Wfn5+TUaDAAAZ2WpzkqZmZlq37697XmTJk1ktVoVEBBwx+X9/RtWL10t++TtAY6OACfCeEJNY0w5r2od+darV6/M89LSUrm4uNRIIAAAnF21ytff319ZWVm25z/88IP8/PxqLBQAAM6sWuUbHBysjIwMSdLJkycVEBAgT0/PGg0GAICzqtZnvp06dVL79u01aNAgubm5KTY2tqZzAQDgtFxKS0tLHR0CAIAHCXe4AgDAMMoXAADDKF8AAAyjfB2ssLBQ58+f14ULF1RUVOToOHBSFy9edHQEOJlr1645OkKdxgVXDnLy5EnNnTtXmZmZ8vX1VWlpqX744Qf94z/+o6ZOnaq2bds6OiLqmLS0NM2dO1c3btxQcHCwpk2bpgYNGkiShg8frnXr1jk4IZwJY+reVOurRrh3MTExmjdvXrmSPXnypKZNm6bk5GQHJUNdtWrVKqWmpsrHx0fJyckaMWKE3nvvPfn4+Ij32KiODz74oMJ5ly5dMpjE+VC+DuLu7n7Ho9uOHTvqxo0bDkiEus5isahRo0aSpKioKDVt2lQjRozQypUruf0rqmXNmjUKCgpSs2bNys3j99S9oXwdJCgoSKNHj1ZYWJiaNGkiScrKylJGRoaefPJJB6dDXfSrX/1KY8aM0aJFi+Tl5aUePXrIw8NDw4cP5/M5VMvSpUs1Z84cTZ8+Xe7u7mXmHTx40EGpnAOf+TrQwYMHdfDgQWVmZqpevXry9/dXcHCwOnbs6OhoqKMOHz6sLl26yNX1/6+lvH79urZt26bnnnvOgclQV+Xn58vDw6PMmJJufUTG76rqo3wBADCMrxoBAGAY5QsAgGGUL+7aunXrFBUVpQEDBmjZsmV3XObgwYN6/vnn7zgvNjZWJ06cqJVsn3zyiUpKSmrltX8qMzNTw4cPr/Dn/ClH7ZOqiI6OrtZXR4YNG6Yvvvjinrd/48YNxcTEaMiQIYqIiND7779/x+WWLFmihQsXVpjl5s2b1c6wefNmpaSkVHn5zMxMvfLKKzX6mnhwcLUz7sq3336rlJQUffTRR3JxcdGzzz6r0NBQtW/fvkrrl5aWatq0abWWb8mSJXrmmWfKXRxSGyZOnKhu3bppz5499/Q6tb1PflRSUlLhfqmo0ExtPzk5WQUFBdqwYYMKCwvVq1cv9ezZUwEBAVV+3cTExHvKFxERcVeZ/f39FR8ff9evCUiUL+7Snj171KNHD9vXDsLCwpSRkVFp+S5ZskQXLlzQpUuX9O///u966623NHbsWLVp00aTJk1SaWmpcnJyFBUVpaFDh5ZZd926ddqyZYvc3d3l5eWlt956S35+flq9erV27twpV1dXNW/eXHPmzNHKlSt15swZvfTSS3rnnXd09OhRLV26VB4eHvLw8FBsbKxatGihBQsW6MCBA5Kkf/iHf1BcXJxcXV316quvymq1Kj8/X71799bo0aMr3RfLly/XyZMnq1W+1d0ne/fuVWJiot577z1Jt65ujouLU0pKyh33SVZWlsaOHauOHTsqICBAPXv2VExMjCwWi65fv65x48apV69eCg0N1erVqxUQEKDY2Fh98803Kigo0EsvvaR+/frpL3/5i+bOnSuL5davjJiYGLVr167Mz7Rs2TLt2bNHbm5uCgwM1MyZM3Xp0qUy2x8/fvwd98fgwYM1cOBASZKHh4e8vLx0/fr1SvfhsGHD9POf/1zffPON3n//ff385z/XyZMndfjwYS1YsEDu7u4qLCzU66+/rq5du9rWi4uLU+PGjTVmzBhb7tzcXHl6eurGjRuKjo5W165dFRUVpYKCAk2fPl2vv/66Tp06pX/6p3+Si4uLgoKC9Pjjj2vo0KH67LPPNHnyZD300EP69ttv9be//U2RkZEaM2aMlixZYnvNTz/9VMuXL5eLi4sCAgI0Z84cXb9+XZMnT1ZRUZFyc3M1fPhw236Ac+O0M+7K5cuX5efnZ3vu5+dXpdOVZ86c0Xvvvad/+Zd/sU3bvn27WrduraSkJG3atOmOX9pfvHixVqxYoQ0bNuj3v/+9Ll68qGPHjunTTz9VUlKSkpKS1KxZM61fv952CnDNmjXy8PDQtGnTtHjxYiUlJSkkJEQLFy7U1atX9cEHH2jjxo3atGmTBgwYIKvVKqvVqieeeEIffPCBNmzYoBUrVtj95f/jrRurqzr75KmnntK3336rK1eu2JYfMGBAhftEkr7//nuNGjVK48ePV3JyskJDQ5WUlKTExERdvXq1zOunpaUpMzNTH3zwgVasWKGPP/5YN2/e1JQpU/Taa68pKSlJI0aM0H/+53+WWe/IkSNKS0uz7b+rV68qNTW13PYr4u7urvr160uSduzYoQYNGuhnP/uZ3X3o6emptWvXys3NzTZt7dq1GjFihD788EMtXrxYmZmZZdbp37+/0tLSyuzzAQMGlFkmNzdXTzzxhGJiYvT555/rv//7v7Vp0ya9+uqr+uyzz8odDbu5uen06dN69913tWbNGq1YsaLM/Pz8fM2YMUPLli3T+vXr5ePjoyNHjujy5cuKjIxUYmKili9frrlz59r9meEcOPLFXbv922mlpaVVOsXbuXPncndZeuKJJ7R69Wq9+uqrCg4OVlRUVLn1+vbtq5EjRyo8PFy9evVS27ZttWbNGp0+fVr/+q//KunWL7YuXbqUWe/06dPy9/fXQw89JOnWTU02btyoRo0aqWvXrnrhhRfUs2dPhYeHq1WrVsrJydHRo0e1ZcsWWSwWFRYW6sqVK/dcsJWpzj6xWCzq0aOHdu3apYiICGVkZGjz5s1KTU2tcJ/4+PioTZs2km6dqZgyZYrOnz+v4ODgcqdFjxw5ol/+8peSbr2xWrVqla5du6YffvhBnTt3lnRrX06aNKnMen/5y1/0+OOP286IPP744zpx4oR+/etfl9m+PWlpaYqPj9eqVauqNK4ee+yxctPCw8P11ltv6dixY+revbt69+5dZn6HDh1UVFSks2fPqqioSG5ubmrXrp3S09Nty5SWltr233fffaeuXbvazijc/mbpdr/+9a8lSQ899JByc3PLfP58+vRp+fn52W6oM336dEnS3//+d6WnpyspKUmurq62N1VwfpQv7kqLFi10+fJl2/PLly+rRYsW2rt3r+1U6MSJE8ut99O741Z+2IAAAAUFSURBVEhSu3bttGPHDh04cEBpaWlKSEjQ5s2byxzFvPHGGzpz5ow+/fRTjR07VhMnTpSLi4tCQ0MVExNTYc6ffn399jcJq1at0jfffKPPPvtMQ4cO1cKFC/XFF1+oqKhIH374oaRbd4uqjp/uhzuVw4+qu0/69eund999V61atVL79u3VpEmTCvfJuXPnymwnKChI27dv1xdffKFNmzYpJSVF7777rm1+aWlpuQvW7rQv7bl9f9/p57yTrVu36v3331diYqKaNm0q6dap/R8v6Fq5cmW5de702gMHDlRwcLD279+vd955R7t27dKMGTPKLNO3b1+lpaUpPz9f/fv3v2OeH1/7p5/7VnSrznr16pV5/tM3qXe6EHDx4sV6+OGHtXjxYl27ds32xgfOj9POuCs/fsZbWFiooqIi7dq1Sz179lT37t2VmJioxMTESgvndlu3btVXX32lkJAQW8nm5uba5l+9elVvv/22WrVqpeHDhysiIkJHjhxRly5dtG/fPtuy69ev1+HDhyXd+sVYUFCg1q1by2q16u9//7skad++fercubPOnj2rFStWqH379ho9erS6deum48eP6+rVq2rdurVcXFyUnp6uoqKiav2Jx+rsh7vZJ5LUpUsXnT17Vh9//LGtOCrbJ7dLTEzUuXPn1KtXL82cObPcMl26dNHnn38u6dadsQYPHiwvLy/5+fnp2LFjkm7ty0cffbTMeo899pgOHjyo4uJilZaWav/+/bYj5ar4n//5Hy1dulTvv/++rXglaezYsbb96enpWaXXio+PV3Fxsfr376/o6Og77oe+fftqz5492r17t/r27Vvp67Vu3VrHjh1TaWmpLl++rOPHj1f557r9NaxWq+1PO86dO1e7du3SlStX1Lp1a0lSamqqXF1d+dOiDwiOfHFX2rRpo0GDBmnw4MFydXXV4MGDq3xK8acCAwM1a9YsLVu2TAUFBRo3bpx8fHxs8xs1aqSSkhJFRUXJy8tLkvTmm28qICBAL7zwgoYNGyZ3d3f5+/urX79+kqRu3brp+eef19KlSxUbG6tXXnlF7u7uatCggWJjY9WoUSOdPn1aUVFRqlevnurXr69Jkybp0qVLmjhxov70pz8pLCxMAwYM0NSpU5WcnKxhw4ZpzZo1ZY4+L1y4oFdffVXXrl3TuXPnNGzYMIWEhOh3v/vdPexd+/tEuvUGIzw8XOvXr9esWbMkSf/8z/98x33y09OYgYGBmjFjhjw9PVVQUKDXX3+9zPzevXvrz3/+s4YMGaLi4mKNGDFC7u7uiouL05tvvimLxSI3Nzfbdn/UuXNn9e7dWy+88IJcXV3VsWNH9e3bVxcuXCizXGZmpmbPnl3uKuF169apsLBQEyZMsE0bOXKkunfvftf7sE2bNho3bpy8vb1VUFBwxzMxAQEBcnFxUdOmTe/4RwNuFxwcrI8//liRkZFq27atunTpUmYsVIWXl5diY2M1YcIEWSwWBQQEKCQkRPXr19cbb7yhzZs369lnn9UTTzyhKVOmaNGiRXf1+qh7uL0kYMfMmTP1xhtvODqG04iJiSl3wdb9LCcnRxkZGRo4cKBu3rypQYMGac6cORV+9gtUBUe+gB1PPfWUoyM4jaKiIoWGhjo6xl3x9vbWl19+qXXr1snV1VXBwcEUL+4ZR74AABjGBVcAABhG+QIAYBjlCwCAYZQvAACGUb4AABj2f/NLyA+M99xMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# count label classes\n",
    "data['species'].value_counts().sort_values(ascending=False).plot.bar(figsize=(8,5))\n",
    "plt.title('Label Class Count')\n",
    "plt.xlabel('0-Iris setosa, 1- Iris versicolor, 2-Iris virginica')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (OrderedDict([(sepal_length, (None,)), (sepal_width, (None,)), (petal_length, (None,)), (petal_width, (None,))]), (None,)), types: (OrderedDict([(sepal_length, tf.float32), (sepal_width, tf.float32), (petal_length, tf.float32), (petal_width, tf.float32)]), tf.int32)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- generate csv file data pipeline with tf.data.Dataset\n",
    "'''\n",
    "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "\n",
    "dataCSV = tf.data.experimental.make_csv_dataset(\n",
    "    file_pattern = train_dataset_fp, \n",
    "    batch_size = 32,\n",
    "    column_names= column_names, \n",
    "    label_name= 'species', \n",
    "    num_epochs = 1,\n",
    "    shuffle=True,\n",
    "    shuffle_buffer_size=100, \n",
    ")\n",
    "dataCSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal_length :\n",
      " [6.3 7.7 6.4 5.4 6.5 5.1 6.6 4.6 6.4 5.1 4.9 6.3 4.8 5.6 6.5 6.  5.8 6.1\n",
      " 5.7 4.9 5.  6.5 5.1 5.7 4.4 7.7 6.7 6.9 5.4 6.6 5.  6.1]\n",
      "sepal_width :\n",
      " [3.3 3.8 2.8 3.9 3.  3.8 2.9 3.6 3.2 3.8 2.4 2.5 3.1 2.5 3.  2.7 2.7 2.8\n",
      " 2.9 3.1 2.3 2.8 3.7 4.4 3.2 2.6 3.  3.1 3.4 3.  3.  2.9]\n",
      "petal_length :\n",
      " [4.7 6.7 5.6 1.3 5.8 1.9 4.6 1.  4.5 1.6 3.3 5.  1.6 3.9 5.5 5.1 4.1 4.\n",
      " 4.2 1.5 3.3 4.6 1.5 1.5 1.3 6.9 5.2 5.1 1.5 4.4 1.6 4.7]\n",
      "petal_width :\n",
      " [1.6 2.2 2.2 0.4 2.2 0.4 1.3 0.2 1.5 0.2 1.  1.9 0.2 1.1 1.8 1.6 1.  1.3\n",
      " 1.3 0.1 1.  1.5 0.4 0.4 0.2 2.3 2.3 2.3 0.4 1.4 0.2 1.4]\n",
      "labels:\n",
      " [1 2 2 0 2 0 1 0 1 0 1 2 0 1 2 1 1 1 1 0 1 1 0 0 0 2 2 2 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- print one batch data and labels\n",
    "'''\n",
    "features, labels = next(iter(dataCSV))\n",
    "for key, value in features.items():\n",
    "    print(key,\":\\n\",value.numpy())\n",
    "print('labels:\\n',labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- pack data into tensor \n",
    "'''\n",
    "def pack_rows (features, labels):\n",
    "    labels = tf.one_hot(labels, 3)\n",
    "    return tf.stack(list(features.values()),axis=1),labels\n",
    "\n",
    "packed_dataset = dataCSV.map(pack_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: \n",
      " tf.Tensor(\n",
      "[[6.8 3.  5.5 2.1]\n",
      " [6.  2.2 5.  1.5]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.4 3.9 1.3 0.4]], shape=(10, 4), dtype=float32)\n",
      "labels: \n",
      " tf.Tensor(\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]], shape=(10, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "features, labels = next(iter(packed_dataset))\n",
    "print('features: \\n',features[:10])\n",
    "print('labels: \\n',labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 739\n",
      "Trainable params: 739\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- build model with tf.kers API\n",
    "'''\n",
    "def get_model():\n",
    "    ipt = tf.keras.Input(shape=(4,))\n",
    "    opt = tf.keras.layers.Dense(32, activation='relu')(ipt)\n",
    "    opt = tf.keras.layers.Dense(16, activation='relu')(opt)\n",
    "    opt = tf.keras.layers.Dense(3, activation='softmax')(opt)\n",
    "    model = tf.keras.Model(ipt, opt)\n",
    "    return model\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- custom training process\n",
    "- batch_size -> 32\n",
    "- num_epochs -> 1 -> dataset will be repeated 1 time per epoch\n",
    "- loss -> categorical_cross_entroty\n",
    "- optimizer -> SGD\n",
    "'''\n",
    "loss_func = tf.keras.losses.CategoricalCrossentropy()# define optimizer\n",
    "SGD = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "def grad(target, model, data):\n",
    "    with tf.GradientTape() as tape: \n",
    "        loss = loss_func(target, model(data))\n",
    "    return loss, tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "def train_model(epochs, model, packed_dataset):\n",
    "    history_acc = []\n",
    "    history_loss = []\n",
    "    for _ in range(epochs):\n",
    "        step_loss = tf.keras.metrics.Mean()\n",
    "        step_acc = tf.keras.metrics.CategoricalCrossentropy()\n",
    "        '''\n",
    "        - train model for steps per epoch\n",
    "        '''\n",
    "        for x,y in packed_dataset: \n",
    "            '''\n",
    "            - calculate loss and grad\n",
    "            - update weights with grad\n",
    "            '''\n",
    "            loss, gradient = grad(y, model, x)\n",
    "            SGD.apply_gradients(zip(gradient, model.trainable_variables))\n",
    "            '''\n",
    "            - record each step loss and acc\n",
    "            '''\n",
    "            step_loss.update_state(loss)\n",
    "            step_acc.update_state(y, model(x, training = True))\n",
    "        '''\n",
    "        - record training loss and acc per epochs\n",
    "        '''\n",
    "        history_loss.append(step_loss.result())\n",
    "        history_acc.append(1-step_acc.result())\n",
    "        \n",
    "        print(f'Epoch: {_}, Loss: {step_loss.result()}, Acc: {1-step_acc.result()}')\n",
    "    return history_acc, history_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.128522276878357, Acc: -0.10642123222351074\n",
      "Epoch: 1, Loss: 1.0775713920593262, Acc: -0.05922091007232666\n",
      "Epoch: 2, Loss: 1.0375187397003174, Acc: -0.032527804374694824\n",
      "Epoch: 3, Loss: 1.0207926034927368, Acc: -0.009603738784790039\n",
      "Epoch: 4, Loss: 1.0010281801223755, Acc: 0.005670011043548584\n",
      "Epoch: 5, Loss: 0.9860828518867493, Acc: 0.019353628158569336\n",
      "Epoch: 6, Loss: 0.9724648594856262, Acc: 0.03332418203353882\n",
      "Epoch: 7, Loss: 0.9585701823234558, Acc: 0.046520888805389404\n",
      "Epoch: 8, Loss: 0.9444019198417664, Acc: 0.05979764461517334\n",
      "Epoch: 9, Loss: 0.9340907335281372, Acc: 0.07414305210113525\n",
      "Epoch: 10, Loss: 0.9222069978713989, Acc: 0.08772224187850952\n",
      "Epoch: 11, Loss: 0.9075404405593872, Acc: 0.09875106811523438\n",
      "Epoch: 12, Loss: 0.8952983617782593, Acc: 0.11201435327529907\n",
      "Epoch: 13, Loss: 0.8804292678833008, Acc: 0.12476593255996704\n",
      "Epoch: 14, Loss: 0.8760743141174316, Acc: 0.14447230100631714\n",
      "Epoch: 15, Loss: 0.8531907200813293, Acc: 0.15204256772994995\n",
      "Epoch: 16, Loss: 0.8397291302680969, Acc: 0.16617357730865479\n",
      "Epoch: 17, Loss: 0.8246561884880066, Acc: 0.18227171897888184\n",
      "Epoch: 18, Loss: 0.8117374777793884, Acc: 0.19450455904006958\n",
      "Epoch: 19, Loss: 0.8032983541488647, Acc: 0.2110949158668518\n",
      "Epoch: 20, Loss: 0.7831584215164185, Acc: 0.22372114658355713\n",
      "Epoch: 21, Loss: 0.7721139788627625, Acc: 0.23838913440704346\n",
      "Epoch: 22, Loss: 0.7602903246879578, Acc: 0.2543174624443054\n",
      "Epoch: 23, Loss: 0.7443038821220398, Acc: 0.26870691776275635\n",
      "Epoch: 24, Loss: 0.7275286912918091, Acc: 0.2806835174560547\n",
      "Epoch: 25, Loss: 0.716801106929779, Acc: 0.29197949171066284\n",
      "Epoch: 26, Loss: 0.6979310512542725, Acc: 0.30393338203430176\n",
      "Epoch: 27, Loss: 0.6908566951751709, Acc: 0.32236701250076294\n",
      "Epoch: 28, Loss: 0.6730027198791504, Acc: 0.3313406705856323\n",
      "Epoch: 29, Loss: 0.6625012159347534, Acc: 0.3408433198928833\n",
      "Epoch: 30, Loss: 0.6589752435684204, Acc: 0.3549215793609619\n",
      "Epoch: 31, Loss: 0.6394973397254944, Acc: 0.3633500337600708\n",
      "Epoch: 32, Loss: 0.6308779716491699, Acc: 0.37469661235809326\n",
      "Epoch: 33, Loss: 0.6279997229576111, Acc: 0.38563525676727295\n",
      "Epoch: 34, Loss: 0.6034414768218994, Acc: 0.39580899477005005\n",
      "Epoch: 35, Loss: 0.6000289916992188, Acc: 0.40542763471603394\n",
      "Epoch: 36, Loss: 0.5885556936264038, Acc: 0.41260606050491333\n",
      "Epoch: 37, Loss: 0.5841960310935974, Acc: 0.4225025177001953\n",
      "Epoch: 38, Loss: 0.5689521431922913, Acc: 0.43049782514572144\n",
      "Epoch: 39, Loss: 0.572537899017334, Acc: 0.43935078382492065\n",
      "Epoch: 40, Loss: 0.5603607296943665, Acc: 0.44556087255477905\n",
      "Epoch: 41, Loss: 0.5572614669799805, Acc: 0.4545777440071106\n",
      "Epoch: 42, Loss: 0.5439313650131226, Acc: 0.4655498266220093\n",
      "Epoch: 43, Loss: 0.5399596691131592, Acc: 0.46576982736587524\n",
      "Epoch: 44, Loss: 0.5273240804672241, Acc: 0.47307008504867554\n",
      "Epoch: 45, Loss: 0.5243372917175293, Acc: 0.478459894657135\n",
      "Epoch: 46, Loss: 0.5157133340835571, Acc: 0.48479145765304565\n",
      "Epoch: 47, Loss: 0.514266312122345, Acc: 0.4908336400985718\n",
      "Epoch: 48, Loss: 0.5056400299072266, Acc: 0.4967551827430725\n",
      "Epoch: 49, Loss: 0.4998339116573334, Acc: 0.5012382864952087\n",
      "Epoch: 50, Loss: 0.4979810118675232, Acc: 0.5089830160140991\n",
      "Epoch: 51, Loss: 0.48943856358528137, Acc: 0.5103033781051636\n",
      "Epoch: 52, Loss: 0.48092517256736755, Acc: 0.5152567028999329\n",
      "Epoch: 53, Loss: 0.47970980405807495, Acc: 0.5191136002540588\n",
      "Epoch: 54, Loss: 0.4795500636100769, Acc: 0.5265973806381226\n",
      "Epoch: 55, Loss: 0.4704115688800812, Acc: 0.5283938050270081\n",
      "Epoch: 56, Loss: 0.4670964777469635, Acc: 0.5332866907119751\n",
      "Epoch: 57, Loss: 0.4643650949001312, Acc: 0.5361006855964661\n",
      "Epoch: 58, Loss: 0.45938611030578613, Acc: 0.5409132242202759\n",
      "Epoch: 59, Loss: 0.4636020362377167, Acc: 0.5485612154006958\n",
      "Epoch: 60, Loss: 0.46729153394699097, Acc: 0.5522265434265137\n",
      "Epoch: 61, Loss: 0.45505189895629883, Acc: 0.5525037050247192\n",
      "Epoch: 62, Loss: 0.4543333053588867, Acc: 0.5569030046463013\n",
      "Epoch: 63, Loss: 0.4429933726787567, Acc: 0.5582484006881714\n",
      "Epoch: 64, Loss: 0.436459481716156, Acc: 0.5637400150299072\n",
      "Epoch: 65, Loss: 0.43714115023612976, Acc: 0.5651918053627014\n",
      "Epoch: 66, Loss: 0.4274294376373291, Acc: 0.5700043439865112\n",
      "Epoch: 67, Loss: 0.4273434281349182, Acc: 0.5732715725898743\n",
      "Epoch: 68, Loss: 0.42639049887657166, Acc: 0.5785326957702637\n",
      "Epoch: 69, Loss: 0.4243564307689667, Acc: 0.5778986215591431\n",
      "Epoch: 70, Loss: 0.42191368341445923, Acc: 0.5824084281921387\n",
      "Epoch: 71, Loss: 0.4184071123600006, Acc: 0.5845479965209961\n",
      "Epoch: 72, Loss: 0.42022398114204407, Acc: 0.5908632278442383\n",
      "Epoch: 73, Loss: 0.4081652760505676, Acc: 0.5903240442276001\n",
      "Epoch: 74, Loss: 0.4121148884296417, Acc: 0.6000114679336548\n",
      "Epoch: 75, Loss: 0.4042656421661377, Acc: 0.5969674587249756\n",
      "Epoch: 76, Loss: 0.4036339819431305, Acc: 0.5995883941650391\n",
      "Epoch: 77, Loss: 0.4004628360271454, Acc: 0.6017261743545532\n",
      "Epoch: 78, Loss: 0.4009653925895691, Acc: 0.6066875457763672\n",
      "Epoch: 79, Loss: 0.392261266708374, Acc: 0.6107834577560425\n",
      "Epoch: 80, Loss: 0.39184778928756714, Acc: 0.6101902723312378\n",
      "Epoch: 81, Loss: 0.3976248800754547, Acc: 0.6193989515304565\n",
      "Epoch: 82, Loss: 0.38815146684646606, Acc: 0.6174694299697876\n",
      "Epoch: 83, Loss: 0.3841530680656433, Acc: 0.6210868954658508\n",
      "Epoch: 84, Loss: 0.37810927629470825, Acc: 0.6222882270812988\n",
      "Epoch: 85, Loss: 0.3805425465106964, Acc: 0.6230396032333374\n",
      "Epoch: 86, Loss: 0.3804455101490021, Acc: 0.6291965246200562\n",
      "Epoch: 87, Loss: 0.377971351146698, Acc: 0.6317124366760254\n",
      "Epoch: 88, Loss: 0.38016560673713684, Acc: 0.6347881555557251\n",
      "Epoch: 89, Loss: 0.3764960467815399, Acc: 0.6384963393211365\n",
      "Epoch: 90, Loss: 0.3752782344818115, Acc: 0.6417392492294312\n",
      "Epoch: 91, Loss: 0.36516907811164856, Acc: 0.6413552761077881\n",
      "Epoch: 92, Loss: 0.36458703875541687, Acc: 0.6470342874526978\n",
      "Epoch: 93, Loss: 0.3558870553970337, Acc: 0.6436453461647034\n",
      "Epoch: 94, Loss: 0.3583405613899231, Acc: 0.6469340324401855\n",
      "Epoch: 95, Loss: 0.355849027633667, Acc: 0.6495591402053833\n",
      "Epoch: 96, Loss: 0.34466108679771423, Acc: 0.652650773525238\n",
      "Epoch: 97, Loss: 0.3446657359600067, Acc: 0.6534885168075562\n",
      "Epoch: 98, Loss: 0.3480687439441681, Acc: 0.6576757431030273\n",
      "Epoch: 99, Loss: 0.34112024307250977, Acc: 0.6590045690536499\n",
      "Epoch: 100, Loss: 0.3410522937774658, Acc: 0.6621279716491699\n",
      "Epoch: 101, Loss: 0.3405204117298126, Acc: 0.6629665493965149\n",
      "Epoch: 102, Loss: 0.33462679386138916, Acc: 0.6700749397277832\n",
      "Epoch: 103, Loss: 0.3348042666912079, Acc: 0.6717830896377563\n",
      "Epoch: 104, Loss: 0.3370121121406555, Acc: 0.672331690788269\n",
      "Epoch: 105, Loss: 0.3306729793548584, Acc: 0.6797775030136108\n",
      "Epoch: 106, Loss: 0.3295777142047882, Acc: 0.6762614250183105\n",
      "Epoch: 107, Loss: 0.319654643535614, Acc: 0.6780174970626831\n",
      "Epoch: 108, Loss: 0.32754653692245483, Acc: 0.6889224052429199\n",
      "Epoch: 109, Loss: 0.3200467824935913, Acc: 0.685545802116394\n",
      "Epoch: 110, Loss: 0.3216625154018402, Acc: 0.6850285530090332\n",
      "Epoch: 111, Loss: 0.3143242597579956, Acc: 0.6935824155807495\n",
      "Epoch: 112, Loss: 0.31835782527923584, Acc: 0.6913527846336365\n",
      "Epoch: 113, Loss: 0.31355613470077515, Acc: 0.692015528678894\n",
      "Epoch: 114, Loss: 0.3086566925048828, Acc: 0.6946027278900146\n",
      "Epoch: 115, Loss: 0.309818297624588, Acc: 0.6994651556015015\n",
      "Epoch: 116, Loss: 0.30742138624191284, Acc: 0.7030208110809326\n",
      "Epoch: 117, Loss: 0.30231720209121704, Acc: 0.7013723254203796\n",
      "Epoch: 118, Loss: 0.3025655150413513, Acc: 0.7051094174385071\n",
      "Epoch: 119, Loss: 0.30127355456352234, Acc: 0.7045450210571289\n",
      "Epoch: 120, Loss: 0.3028888702392578, Acc: 0.710045337677002\n",
      "Epoch: 121, Loss: 0.30939263105392456, Acc: 0.7208199501037598\n",
      "Epoch: 122, Loss: 0.28895241022109985, Acc: 0.712653636932373\n",
      "Epoch: 123, Loss: 0.2959534227848053, Acc: 0.7191553711891174\n",
      "Epoch: 124, Loss: 0.2856486141681671, Acc: 0.7150939702987671\n",
      "Epoch: 125, Loss: 0.2861311435699463, Acc: 0.7170826196670532\n",
      "Epoch: 126, Loss: 0.2813669443130493, Acc: 0.7215825319290161\n",
      "Epoch: 127, Loss: 0.2819766402244568, Acc: 0.7216544151306152\n",
      "Epoch: 128, Loss: 0.27645009756088257, Acc: 0.724928617477417\n",
      "Epoch: 129, Loss: 0.2801893353462219, Acc: 0.7297295928001404\n",
      "Epoch: 130, Loss: 0.28235888481140137, Acc: 0.7293896079063416\n",
      "Epoch: 131, Loss: 0.27603405714035034, Acc: 0.7327929139137268\n",
      "Epoch: 132, Loss: 0.2666296660900116, Acc: 0.7347089052200317\n",
      "Epoch: 133, Loss: 0.2706213593482971, Acc: 0.7348231673240662\n",
      "Epoch: 134, Loss: 0.2688506543636322, Acc: 0.7383673191070557\n",
      "Epoch: 135, Loss: 0.2700147330760956, Acc: 0.7414794564247131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 136, Loss: 0.2669355571269989, Acc: 0.7437849044799805\n",
      "Epoch: 137, Loss: 0.2635699212551117, Acc: 0.7483564615249634\n",
      "Epoch: 138, Loss: 0.2663692831993103, Acc: 0.7463513016700745\n",
      "Epoch: 139, Loss: 0.2548671364784241, Acc: 0.744114875793457\n",
      "Epoch: 140, Loss: 0.2538747489452362, Acc: 0.7489700317382812\n",
      "Epoch: 141, Loss: 0.260665625333786, Acc: 0.7561403512954712\n",
      "Epoch: 142, Loss: 0.2590346336364746, Acc: 0.7569783926010132\n",
      "Epoch: 143, Loss: 0.2516389489173889, Acc: 0.7522348165512085\n",
      "Epoch: 144, Loss: 0.25484007596969604, Acc: 0.7600542306900024\n",
      "Epoch: 145, Loss: 0.24231639504432678, Acc: 0.7554553747177124\n",
      "Epoch: 146, Loss: 0.24212025105953217, Acc: 0.761249303817749\n",
      "Epoch: 147, Loss: 0.2421475350856781, Acc: 0.7589137554168701\n",
      "Epoch: 148, Loss: 0.24273449182510376, Acc: 0.7631757259368896\n",
      "Epoch: 149, Loss: 0.2444058060646057, Acc: 0.7686772346496582\n",
      "Epoch: 150, Loss: 0.23803120851516724, Acc: 0.7714821696281433\n",
      "Epoch: 151, Loss: 0.24456891417503357, Acc: 0.769706130027771\n",
      "Epoch: 152, Loss: 0.22873790562152863, Acc: 0.7698966264724731\n",
      "Epoch: 153, Loss: 0.23468655347824097, Acc: 0.7695570588111877\n",
      "Epoch: 154, Loss: 0.23462720215320587, Acc: 0.7742030620574951\n",
      "Epoch: 155, Loss: 0.22937554121017456, Acc: 0.7728919386863708\n",
      "Epoch: 156, Loss: 0.23888668417930603, Acc: 0.7798681259155273\n",
      "Epoch: 157, Loss: 0.23270803689956665, Acc: 0.7772592902183533\n",
      "Epoch: 158, Loss: 0.2204802930355072, Acc: 0.7772143483161926\n",
      "Epoch: 159, Loss: 0.22283612191677094, Acc: 0.7781622409820557\n",
      "Epoch: 160, Loss: 0.22259114682674408, Acc: 0.782312273979187\n",
      "Epoch: 161, Loss: 0.217806875705719, Acc: 0.7824975252151489\n",
      "Epoch: 162, Loss: 0.21991391479969025, Acc: 0.7841092348098755\n",
      "Epoch: 163, Loss: 0.22507044672966003, Acc: 0.7912713289260864\n",
      "Epoch: 164, Loss: 0.2202257215976715, Acc: 0.7877470850944519\n",
      "Epoch: 165, Loss: 0.22549398243427277, Acc: 0.7964664101600647\n",
      "Epoch: 166, Loss: 0.21840734779834747, Acc: 0.7938696146011353\n",
      "Epoch: 167, Loss: 0.2109830379486084, Acc: 0.789624810218811\n",
      "Epoch: 168, Loss: 0.21340706944465637, Acc: 0.791366696357727\n",
      "Epoch: 169, Loss: 0.21441560983657837, Acc: 0.794217586517334\n",
      "Epoch: 170, Loss: 0.21157081425189972, Acc: 0.7961630821228027\n",
      "Epoch: 171, Loss: 0.2030746191740036, Acc: 0.7960205078125\n",
      "Epoch: 172, Loss: 0.20999886095523834, Acc: 0.8004710674285889\n",
      "Epoch: 173, Loss: 0.20746149122714996, Acc: 0.8013519644737244\n",
      "Epoch: 174, Loss: 0.21768739819526672, Acc: 0.8088762760162354\n",
      "Epoch: 175, Loss: 0.20280762016773224, Acc: 0.8046609163284302\n",
      "Epoch: 176, Loss: 0.19945649802684784, Acc: 0.801671028137207\n",
      "Epoch: 177, Loss: 0.21143236756324768, Acc: 0.8130354881286621\n",
      "Epoch: 178, Loss: 0.19490006566047668, Acc: 0.8088985085487366\n",
      "Epoch: 179, Loss: 0.20532885193824768, Acc: 0.8081813454627991\n",
      "Epoch: 180, Loss: 0.2009495198726654, Acc: 0.8108980655670166\n",
      "Epoch: 181, Loss: 0.20354652404785156, Acc: 0.8191704750061035\n",
      "Epoch: 182, Loss: 0.1967281997203827, Acc: 0.8125355243682861\n",
      "Epoch: 183, Loss: 0.20057442784309387, Acc: 0.8182398676872253\n",
      "Epoch: 184, Loss: 0.19268174469470978, Acc: 0.8113124370574951\n",
      "Epoch: 185, Loss: 0.1880769580602646, Acc: 0.8138713836669922\n",
      "Epoch: 186, Loss: 0.1916697919368744, Acc: 0.815363347530365\n",
      "Epoch: 187, Loss: 0.1910913735628128, Acc: 0.8200176954269409\n",
      "Epoch: 188, Loss: 0.18550178408622742, Acc: 0.8186987042427063\n",
      "Epoch: 189, Loss: 0.19437512755393982, Acc: 0.8248122334480286\n",
      "Epoch: 190, Loss: 0.18784013390541077, Acc: 0.8191774487495422\n",
      "Epoch: 191, Loss: 0.1869453638792038, Acc: 0.8229018449783325\n",
      "Epoch: 192, Loss: 0.19207170605659485, Acc: 0.8286019563674927\n",
      "Epoch: 193, Loss: 0.18686771392822266, Acc: 0.8210987448692322\n",
      "Epoch: 194, Loss: 0.18050001561641693, Acc: 0.8274978995323181\n",
      "Epoch: 195, Loss: 0.17934298515319824, Acc: 0.8233622908592224\n",
      "Epoch: 196, Loss: 0.18024682998657227, Acc: 0.8265882134437561\n",
      "Epoch: 197, Loss: 0.17470614612102509, Acc: 0.8268166780471802\n",
      "Epoch: 198, Loss: 0.1799389272928238, Acc: 0.8321650624275208\n",
      "Epoch: 199, Loss: 0.18963204324245453, Acc: 0.835503876209259\n",
      "Epoch: 200, Loss: 0.18371973931789398, Acc: 0.8332595229148865\n",
      "Epoch: 201, Loss: 0.18081554770469666, Acc: 0.834144115447998\n",
      "Epoch: 202, Loss: 0.17301104962825775, Acc: 0.8343350887298584\n",
      "Epoch: 203, Loss: 0.1702267974615097, Acc: 0.8319556713104248\n",
      "Epoch: 204, Loss: 0.17205879092216492, Acc: 0.83536696434021\n",
      "Epoch: 205, Loss: 0.17467764019966125, Acc: 0.8391029238700867\n",
      "Epoch: 206, Loss: 0.17891082167625427, Acc: 0.8396190404891968\n",
      "Epoch: 207, Loss: 0.16938547790050507, Acc: 0.8346335887908936\n",
      "Epoch: 208, Loss: 0.1659286469221115, Acc: 0.8362430930137634\n",
      "Epoch: 209, Loss: 0.17724564671516418, Acc: 0.8551115393638611\n",
      "Epoch: 210, Loss: 0.18353819847106934, Acc: 0.8445154428482056\n",
      "Epoch: 211, Loss: 0.17186854779720306, Acc: 0.8417942523956299\n",
      "Epoch: 212, Loss: 0.16087281703948975, Acc: 0.8385114669799805\n",
      "Epoch: 213, Loss: 0.1622740775346756, Acc: 0.8398292660713196\n",
      "Epoch: 214, Loss: 0.1742735505104065, Acc: 0.8565776348114014\n",
      "Epoch: 215, Loss: 0.16142874956130981, Acc: 0.8415976166725159\n",
      "Epoch: 216, Loss: 0.1565602719783783, Acc: 0.8427567481994629\n",
      "Epoch: 217, Loss: 0.1628122478723526, Acc: 0.8461513519287109\n",
      "Epoch: 218, Loss: 0.16377463936805725, Acc: 0.846772313117981\n",
      "Epoch: 219, Loss: 0.1568368524312973, Acc: 0.8582503795623779\n",
      "Epoch: 220, Loss: 0.17880627512931824, Acc: 0.8494604825973511\n",
      "Epoch: 221, Loss: 0.15283186733722687, Acc: 0.8478254079818726\n",
      "Epoch: 222, Loss: 0.1568465381860733, Acc: 0.8490760326385498\n",
      "Epoch: 223, Loss: 0.1579127460718155, Acc: 0.850077748298645\n",
      "Epoch: 224, Loss: 0.1533358097076416, Acc: 0.8513841032981873\n",
      "Epoch: 225, Loss: 0.1526053547859192, Acc: 0.8497874736785889\n",
      "Epoch: 226, Loss: 0.15429115295410156, Acc: 0.850960373878479\n",
      "Epoch: 227, Loss: 0.15316060185432434, Acc: 0.8565993905067444\n",
      "Epoch: 228, Loss: 0.1557498723268509, Acc: 0.8553135395050049\n",
      "Epoch: 229, Loss: 0.15418116748332977, Acc: 0.8537752032279968\n",
      "Epoch: 230, Loss: 0.15554369986057281, Acc: 0.8611032962799072\n",
      "Epoch: 231, Loss: 0.15061545372009277, Acc: 0.8601080179214478\n",
      "Epoch: 232, Loss: 0.1514003723859787, Acc: 0.8524996638298035\n",
      "Epoch: 233, Loss: 0.148391991853714, Acc: 0.8540515899658203\n",
      "Epoch: 234, Loss: 0.14829228818416595, Acc: 0.857059121131897\n",
      "Epoch: 235, Loss: 0.14812049269676208, Acc: 0.8577636480331421\n",
      "Epoch: 236, Loss: 0.146421417593956, Acc: 0.8596705794334412\n",
      "Epoch: 237, Loss: 0.14437271654605865, Acc: 0.8763895034790039\n",
      "Epoch: 238, Loss: 0.14952433109283447, Acc: 0.8676888346672058\n",
      "Epoch: 239, Loss: 0.15823905169963837, Acc: 0.8628091812133789\n",
      "Epoch: 240, Loss: 0.15227721631526947, Acc: 0.865082859992981\n",
      "Epoch: 241, Loss: 0.14628906548023224, Acc: 0.8596316576004028\n",
      "Epoch: 242, Loss: 0.1486137956380844, Acc: 0.8644354939460754\n",
      "Epoch: 243, Loss: 0.14217810332775116, Acc: 0.8603740334510803\n",
      "Epoch: 244, Loss: 0.137736514210701, Acc: 0.8610613346099854\n",
      "Epoch: 245, Loss: 0.15240584313869476, Acc: 0.8664038181304932\n",
      "Epoch: 246, Loss: 0.1434800773859024, Acc: 0.8647187948226929\n",
      "Epoch: 247, Loss: 0.1366504728794098, Acc: 0.8645524978637695\n",
      "Epoch: 248, Loss: 0.13391564786434174, Acc: 0.8675127625465393\n",
      "Epoch: 249, Loss: 0.13981939852237701, Acc: 0.8636941909790039\n",
      "Epoch: 250, Loss: 0.14534570276737213, Acc: 0.8716868162155151\n",
      "Epoch: 251, Loss: 0.13669493794441223, Acc: 0.8700471520423889\n",
      "Epoch: 252, Loss: 0.14328938722610474, Acc: 0.8693435192108154\n",
      "Epoch: 253, Loss: 0.14082950353622437, Acc: 0.869243323802948\n",
      "Epoch: 254, Loss: 0.14421431720256805, Acc: 0.8675655126571655\n",
      "Epoch: 255, Loss: 0.14326834678649902, Acc: 0.8856464624404907\n",
      "Epoch: 256, Loss: 0.1408306062221527, Acc: 0.8695364594459534\n",
      "Epoch: 257, Loss: 0.13344915211200714, Acc: 0.867811918258667\n",
      "Epoch: 258, Loss: 0.13366743922233582, Acc: 0.8751294612884521\n",
      "Epoch: 259, Loss: 0.13542351126670837, Acc: 0.8732194900512695\n",
      "Epoch: 260, Loss: 0.15187472105026245, Acc: 0.8941805958747864\n",
      "Epoch: 261, Loss: 0.13888156414031982, Acc: 0.8763481378555298\n",
      "Epoch: 262, Loss: 0.13706564903259277, Acc: 0.8720943927764893\n",
      "Epoch: 263, Loss: 0.1490815430879593, Acc: 0.8818011283874512\n",
      "Epoch: 264, Loss: 0.1281106173992157, Acc: 0.8755268454551697\n",
      "Epoch: 265, Loss: 0.13839857280254364, Acc: 0.8893147706985474\n",
      "Epoch: 266, Loss: 0.14189289510250092, Acc: 0.8807393908500671\n",
      "Epoch: 267, Loss: 0.12449327111244202, Acc: 0.8783560991287231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 268, Loss: 0.12955304980278015, Acc: 0.8728286623954773\n",
      "Epoch: 269, Loss: 0.12986253201961517, Acc: 0.8742532730102539\n",
      "Epoch: 270, Loss: 0.12809553742408752, Acc: 0.8802440166473389\n",
      "Epoch: 271, Loss: 0.1592806726694107, Acc: 0.8899646401405334\n",
      "Epoch: 272, Loss: 0.1555519700050354, Acc: 0.8915351629257202\n",
      "Epoch: 273, Loss: 0.13085612654685974, Acc: 0.8776606321334839\n",
      "Epoch: 274, Loss: 0.12511849403381348, Acc: 0.8764905333518982\n",
      "Epoch: 275, Loss: 0.13403484225273132, Acc: 0.8762674927711487\n",
      "Epoch: 276, Loss: 0.13071230053901672, Acc: 0.8791846632957458\n",
      "Epoch: 277, Loss: 0.13510948419570923, Acc: 0.8806577920913696\n",
      "Epoch: 278, Loss: 0.1289755254983902, Acc: 0.8775029182434082\n",
      "Epoch: 279, Loss: 0.131845623254776, Acc: 0.8854120969772339\n",
      "Epoch: 280, Loss: 0.13561271131038666, Acc: 0.8817256689071655\n",
      "Epoch: 281, Loss: 0.12076453864574432, Acc: 0.8872331380844116\n",
      "Epoch: 282, Loss: 0.14599505066871643, Acc: 0.8955195546150208\n",
      "Epoch: 283, Loss: 0.12447516620159149, Acc: 0.8832368850708008\n",
      "Epoch: 284, Loss: 0.12362957745790482, Acc: 0.8793209791183472\n",
      "Epoch: 285, Loss: 0.14262473583221436, Acc: 0.9078638553619385\n",
      "Epoch: 286, Loss: 0.1416042298078537, Acc: 0.8863116502761841\n",
      "Epoch: 287, Loss: 0.12197571247816086, Acc: 0.8809001445770264\n",
      "Epoch: 288, Loss: 0.12587550282478333, Acc: 0.8805050849914551\n",
      "Epoch: 289, Loss: 0.1388019174337387, Acc: 0.8947439193725586\n",
      "Epoch: 290, Loss: 0.11907368153333664, Acc: 0.8809831142425537\n",
      "Epoch: 291, Loss: 0.13564004004001617, Acc: 0.8985605239868164\n",
      "Epoch: 292, Loss: 0.1320917308330536, Acc: 0.8847761154174805\n",
      "Epoch: 293, Loss: 0.12022890150547028, Acc: 0.882794201374054\n",
      "Epoch: 294, Loss: 0.12305009365081787, Acc: 0.8838866949081421\n",
      "Epoch: 295, Loss: 0.12353140115737915, Acc: 0.8853503465652466\n",
      "Epoch: 296, Loss: 0.1235717236995697, Acc: 0.8860459327697754\n",
      "Epoch: 297, Loss: 0.1314687728881836, Acc: 0.897733211517334\n",
      "Epoch: 298, Loss: 0.13756342232227325, Acc: 0.8918325901031494\n",
      "Epoch: 299, Loss: 0.12676310539245605, Acc: 0.885201096534729\n",
      "Epoch: 300, Loss: 0.1281079351902008, Acc: 0.8990429043769836\n",
      "Epoch: 301, Loss: 0.14188402891159058, Acc: 0.9047022461891174\n",
      "Epoch: 302, Loss: 0.11803436279296875, Acc: 0.8873255252838135\n",
      "Epoch: 303, Loss: 0.11887554824352264, Acc: 0.886978030204773\n",
      "Epoch: 304, Loss: 0.11642630398273468, Acc: 0.8897109031677246\n",
      "Epoch: 305, Loss: 0.11841689050197601, Acc: 0.8957881927490234\n",
      "Epoch: 306, Loss: 0.12308675050735474, Acc: 0.8881452083587646\n",
      "Epoch: 307, Loss: 0.131621316075325, Acc: 0.8982203006744385\n",
      "Epoch: 308, Loss: 0.13247822225093842, Acc: 0.8963330388069153\n",
      "Epoch: 309, Loss: 0.12481850385665894, Acc: 0.8928369283676147\n",
      "Epoch: 310, Loss: 0.11366743594408035, Acc: 0.8902890682220459\n",
      "Epoch: 311, Loss: 0.12157316505908966, Acc: 0.8948134779930115\n",
      "Epoch: 312, Loss: 0.11954604089260101, Acc: 0.8888569474220276\n",
      "Epoch: 313, Loss: 0.11748458445072174, Acc: 0.8930280208587646\n",
      "Epoch: 314, Loss: 0.11891341954469681, Acc: 0.8955557942390442\n",
      "Epoch: 315, Loss: 0.11467815190553665, Acc: 0.8920160531997681\n",
      "Epoch: 316, Loss: 0.1202126070857048, Acc: 0.8956661820411682\n",
      "Epoch: 317, Loss: 0.11304373294115067, Acc: 0.8895601034164429\n",
      "Epoch: 318, Loss: 0.13211797177791595, Acc: 0.9025754928588867\n",
      "Epoch: 319, Loss: 0.11560702323913574, Acc: 0.8955123424530029\n",
      "Epoch: 320, Loss: 0.11340094357728958, Acc: 0.891451358795166\n",
      "Epoch: 321, Loss: 0.11514705419540405, Acc: 0.891403317451477\n",
      "Epoch: 322, Loss: 0.11446056514978409, Acc: 0.8955599069595337\n",
      "Epoch: 323, Loss: 0.129055917263031, Acc: 0.9008635878562927\n",
      "Epoch: 324, Loss: 0.13133494555950165, Acc: 0.9010187387466431\n",
      "Epoch: 325, Loss: 0.11101245135068893, Acc: 0.8914711475372314\n",
      "Epoch: 326, Loss: 0.11747758090496063, Acc: 0.8935087323188782\n",
      "Epoch: 327, Loss: 0.11058493703603745, Acc: 0.8966536521911621\n",
      "Epoch: 328, Loss: 0.11362873017787933, Acc: 0.8970516324043274\n",
      "Epoch: 329, Loss: 0.11720243841409683, Acc: 0.9024286270141602\n",
      "Epoch: 330, Loss: 0.11415029317140579, Acc: 0.8916621208190918\n",
      "Epoch: 331, Loss: 0.1096416637301445, Acc: 0.8994063138961792\n",
      "Epoch: 332, Loss: 0.11088917404413223, Acc: 0.9012429714202881\n",
      "Epoch: 333, Loss: 0.11161785572767258, Acc: 0.8931605219841003\n",
      "Epoch: 334, Loss: 0.10946942865848541, Acc: 0.8943356871604919\n",
      "Epoch: 335, Loss: 0.10693151503801346, Acc: 0.8960781693458557\n",
      "Epoch: 336, Loss: 0.1192173957824707, Acc: 0.9031665325164795\n",
      "Epoch: 337, Loss: 0.11032828688621521, Acc: 0.8973028659820557\n",
      "Epoch: 338, Loss: 0.10973118990659714, Acc: 0.9030999541282654\n",
      "Epoch: 339, Loss: 0.11780858784914017, Acc: 0.8969663977622986\n",
      "Epoch: 340, Loss: 0.11571287363767624, Acc: 0.9013653993606567\n",
      "Epoch: 341, Loss: 0.11722743511199951, Acc: 0.9056478142738342\n",
      "Epoch: 342, Loss: 0.11192402988672256, Acc: 0.899273693561554\n",
      "Epoch: 343, Loss: 0.11342891305685043, Acc: 0.9095767736434937\n",
      "Epoch: 344, Loss: 0.11267764121294022, Acc: 0.8992886543273926\n",
      "Epoch: 345, Loss: 0.10663237422704697, Acc: 0.8974242210388184\n",
      "Epoch: 346, Loss: 0.10514486581087112, Acc: 0.8967053890228271\n",
      "Epoch: 347, Loss: 0.10599391162395477, Acc: 0.8995724320411682\n",
      "Epoch: 348, Loss: 0.10766176879405975, Acc: 0.9011144638061523\n",
      "Epoch: 349, Loss: 0.10307533293962479, Acc: 0.8969272375106812\n",
      "Epoch: 350, Loss: 0.1091296523809433, Acc: 0.902224600315094\n",
      "Epoch: 351, Loss: 0.10730951279401779, Acc: 0.8982352614402771\n",
      "Epoch: 352, Loss: 0.11003038287162781, Acc: 0.901305079460144\n",
      "Epoch: 353, Loss: 0.10504412651062012, Acc: 0.9023065567016602\n",
      "Epoch: 354, Loss: 0.11016199737787247, Acc: 0.9011858701705933\n",
      "Epoch: 355, Loss: 0.12421821057796478, Acc: 0.909805178642273\n",
      "Epoch: 356, Loss: 0.11717205494642258, Acc: 0.9036185145378113\n",
      "Epoch: 357, Loss: 0.10471907258033752, Acc: 0.900313138961792\n",
      "Epoch: 358, Loss: 0.10141262412071228, Acc: 0.8983370065689087\n",
      "Epoch: 359, Loss: 0.10613405704498291, Acc: 0.8998260498046875\n",
      "Epoch: 360, Loss: 0.12013785541057587, Acc: 0.9034481048583984\n",
      "Epoch: 361, Loss: 0.10918420553207397, Acc: 0.9034572243690491\n",
      "Epoch: 362, Loss: 0.11239796876907349, Acc: 0.9057238101959229\n",
      "Epoch: 363, Loss: 0.11768385767936707, Acc: 0.9061176776885986\n",
      "Epoch: 364, Loss: 0.11388594657182693, Acc: 0.9011228680610657\n",
      "Epoch: 365, Loss: 0.11640878021717072, Acc: 0.9048508405685425\n",
      "Epoch: 366, Loss: 0.1107431948184967, Acc: 0.9014100432395935\n",
      "Epoch: 367, Loss: 0.11000886559486389, Acc: 0.9084887504577637\n",
      "Epoch: 368, Loss: 0.10043469071388245, Acc: 0.9003426432609558\n",
      "Epoch: 369, Loss: 0.10563788563013077, Acc: 0.9058022499084473\n",
      "Epoch: 370, Loss: 0.103660449385643, Acc: 0.9055007696151733\n",
      "Epoch: 371, Loss: 0.09654448926448822, Acc: 0.901665985584259\n",
      "Epoch: 372, Loss: 0.10998426377773285, Acc: 0.9070721864700317\n",
      "Epoch: 373, Loss: 0.10185684263706207, Acc: 0.903253436088562\n",
      "Epoch: 374, Loss: 0.10957856476306915, Acc: 0.9079740643501282\n",
      "Epoch: 375, Loss: 0.10286283493041992, Acc: 0.9067652225494385\n",
      "Epoch: 376, Loss: 0.10625457018613815, Acc: 0.9038772583007812\n",
      "Epoch: 377, Loss: 0.09807117283344269, Acc: 0.9039628505706787\n",
      "Epoch: 378, Loss: 0.10379692167043686, Acc: 0.9083089828491211\n",
      "Epoch: 379, Loss: 0.098792664706707, Acc: 0.9092758297920227\n",
      "Epoch: 380, Loss: 0.11574910581111908, Acc: 0.9109954833984375\n",
      "Epoch: 381, Loss: 0.11788530647754669, Acc: 0.9090798497200012\n",
      "Epoch: 382, Loss: 0.10550948977470398, Acc: 0.9062055945396423\n",
      "Epoch: 383, Loss: 0.10043106228113174, Acc: 0.9041011333465576\n",
      "Epoch: 384, Loss: 0.11264069378376007, Acc: 0.9136060476303101\n",
      "Epoch: 385, Loss: 0.10004711151123047, Acc: 0.9067269563674927\n",
      "Epoch: 386, Loss: 0.0953679233789444, Acc: 0.9051879644393921\n",
      "Epoch: 387, Loss: 0.10191257297992706, Acc: 0.9056269526481628\n",
      "Epoch: 388, Loss: 0.1010647788643837, Acc: 0.9100056290626526\n",
      "Epoch: 389, Loss: 0.09506110846996307, Acc: 0.9075075387954712\n",
      "Epoch: 390, Loss: 0.09772715717554092, Acc: 0.9223225712776184\n",
      "Epoch: 391, Loss: 0.13805937767028809, Acc: 0.9297824501991272\n",
      "Epoch: 392, Loss: 0.10367503017187119, Acc: 0.9035347104072571\n",
      "Epoch: 393, Loss: 0.10600816458463669, Acc: 0.906914472579956\n",
      "Epoch: 394, Loss: 0.09611345827579498, Acc: 0.9055412411689758\n",
      "Epoch: 395, Loss: 0.09783962368965149, Acc: 0.9060361981391907\n",
      "Epoch: 396, Loss: 0.09931577742099762, Acc: 0.9090930819511414\n",
      "Epoch: 397, Loss: 0.10962679982185364, Acc: 0.9140061736106873\n",
      "Epoch: 398, Loss: 0.10730797052383423, Acc: 0.9084963798522949\n",
      "Epoch: 399, Loss: 0.10964080691337585, Acc: 0.9115337133407593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 400, Loss: 0.10915713757276535, Acc: 0.9089194536209106\n",
      "Epoch: 401, Loss: 0.09710361808538437, Acc: 0.909369945526123\n",
      "Epoch: 402, Loss: 0.09338495880365372, Acc: 0.910740852355957\n",
      "Epoch: 403, Loss: 0.0970333069562912, Acc: 0.9082837700843811\n",
      "Epoch: 404, Loss: 0.09376553446054459, Acc: 0.909294068813324\n",
      "Epoch: 405, Loss: 0.09642698615789413, Acc: 0.9067450761795044\n",
      "Epoch: 406, Loss: 0.09682288020849228, Acc: 0.9062542915344238\n",
      "Epoch: 407, Loss: 0.0980173796415329, Acc: 0.9077172875404358\n",
      "Epoch: 408, Loss: 0.09391982108354568, Acc: 0.9068710207939148\n",
      "Epoch: 409, Loss: 0.10856204479932785, Acc: 0.9139585494995117\n",
      "Epoch: 410, Loss: 0.10700410604476929, Acc: 0.9167183041572571\n",
      "Epoch: 411, Loss: 0.10083511471748352, Acc: 0.9069642424583435\n",
      "Epoch: 412, Loss: 0.10228688269853592, Acc: 0.9149388670921326\n",
      "Epoch: 413, Loss: 0.09578804671764374, Acc: 0.9104717969894409\n",
      "Epoch: 414, Loss: 0.09949935972690582, Acc: 0.9089251160621643\n",
      "Epoch: 415, Loss: 0.08965613692998886, Acc: 0.9081920981407166\n",
      "Epoch: 416, Loss: 0.10359928011894226, Acc: 0.9106106758117676\n",
      "Epoch: 417, Loss: 0.09993702173233032, Acc: 0.9117735028266907\n",
      "Epoch: 418, Loss: 0.0979224145412445, Acc: 0.9093262553215027\n",
      "Epoch: 419, Loss: 0.08966321498155594, Acc: 0.9076621532440186\n",
      "Epoch: 420, Loss: 0.09526155889034271, Acc: 0.9105507135391235\n",
      "Epoch: 421, Loss: 0.09714613109827042, Acc: 0.9121630191802979\n",
      "Epoch: 422, Loss: 0.10760320723056793, Acc: 0.9157187938690186\n",
      "Epoch: 423, Loss: 0.10311254858970642, Acc: 0.914560079574585\n",
      "Epoch: 424, Loss: 0.08984367549419403, Acc: 0.9088209867477417\n",
      "Epoch: 425, Loss: 0.09741317480802536, Acc: 0.9108738303184509\n",
      "Epoch: 426, Loss: 0.0964583307504654, Acc: 0.9127029180526733\n",
      "Epoch: 427, Loss: 0.10213442146778107, Acc: 0.910661518573761\n",
      "Epoch: 428, Loss: 0.09647656977176666, Acc: 0.9108450412750244\n",
      "Epoch: 429, Loss: 0.09449033439159393, Acc: 0.9152997732162476\n",
      "Epoch: 430, Loss: 0.11038395762443542, Acc: 0.9179290533065796\n",
      "Epoch: 431, Loss: 0.09820844978094101, Acc: 0.9126182198524475\n",
      "Epoch: 432, Loss: 0.08966325223445892, Acc: 0.910219132900238\n",
      "Epoch: 433, Loss: 0.11097581684589386, Acc: 0.9169918298721313\n",
      "Epoch: 434, Loss: 0.09947483241558075, Acc: 0.909598708152771\n",
      "Epoch: 435, Loss: 0.10555899143218994, Acc: 0.9187917113304138\n",
      "Epoch: 436, Loss: 0.09415490180253983, Acc: 0.9117900729179382\n",
      "Epoch: 437, Loss: 0.09367524087429047, Acc: 0.9135716557502747\n",
      "Epoch: 438, Loss: 0.09062373638153076, Acc: 0.909895658493042\n",
      "Epoch: 439, Loss: 0.09140048176050186, Acc: 0.9132040739059448\n",
      "Epoch: 440, Loss: 0.09572618454694748, Acc: 0.9116942882537842\n",
      "Epoch: 441, Loss: 0.10302478075027466, Acc: 0.9131262898445129\n",
      "Epoch: 442, Loss: 0.09162531793117523, Acc: 0.9148093461990356\n",
      "Epoch: 443, Loss: 0.10770800709724426, Acc: 0.9225645065307617\n",
      "Epoch: 444, Loss: 0.09803074598312378, Acc: 0.9128114581108093\n",
      "Epoch: 445, Loss: 0.10834053158760071, Acc: 0.9235191345214844\n",
      "Epoch: 446, Loss: 0.1345919668674469, Acc: 0.9267898201942444\n",
      "Epoch: 447, Loss: 0.09645034372806549, Acc: 0.9149566888809204\n",
      "Epoch: 448, Loss: 0.09681269526481628, Acc: 0.9169449210166931\n",
      "Epoch: 449, Loss: 0.08605338633060455, Acc: 0.9196439385414124\n",
      "Epoch: 450, Loss: 0.10411330312490463, Acc: 0.9131884574890137\n",
      "Epoch: 451, Loss: 0.10029962658882141, Acc: 0.9177906513214111\n",
      "Epoch: 452, Loss: 0.10142055153846741, Acc: 0.9201938509941101\n",
      "Epoch: 453, Loss: 0.10275925695896149, Acc: 0.9162490367889404\n",
      "Epoch: 454, Loss: 0.09186646342277527, Acc: 0.9124855995178223\n",
      "Epoch: 455, Loss: 0.09305773675441742, Acc: 0.9159404039382935\n",
      "Epoch: 456, Loss: 0.09366901963949203, Acc: 0.9162179231643677\n",
      "Epoch: 457, Loss: 0.0979764387011528, Acc: 0.9209365248680115\n",
      "Epoch: 458, Loss: 0.09564155340194702, Acc: 0.9146242141723633\n",
      "Epoch: 459, Loss: 0.09174373000860214, Acc: 0.913849413394928\n",
      "Epoch: 460, Loss: 0.09604011476039886, Acc: 0.9251500964164734\n",
      "Epoch: 461, Loss: 0.09342281520366669, Acc: 0.9127472639083862\n",
      "Epoch: 462, Loss: 0.09084906429052353, Acc: 0.9164857864379883\n",
      "Epoch: 463, Loss: 0.08479110896587372, Acc: 0.9128032326698303\n",
      "Epoch: 464, Loss: 0.08867983520030975, Acc: 0.918119490146637\n",
      "Epoch: 465, Loss: 0.09148874878883362, Acc: 0.9136399030685425\n",
      "Epoch: 466, Loss: 0.08620317280292511, Acc: 0.9164667129516602\n",
      "Epoch: 467, Loss: 0.09430080652236938, Acc: 0.9185072779655457\n",
      "Epoch: 468, Loss: 0.08970344066619873, Acc: 0.922585129737854\n",
      "Epoch: 469, Loss: 0.09964706003665924, Acc: 0.9202990531921387\n",
      "Epoch: 470, Loss: 0.09331848472356796, Acc: 0.9206634163856506\n",
      "Epoch: 471, Loss: 0.10904115438461304, Acc: 0.9233897924423218\n",
      "Epoch: 472, Loss: 0.10808706283569336, Acc: 0.9393788576126099\n",
      "Epoch: 473, Loss: 0.0856015756726265, Acc: 0.9153887033462524\n",
      "Epoch: 474, Loss: 0.12045294046401978, Acc: 0.9272173643112183\n",
      "Epoch: 475, Loss: 0.0878743976354599, Acc: 0.9279480576515198\n",
      "Epoch: 476, Loss: 0.08528617024421692, Acc: 0.9273799657821655\n",
      "Epoch: 477, Loss: 0.0997774600982666, Acc: 0.9163650274276733\n",
      "Epoch: 478, Loss: 0.0959978699684143, Acc: 0.9223464727401733\n",
      "Epoch: 479, Loss: 0.09930102527141571, Acc: 0.9226284623146057\n",
      "Epoch: 480, Loss: 0.09203571081161499, Acc: 0.9140437841415405\n",
      "Epoch: 481, Loss: 0.09379637241363525, Acc: 0.9170609712600708\n",
      "Epoch: 482, Loss: 0.10765354335308075, Acc: 0.9250501394271851\n",
      "Epoch: 483, Loss: 0.10067090392112732, Acc: 0.9193673133850098\n",
      "Epoch: 484, Loss: 0.09074822813272476, Acc: 0.9185214042663574\n",
      "Epoch: 485, Loss: 0.09347265958786011, Acc: 0.9162625670433044\n",
      "Epoch: 486, Loss: 0.11117857694625854, Acc: 0.9297956228256226\n",
      "Epoch: 487, Loss: 0.08898898214101791, Acc: 0.9197402596473694\n",
      "Epoch: 488, Loss: 0.08678992837667465, Acc: 0.9174555540084839\n",
      "Epoch: 489, Loss: 0.09610376507043839, Acc: 0.9210050702095032\n",
      "Epoch: 490, Loss: 0.09132268279790878, Acc: 0.9169630408287048\n",
      "Epoch: 491, Loss: 0.12448464334011078, Acc: 0.9431767463684082\n",
      "Epoch: 492, Loss: 0.12275062501430511, Acc: 0.9158365726470947\n",
      "Epoch: 493, Loss: 0.09364250302314758, Acc: 0.9231787919998169\n",
      "Epoch: 494, Loss: 0.08909069001674652, Acc: 0.917768657207489\n",
      "Epoch: 495, Loss: 0.08499053865671158, Acc: 0.9181958436965942\n",
      "Epoch: 496, Loss: 0.08793880045413971, Acc: 0.9200359582901001\n",
      "Epoch: 497, Loss: 0.09799223393201828, Acc: 0.9218646287918091\n",
      "Epoch: 498, Loss: 0.08983271569013596, Acc: 0.917972207069397\n",
      "Epoch: 499, Loss: 0.08657428622245789, Acc: 0.9221474528312683\n",
      "Epoch: 500, Loss: 0.08321043848991394, Acc: 0.9177791476249695\n",
      "Epoch: 501, Loss: 0.08611717075109482, Acc: 0.9175296425819397\n",
      "Epoch: 502, Loss: 0.0884113684296608, Acc: 0.930467963218689\n",
      "Epoch: 503, Loss: 0.08649058640003204, Acc: 0.9171867966651917\n",
      "Epoch: 504, Loss: 0.09650620073080063, Acc: 0.9217773675918579\n",
      "Epoch: 505, Loss: 0.08752351999282837, Acc: 0.9168856143951416\n",
      "Epoch: 506, Loss: 0.09725641459226608, Acc: 0.9295364618301392\n",
      "Epoch: 507, Loss: 0.09068571776151657, Acc: 0.9219273924827576\n",
      "Epoch: 508, Loss: 0.08380641788244247, Acc: 0.9173282384872437\n",
      "Epoch: 509, Loss: 0.08073513954877853, Acc: 0.9176884889602661\n",
      "Epoch: 510, Loss: 0.08433348685503006, Acc: 0.9188067317008972\n",
      "Epoch: 511, Loss: 0.08370283246040344, Acc: 0.9198230504989624\n",
      "Epoch: 512, Loss: 0.08343962579965591, Acc: 0.9179666638374329\n",
      "Epoch: 513, Loss: 0.09109319001436234, Acc: 0.9240447282791138\n",
      "Epoch: 514, Loss: 0.0830996185541153, Acc: 0.9213088750839233\n",
      "Epoch: 515, Loss: 0.0863707959651947, Acc: 0.9217358827590942\n",
      "Epoch: 516, Loss: 0.0853039100766182, Acc: 0.9200248122215271\n",
      "Epoch: 517, Loss: 0.09317133575677872, Acc: 0.9225168228149414\n",
      "Epoch: 518, Loss: 0.10542482882738113, Acc: 0.9305740594863892\n",
      "Epoch: 519, Loss: 0.0858205258846283, Acc: 0.918147623538971\n",
      "Epoch: 520, Loss: 0.08682422339916229, Acc: 0.9210400581359863\n",
      "Epoch: 521, Loss: 0.09043267369270325, Acc: 0.9270828366279602\n",
      "Epoch: 522, Loss: 0.0837840586900711, Acc: 0.9233267903327942\n",
      "Epoch: 523, Loss: 0.08505944162607193, Acc: 0.9251293540000916\n",
      "Epoch: 524, Loss: 0.08154012262821198, Acc: 0.9259800910949707\n",
      "Epoch: 525, Loss: 0.08328691124916077, Acc: 0.9216167330741882\n",
      "Epoch: 526, Loss: 0.0808052346110344, Acc: 0.9183231592178345\n",
      "Epoch: 527, Loss: 0.07840264588594437, Acc: 0.9189548492431641\n",
      "Epoch: 528, Loss: 0.0865231603384018, Acc: 0.9335580468177795\n",
      "Epoch: 529, Loss: 0.09466888755559921, Acc: 0.9176296591758728\n",
      "Epoch: 530, Loss: 0.08004442602396011, Acc: 0.9192220568656921\n",
      "Epoch: 531, Loss: 0.09003105759620667, Acc: 0.9214349389076233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 532, Loss: 0.08730662614107132, Acc: 0.9205788969993591\n",
      "Epoch: 533, Loss: 0.08209975808858871, Acc: 0.9199047088623047\n",
      "Epoch: 534, Loss: 0.08528409898281097, Acc: 0.9325392246246338\n",
      "Epoch: 535, Loss: 0.10959146916866302, Acc: 0.9344111680984497\n",
      "Epoch: 536, Loss: 0.1007658839225769, Acc: 0.9251377582550049\n",
      "Epoch: 537, Loss: 0.09766073524951935, Acc: 0.9251247644424438\n",
      "Epoch: 538, Loss: 0.08795180916786194, Acc: 0.9227093458175659\n",
      "Epoch: 539, Loss: 0.08226875960826874, Acc: 0.9226117134094238\n",
      "Epoch: 540, Loss: 0.08248192816972733, Acc: 0.93119215965271\n",
      "Epoch: 541, Loss: 0.10483090579509735, Acc: 0.9362618327140808\n",
      "Epoch: 542, Loss: 0.08165029436349869, Acc: 0.9226694703102112\n",
      "Epoch: 543, Loss: 0.08584167808294296, Acc: 0.9205334782600403\n",
      "Epoch: 544, Loss: 0.08430573344230652, Acc: 0.9243793487548828\n",
      "Epoch: 545, Loss: 0.09153088182210922, Acc: 0.9217562675476074\n",
      "Epoch: 546, Loss: 0.08456025272607803, Acc: 0.9243383407592773\n",
      "Epoch: 547, Loss: 0.08589186519384384, Acc: 0.9306893348693848\n",
      "Epoch: 548, Loss: 0.09548594057559967, Acc: 0.9380787014961243\n",
      "Epoch: 549, Loss: 0.11819608509540558, Acc: 0.9243050217628479\n",
      "Epoch: 550, Loss: 0.09027543663978577, Acc: 0.92681485414505\n",
      "Epoch: 551, Loss: 0.09212275594472885, Acc: 0.9255300760269165\n",
      "Epoch: 552, Loss: 0.09225847572088242, Acc: 0.9317387342453003\n",
      "Epoch: 553, Loss: 0.09793753921985626, Acc: 0.9214744567871094\n",
      "Epoch: 554, Loss: 0.0929473489522934, Acc: 0.9273018836975098\n",
      "Epoch: 555, Loss: 0.08074619621038437, Acc: 0.9264119267463684\n",
      "Epoch: 556, Loss: 0.08386754989624023, Acc: 0.9217417240142822\n",
      "Epoch: 557, Loss: 0.08736245334148407, Acc: 0.9328365325927734\n",
      "Epoch: 558, Loss: 0.08926736563444138, Acc: 0.9233652353286743\n",
      "Epoch: 559, Loss: 0.08597936481237411, Acc: 0.9240811467170715\n",
      "Epoch: 560, Loss: 0.08191710710525513, Acc: 0.9215633869171143\n",
      "Epoch: 561, Loss: 0.10211820900440216, Acc: 0.9343515634536743\n",
      "Epoch: 562, Loss: 0.08514782041311264, Acc: 0.9406007528305054\n",
      "Epoch: 563, Loss: 0.11285272985696793, Acc: 0.9297443628311157\n",
      "Epoch: 564, Loss: 0.08710514008998871, Acc: 0.9303950667381287\n",
      "Epoch: 565, Loss: 0.09596750140190125, Acc: 0.9276388883590698\n",
      "Epoch: 566, Loss: 0.10022050887346268, Acc: 0.9291887879371643\n",
      "Epoch: 567, Loss: 0.11392396688461304, Acc: 0.9306347370147705\n",
      "Epoch: 568, Loss: 0.0935005322098732, Acc: 0.9317635297775269\n",
      "Epoch: 569, Loss: 0.09944398701190948, Acc: 0.9357724189758301\n",
      "Epoch: 570, Loss: 0.08969271183013916, Acc: 0.9289560317993164\n",
      "Epoch: 571, Loss: 0.09196674823760986, Acc: 0.9262630939483643\n",
      "Epoch: 572, Loss: 0.07937684655189514, Acc: 0.921764612197876\n",
      "Epoch: 573, Loss: 0.08242684602737427, Acc: 0.9243679046630859\n",
      "Epoch: 574, Loss: 0.09997950494289398, Acc: 0.9301063418388367\n",
      "Epoch: 575, Loss: 0.08637590706348419, Acc: 0.9332736134529114\n",
      "Epoch: 576, Loss: 0.08313571661710739, Acc: 0.9233471751213074\n",
      "Epoch: 577, Loss: 0.08199448883533478, Acc: 0.9256170392036438\n",
      "Epoch: 578, Loss: 0.0856911689043045, Acc: 0.9266648888587952\n",
      "Epoch: 579, Loss: 0.0799340307712555, Acc: 0.9254394173622131\n",
      "Epoch: 580, Loss: 0.07966821640729904, Acc: 0.9232788681983948\n",
      "Epoch: 581, Loss: 0.08262820541858673, Acc: 0.924795925617218\n",
      "Epoch: 582, Loss: 0.07708480954170227, Acc: 0.92249995470047\n",
      "Epoch: 583, Loss: 0.08387155830860138, Acc: 0.9236949682235718\n",
      "Epoch: 584, Loss: 0.09700728952884674, Acc: 0.9355936646461487\n",
      "Epoch: 585, Loss: 0.0977851152420044, Acc: 0.9250059723854065\n",
      "Epoch: 586, Loss: 0.08054102212190628, Acc: 0.9245573878288269\n",
      "Epoch: 587, Loss: 0.07715630531311035, Acc: 0.9259718656539917\n",
      "Epoch: 588, Loss: 0.09927176684141159, Acc: 0.9272940754890442\n",
      "Epoch: 589, Loss: 0.08623828738927841, Acc: 0.9410322904586792\n",
      "Epoch: 590, Loss: 0.11089077591896057, Acc: 0.9411866664886475\n",
      "Epoch: 591, Loss: 0.08264841884374619, Acc: 0.9256618022918701\n",
      "Epoch: 592, Loss: 0.1165907084941864, Acc: 0.9420254230499268\n",
      "Epoch: 593, Loss: 0.10318230092525482, Acc: 0.9355295300483704\n",
      "Epoch: 594, Loss: 0.08010076731443405, Acc: 0.9260099530220032\n",
      "Epoch: 595, Loss: 0.08261966705322266, Acc: 0.9243394136428833\n",
      "Epoch: 596, Loss: 0.08816476166248322, Acc: 0.9258667826652527\n",
      "Epoch: 597, Loss: 0.08229471743106842, Acc: 0.9237527847290039\n",
      "Epoch: 598, Loss: 0.08353690803050995, Acc: 0.9287508726119995\n",
      "Epoch: 599, Loss: 0.0898953378200531, Acc: 0.9282019138336182\n",
      "Epoch: 600, Loss: 0.08712278306484222, Acc: 0.9266806840896606\n",
      "Epoch: 601, Loss: 0.11279156059026718, Acc: 0.9484730958938599\n",
      "Epoch: 602, Loss: 0.08846120536327362, Acc: 0.9337671995162964\n",
      "Epoch: 603, Loss: 0.0926750898361206, Acc: 0.9359779357910156\n",
      "Epoch: 604, Loss: 0.09044471383094788, Acc: 0.9270878434181213\n",
      "Epoch: 605, Loss: 0.0900413990020752, Acc: 0.9292001724243164\n",
      "Epoch: 606, Loss: 0.0932321548461914, Acc: 0.9317387938499451\n",
      "Epoch: 607, Loss: 0.073829285800457, Acc: 0.9259679317474365\n",
      "Epoch: 608, Loss: 0.0825057327747345, Acc: 0.9338083863258362\n",
      "Epoch: 609, Loss: 0.08525925129652023, Acc: 0.9308131337165833\n",
      "Epoch: 610, Loss: 0.08590026199817657, Acc: 0.9294534921646118\n",
      "Epoch: 611, Loss: 0.07427466660737991, Acc: 0.9283471703529358\n",
      "Epoch: 612, Loss: 0.08539143204689026, Acc: 0.9300355911254883\n",
      "Epoch: 613, Loss: 0.09219332039356232, Acc: 0.929141104221344\n",
      "Epoch: 614, Loss: 0.08229129761457443, Acc: 0.9267200231552124\n",
      "Epoch: 615, Loss: 0.08271368592977524, Acc: 0.9252293705940247\n",
      "Epoch: 616, Loss: 0.09025958180427551, Acc: 0.9338035583496094\n",
      "Epoch: 617, Loss: 0.09614727646112442, Acc: 0.9311261177062988\n",
      "Epoch: 618, Loss: 0.09217768907546997, Acc: 0.9264711737632751\n",
      "Epoch: 619, Loss: 0.08314304798841476, Acc: 0.9287420511245728\n",
      "Epoch: 620, Loss: 0.08313019573688507, Acc: 0.9305506348609924\n",
      "Epoch: 621, Loss: 0.08492866903543472, Acc: 0.9340677261352539\n",
      "Epoch: 622, Loss: 0.09778903424739838, Acc: 0.9352390170097351\n",
      "Epoch: 623, Loss: 0.08311732858419418, Acc: 0.9250051975250244\n",
      "Epoch: 624, Loss: 0.10414662212133408, Acc: 0.9392569065093994\n",
      "Epoch: 625, Loss: 0.08154585212469101, Acc: 0.9365622401237488\n",
      "Epoch: 626, Loss: 0.07801341265439987, Acc: 0.9360226392745972\n",
      "Epoch: 627, Loss: 0.09950589388608932, Acc: 0.9321475625038147\n",
      "Epoch: 628, Loss: 0.08593209087848663, Acc: 0.9335291385650635\n",
      "Epoch: 629, Loss: 0.07945926487445831, Acc: 0.9274178147315979\n",
      "Epoch: 630, Loss: 0.07714327424764633, Acc: 0.9269937872886658\n",
      "Epoch: 631, Loss: 0.09391404688358307, Acc: 0.9409833550453186\n",
      "Epoch: 632, Loss: 0.08758948743343353, Acc: 0.9318070411682129\n",
      "Epoch: 633, Loss: 0.07864842563867569, Acc: 0.9264687299728394\n",
      "Epoch: 634, Loss: 0.07370525598526001, Acc: 0.9285381436347961\n",
      "Epoch: 635, Loss: 0.07542908191680908, Acc: 0.9243052005767822\n",
      "Epoch: 636, Loss: 0.07842742651700974, Acc: 0.9354748725891113\n",
      "Epoch: 637, Loss: 0.08638523519039154, Acc: 0.9240505695343018\n",
      "Epoch: 638, Loss: 0.08628371357917786, Acc: 0.92709881067276\n",
      "Epoch: 639, Loss: 0.0831051617860794, Acc: 0.9323009848594666\n",
      "Epoch: 640, Loss: 0.07864472270011902, Acc: 0.9256155490875244\n",
      "Epoch: 641, Loss: 0.07770414650440216, Acc: 0.9270330667495728\n",
      "Epoch: 642, Loss: 0.07449212670326233, Acc: 0.9258142113685608\n",
      "Epoch: 643, Loss: 0.07607167214155197, Acc: 0.9265680909156799\n",
      "Epoch: 644, Loss: 0.08345764875411987, Acc: 0.9331855177879333\n",
      "Epoch: 645, Loss: 0.09948159009218216, Acc: 0.9496538043022156\n",
      "Epoch: 646, Loss: 0.09151428937911987, Acc: 0.9271022081375122\n",
      "Epoch: 647, Loss: 0.09213390201330185, Acc: 0.9390774965286255\n",
      "Epoch: 648, Loss: 0.08217822015285492, Acc: 0.9279766082763672\n",
      "Epoch: 649, Loss: 0.07295918464660645, Acc: 0.9295137524604797\n",
      "Epoch: 650, Loss: 0.08707470446825027, Acc: 0.9302712082862854\n",
      "Epoch: 651, Loss: 0.07825019210577011, Acc: 0.9270966053009033\n",
      "Epoch: 652, Loss: 0.0829971581697464, Acc: 0.929578959941864\n",
      "Epoch: 653, Loss: 0.0856824740767479, Acc: 0.9275014400482178\n",
      "Epoch: 654, Loss: 0.07921826839447021, Acc: 0.937036395072937\n",
      "Epoch: 655, Loss: 0.09900273382663727, Acc: 0.931067705154419\n",
      "Epoch: 656, Loss: 0.07681035995483398, Acc: 0.9257257580757141\n",
      "Epoch: 657, Loss: 0.07693443447351456, Acc: 0.9276478886604309\n",
      "Epoch: 658, Loss: 0.08387310057878494, Acc: 0.9362619519233704\n",
      "Epoch: 659, Loss: 0.08628174662590027, Acc: 0.9296445846557617\n",
      "Epoch: 660, Loss: 0.0800730362534523, Acc: 0.9315873384475708\n",
      "Epoch: 661, Loss: 0.07982592284679413, Acc: 0.9304964542388916\n",
      "Epoch: 662, Loss: 0.08827453851699829, Acc: 0.9280160069465637\n",
      "Epoch: 663, Loss: 0.10205717384815216, Acc: 0.9440138936042786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 664, Loss: 0.07723429054021835, Acc: 0.9330983757972717\n",
      "Epoch: 665, Loss: 0.08134995400905609, Acc: 0.932898223400116\n",
      "Epoch: 666, Loss: 0.08313804864883423, Acc: 0.9322928190231323\n",
      "Epoch: 667, Loss: 0.08131981641054153, Acc: 0.927658200263977\n",
      "Epoch: 668, Loss: 0.09193947911262512, Acc: 0.9339590072631836\n",
      "Epoch: 669, Loss: 0.08723792433738708, Acc: 0.9320623874664307\n",
      "Epoch: 670, Loss: 0.07461583614349365, Acc: 0.9370236396789551\n",
      "Epoch: 671, Loss: 0.08577674627304077, Acc: 0.9382113814353943\n",
      "Epoch: 672, Loss: 0.0752962976694107, Acc: 0.9288408756256104\n",
      "Epoch: 673, Loss: 0.09029299020767212, Acc: 0.9304996728897095\n",
      "Epoch: 674, Loss: 0.06998609006404877, Acc: 0.931064784526825\n",
      "Epoch: 675, Loss: 0.07362306118011475, Acc: 0.926617443561554\n",
      "Epoch: 676, Loss: 0.08091682195663452, Acc: 0.9312387108802795\n",
      "Epoch: 677, Loss: 0.07258027046918869, Acc: 0.9262047410011292\n",
      "Epoch: 678, Loss: 0.07658606767654419, Acc: 0.927942156791687\n",
      "Epoch: 679, Loss: 0.0788390263915062, Acc: 0.9297573566436768\n",
      "Epoch: 680, Loss: 0.07497703284025192, Acc: 0.9285234808921814\n",
      "Epoch: 681, Loss: 0.08000707626342773, Acc: 0.9292708039283752\n",
      "Epoch: 682, Loss: 0.07817310839891434, Acc: 0.9327768087387085\n",
      "Epoch: 683, Loss: 0.07880741357803345, Acc: 0.9311663508415222\n",
      "Epoch: 684, Loss: 0.08081085979938507, Acc: 0.9268828630447388\n",
      "Epoch: 685, Loss: 0.08355973660945892, Acc: 0.9280283451080322\n",
      "Epoch: 686, Loss: 0.08141566067934036, Acc: 0.9367042183876038\n",
      "Epoch: 687, Loss: 0.0804654210805893, Acc: 0.9292261004447937\n",
      "Epoch: 688, Loss: 0.09626689553260803, Acc: 0.9408283829689026\n",
      "Epoch: 689, Loss: 0.07991541177034378, Acc: 0.931038498878479\n",
      "Epoch: 690, Loss: 0.0761173889040947, Acc: 0.928134024143219\n",
      "Epoch: 691, Loss: 0.08023325353860855, Acc: 0.9327245354652405\n",
      "Epoch: 692, Loss: 0.07849739491939545, Acc: 0.9304424524307251\n",
      "Epoch: 693, Loss: 0.0794508159160614, Acc: 0.9345664381980896\n",
      "Epoch: 694, Loss: 0.09326094388961792, Acc: 0.9319630861282349\n",
      "Epoch: 695, Loss: 0.07614156603813171, Acc: 0.9287952780723572\n",
      "Epoch: 696, Loss: 0.0775795504450798, Acc: 0.9308356642723083\n",
      "Epoch: 697, Loss: 0.07360539585351944, Acc: 0.9353248476982117\n",
      "Epoch: 698, Loss: 0.07731983810663223, Acc: 0.9291149377822876\n",
      "Epoch: 699, Loss: 0.10767750442028046, Acc: 0.9415866732597351\n",
      "Epoch: 700, Loss: 0.07536333054304123, Acc: 0.9342832565307617\n",
      "Epoch: 701, Loss: 0.07818721234798431, Acc: 0.9323064684867859\n",
      "Epoch: 702, Loss: 0.0848754420876503, Acc: 0.9331557154655457\n",
      "Epoch: 703, Loss: 0.0797913670539856, Acc: 0.927756130695343\n",
      "Epoch: 704, Loss: 0.07638967037200928, Acc: 0.9312677383422852\n",
      "Epoch: 705, Loss: 0.08915701508522034, Acc: 0.9360825419425964\n",
      "Epoch: 706, Loss: 0.07229402661323547, Acc: 0.9311887621879578\n",
      "Epoch: 707, Loss: 0.0871078372001648, Acc: 0.9334201216697693\n",
      "Epoch: 708, Loss: 0.08367399871349335, Acc: 0.9328121542930603\n",
      "Epoch: 709, Loss: 0.07629039883613586, Acc: 0.9365166425704956\n",
      "Epoch: 710, Loss: 0.07723410427570343, Acc: 0.9323294758796692\n",
      "Epoch: 711, Loss: 0.08783891797065735, Acc: 0.9381125569343567\n",
      "Epoch: 712, Loss: 0.09471789002418518, Acc: 0.935843288898468\n",
      "Epoch: 713, Loss: 0.0714903250336647, Acc: 0.9283218383789062\n",
      "Epoch: 714, Loss: 0.08373934775590897, Acc: 0.9347232580184937\n",
      "Epoch: 715, Loss: 0.08419645577669144, Acc: 0.9351433515548706\n",
      "Epoch: 716, Loss: 0.08667665719985962, Acc: 0.9369624853134155\n",
      "Epoch: 717, Loss: 0.08325298130512238, Acc: 0.9390426874160767\n",
      "Epoch: 718, Loss: 0.08966352045536041, Acc: 0.9343416094779968\n",
      "Epoch: 719, Loss: 0.0836465060710907, Acc: 0.9397634863853455\n",
      "Epoch: 720, Loss: 0.09346702694892883, Acc: 0.935907781124115\n",
      "Epoch: 721, Loss: 0.0733492523431778, Acc: 0.9298626780509949\n",
      "Epoch: 722, Loss: 0.07171371579170227, Acc: 0.9286397695541382\n",
      "Epoch: 723, Loss: 0.07933054864406586, Acc: 0.9347019791603088\n",
      "Epoch: 724, Loss: 0.08899322152137756, Acc: 0.9395134449005127\n",
      "Epoch: 725, Loss: 0.07303476333618164, Acc: 0.9343787431716919\n",
      "Epoch: 726, Loss: 0.09553258866071701, Acc: 0.9417625665664673\n",
      "Epoch: 727, Loss: 0.07219833880662918, Acc: 0.9318284392356873\n",
      "Epoch: 728, Loss: 0.08560830354690552, Acc: 0.93222576379776\n",
      "Epoch: 729, Loss: 0.07269096374511719, Acc: 0.9372895956039429\n",
      "Epoch: 730, Loss: 0.07341582328081131, Acc: 0.9310228824615479\n",
      "Epoch: 731, Loss: 0.08746998012065887, Acc: 0.9343702793121338\n",
      "Epoch: 732, Loss: 0.08063811808824539, Acc: 0.9312096238136292\n",
      "Epoch: 733, Loss: 0.08398047834634781, Acc: 0.9318788647651672\n",
      "Epoch: 734, Loss: 0.0835314691066742, Acc: 0.9309848546981812\n",
      "Epoch: 735, Loss: 0.09029407054185867, Acc: 0.9365419149398804\n",
      "Epoch: 736, Loss: 0.06855899095535278, Acc: 0.9370073080062866\n",
      "Epoch: 737, Loss: 0.07383230328559875, Acc: 0.9341070055961609\n",
      "Epoch: 738, Loss: 0.08251302689313889, Acc: 0.931755781173706\n",
      "Epoch: 739, Loss: 0.08168385177850723, Acc: 0.9394914507865906\n",
      "Epoch: 740, Loss: 0.07356070727109909, Acc: 0.9339026808738708\n",
      "Epoch: 741, Loss: 0.075367271900177, Acc: 0.9331148862838745\n",
      "Epoch: 742, Loss: 0.09404513239860535, Acc: 0.9480583071708679\n",
      "Epoch: 743, Loss: 0.08302554488182068, Acc: 0.930914044380188\n",
      "Epoch: 744, Loss: 0.09749433398246765, Acc: 0.9452599883079529\n",
      "Epoch: 745, Loss: 0.13495823740959167, Acc: 0.9417803287506104\n",
      "Epoch: 746, Loss: 0.08087307214736938, Acc: 0.931756317615509\n",
      "Epoch: 747, Loss: 0.07486085593700409, Acc: 0.9345656633377075\n",
      "Epoch: 748, Loss: 0.0811404138803482, Acc: 0.932077169418335\n",
      "Epoch: 749, Loss: 0.07021541148424149, Acc: 0.9297723770141602\n",
      "Epoch: 750, Loss: 0.07732991874217987, Acc: 0.9316292405128479\n",
      "Epoch: 751, Loss: 0.07844800502061844, Acc: 0.9371490478515625\n",
      "Epoch: 752, Loss: 0.0763760581612587, Acc: 0.9313299655914307\n",
      "Epoch: 753, Loss: 0.07239165157079697, Acc: 0.9316725730895996\n",
      "Epoch: 754, Loss: 0.07232734560966492, Acc: 0.9363847374916077\n",
      "Epoch: 755, Loss: 0.07488483190536499, Acc: 0.9360048770904541\n",
      "Epoch: 756, Loss: 0.0761319249868393, Acc: 0.9320456981658936\n",
      "Epoch: 757, Loss: 0.08537914603948593, Acc: 0.9307435750961304\n",
      "Epoch: 758, Loss: 0.07073380053043365, Acc: 0.931560754776001\n",
      "Epoch: 759, Loss: 0.08447039127349854, Acc: 0.9343628883361816\n",
      "Epoch: 760, Loss: 0.07521586865186691, Acc: 0.9333377480506897\n",
      "Epoch: 761, Loss: 0.07113110274076462, Acc: 0.9315638542175293\n",
      "Epoch: 762, Loss: 0.07306782901287079, Acc: 0.9304688572883606\n",
      "Epoch: 763, Loss: 0.07813294231891632, Acc: 0.9389110207557678\n",
      "Epoch: 764, Loss: 0.06904380768537521, Acc: 0.9308547973632812\n",
      "Epoch: 765, Loss: 0.09334282577037811, Acc: 0.9409130215644836\n",
      "Epoch: 766, Loss: 0.06909404695034027, Acc: 0.9298567771911621\n",
      "Epoch: 767, Loss: 0.07472173869609833, Acc: 0.9370187520980835\n",
      "Epoch: 768, Loss: 0.07890406250953674, Acc: 0.936173677444458\n",
      "Epoch: 769, Loss: 0.0815604031085968, Acc: 0.9395231604576111\n",
      "Epoch: 770, Loss: 0.07153312861919403, Acc: 0.93008953332901\n",
      "Epoch: 771, Loss: 0.07553879171609879, Acc: 0.9314898252487183\n",
      "Epoch: 772, Loss: 0.07268468290567398, Acc: 0.9332446455955505\n",
      "Epoch: 773, Loss: 0.0789066031575203, Acc: 0.9410401582717896\n",
      "Epoch: 774, Loss: 0.07163297384977341, Acc: 0.9334439635276794\n",
      "Epoch: 775, Loss: 0.0855022594332695, Acc: 0.9411202669143677\n",
      "Epoch: 776, Loss: 0.08820662647485733, Acc: 0.9408581256866455\n",
      "Epoch: 777, Loss: 0.08320482075214386, Acc: 0.9379521012306213\n",
      "Epoch: 778, Loss: 0.08597947657108307, Acc: 0.9349486231803894\n",
      "Epoch: 779, Loss: 0.08260321617126465, Acc: 0.9375641345977783\n",
      "Epoch: 780, Loss: 0.07844706624746323, Acc: 0.9335368871688843\n",
      "Epoch: 781, Loss: 0.07741376757621765, Acc: 0.9339923858642578\n",
      "Epoch: 782, Loss: 0.07959520816802979, Acc: 0.9329626560211182\n",
      "Epoch: 783, Loss: 0.07631173729896545, Acc: 0.9372001886367798\n",
      "Epoch: 784, Loss: 0.08135439455509186, Acc: 0.9324222207069397\n",
      "Epoch: 785, Loss: 0.07969240099191666, Acc: 0.9431065320968628\n",
      "Epoch: 786, Loss: 0.1226704940199852, Acc: 0.9397227764129639\n",
      "Epoch: 787, Loss: 0.07340872287750244, Acc: 0.9336845874786377\n",
      "Epoch: 788, Loss: 0.07193318009376526, Acc: 0.9301061630249023\n",
      "Epoch: 789, Loss: 0.08237476646900177, Acc: 0.9365516901016235\n",
      "Epoch: 790, Loss: 0.07480626553297043, Acc: 0.93412184715271\n",
      "Epoch: 791, Loss: 0.0895662009716034, Acc: 0.9412022233009338\n",
      "Epoch: 792, Loss: 0.07059009373188019, Acc: 0.9350728988647461\n",
      "Epoch: 793, Loss: 0.08695581555366516, Acc: 0.9393178820610046\n",
      "Epoch: 794, Loss: 0.09164576232433319, Acc: 0.9339679479598999\n",
      "Epoch: 795, Loss: 0.07088621705770493, Acc: 0.9310587644577026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 796, Loss: 0.07350902259349823, Acc: 0.9354479312896729\n",
      "Epoch: 797, Loss: 0.07723742723464966, Acc: 0.9357777833938599\n",
      "Epoch: 798, Loss: 0.07187330722808838, Acc: 0.9311106204986572\n",
      "Epoch: 799, Loss: 0.07365678250789642, Acc: 0.9344004392623901\n",
      "Epoch: 800, Loss: 0.06994486600160599, Acc: 0.930492639541626\n",
      "Epoch: 801, Loss: 0.07571294158697128, Acc: 0.9361536502838135\n",
      "Epoch: 802, Loss: 0.09437529742717743, Acc: 0.9565778374671936\n",
      "Epoch: 803, Loss: 0.08447060734033585, Acc: 0.9320605993270874\n",
      "Epoch: 804, Loss: 0.07227426022291183, Acc: 0.932403564453125\n",
      "Epoch: 805, Loss: 0.07549750059843063, Acc: 0.9336403608322144\n",
      "Epoch: 806, Loss: 0.08543479442596436, Acc: 0.9348397254943848\n",
      "Epoch: 807, Loss: 0.0839596837759018, Acc: 0.9342292547225952\n",
      "Epoch: 808, Loss: 0.06854909658432007, Acc: 0.9321883916854858\n",
      "Epoch: 809, Loss: 0.07063428312540054, Acc: 0.9310507774353027\n",
      "Epoch: 810, Loss: 0.09152837842702866, Acc: 0.9409312605857849\n",
      "Epoch: 811, Loss: 0.08760730922222137, Acc: 0.9422793984413147\n",
      "Epoch: 812, Loss: 0.07174991816282272, Acc: 0.9325566291809082\n",
      "Epoch: 813, Loss: 0.06666440516710281, Acc: 0.9310445189476013\n",
      "Epoch: 814, Loss: 0.07352666556835175, Acc: 0.9366049766540527\n",
      "Epoch: 815, Loss: 0.06902015954256058, Acc: 0.9326586723327637\n",
      "Epoch: 816, Loss: 0.07600873708724976, Acc: 0.9391440749168396\n",
      "Epoch: 817, Loss: 0.06926172971725464, Acc: 0.9311087131500244\n",
      "Epoch: 818, Loss: 0.0698780044913292, Acc: 0.9322590231895447\n",
      "Epoch: 819, Loss: 0.07249363511800766, Acc: 0.9362924695014954\n",
      "Epoch: 820, Loss: 0.09657948464155197, Acc: 0.9421020746231079\n",
      "Epoch: 821, Loss: 0.08623211830854416, Acc: 0.9378059506416321\n",
      "Epoch: 822, Loss: 0.07911858707666397, Acc: 0.9319226145744324\n",
      "Epoch: 823, Loss: 0.07672320306301117, Acc: 0.9357050061225891\n",
      "Epoch: 824, Loss: 0.08148525655269623, Acc: 0.9376521110534668\n",
      "Epoch: 825, Loss: 0.07883395999670029, Acc: 0.9306076765060425\n",
      "Epoch: 826, Loss: 0.07255431264638901, Acc: 0.9373164176940918\n",
      "Epoch: 827, Loss: 0.09083123505115509, Acc: 0.9528756141662598\n",
      "Epoch: 828, Loss: 0.06894254684448242, Acc: 0.9313492774963379\n",
      "Epoch: 829, Loss: 0.07212859392166138, Acc: 0.9326379895210266\n",
      "Epoch: 830, Loss: 0.06934021413326263, Acc: 0.9343132972717285\n",
      "Epoch: 831, Loss: 0.07438159734010696, Acc: 0.9404894113540649\n",
      "Epoch: 832, Loss: 0.07988333702087402, Acc: 0.9359949827194214\n",
      "Epoch: 833, Loss: 0.06584305316209793, Acc: 0.9356842041015625\n",
      "Epoch: 834, Loss: 0.0833953469991684, Acc: 0.9373584985733032\n",
      "Epoch: 835, Loss: 0.07645386457443237, Acc: 0.9327166080474854\n",
      "Epoch: 836, Loss: 0.0693550780415535, Acc: 0.9330397844314575\n",
      "Epoch: 837, Loss: 0.08000887930393219, Acc: 0.9363468885421753\n",
      "Epoch: 838, Loss: 0.07037609815597534, Acc: 0.9371746778488159\n",
      "Epoch: 839, Loss: 0.08210477232933044, Acc: 0.9314143061637878\n",
      "Epoch: 840, Loss: 0.0764593780040741, Acc: 0.936342179775238\n",
      "Epoch: 841, Loss: 0.08252458274364471, Acc: 0.9418683052062988\n",
      "Epoch: 842, Loss: 0.07769356667995453, Acc: 0.9398481845855713\n",
      "Epoch: 843, Loss: 0.07881104946136475, Acc: 0.9361646175384521\n",
      "Epoch: 844, Loss: 0.09043010324239731, Acc: 0.9487950205802917\n",
      "Epoch: 845, Loss: 0.1261368691921234, Acc: 0.9354971051216125\n",
      "Epoch: 846, Loss: 0.0703258216381073, Acc: 0.9358608722686768\n",
      "Epoch: 847, Loss: 0.08395235240459442, Acc: 0.9435718655586243\n",
      "Epoch: 848, Loss: 0.07977210730314255, Acc: 0.9319437742233276\n",
      "Epoch: 849, Loss: 0.08159127831459045, Acc: 0.9443805813789368\n",
      "Epoch: 850, Loss: 0.08240117877721786, Acc: 0.9533512592315674\n",
      "Epoch: 851, Loss: 0.08085420727729797, Acc: 0.9399917721748352\n",
      "Epoch: 852, Loss: 0.06866592168807983, Acc: 0.934603750705719\n",
      "Epoch: 853, Loss: 0.06736993789672852, Acc: 0.931732714176178\n",
      "Epoch: 854, Loss: 0.07763184607028961, Acc: 0.9387006163597107\n",
      "Epoch: 855, Loss: 0.08382336050271988, Acc: 0.9423606991767883\n",
      "Epoch: 856, Loss: 0.066412553191185, Acc: 0.9335432648658752\n",
      "Epoch: 857, Loss: 0.07236000150442123, Acc: 0.9377371072769165\n",
      "Epoch: 858, Loss: 0.07013826817274094, Acc: 0.934265673160553\n",
      "Epoch: 859, Loss: 0.0675349086523056, Acc: 0.9332953691482544\n",
      "Epoch: 860, Loss: 0.06861013174057007, Acc: 0.9326679110527039\n",
      "Epoch: 861, Loss: 0.06965424120426178, Acc: 0.9325839877128601\n",
      "Epoch: 862, Loss: 0.07113043963909149, Acc: 0.9340278506278992\n",
      "Epoch: 863, Loss: 0.0935259461402893, Acc: 0.954660177230835\n",
      "Epoch: 864, Loss: 0.06618370860815048, Acc: 0.9443199634552002\n",
      "Epoch: 865, Loss: 0.08689567446708679, Acc: 0.9426605105400085\n",
      "Epoch: 866, Loss: 0.07932037860155106, Acc: 0.9383242130279541\n",
      "Epoch: 867, Loss: 0.07951688021421432, Acc: 0.9337995052337646\n",
      "Epoch: 868, Loss: 0.06674499809741974, Acc: 0.9329174160957336\n",
      "Epoch: 869, Loss: 0.07328718900680542, Acc: 0.939274787902832\n",
      "Epoch: 870, Loss: 0.07070483267307281, Acc: 0.9391100406646729\n",
      "Epoch: 871, Loss: 0.07745768129825592, Acc: 0.9431531429290771\n",
      "Epoch: 872, Loss: 0.07679958641529083, Acc: 0.9358924031257629\n",
      "Epoch: 873, Loss: 0.07433215528726578, Acc: 0.9361480474472046\n",
      "Epoch: 874, Loss: 0.07405824214220047, Acc: 0.9394222497940063\n",
      "Epoch: 875, Loss: 0.07699384540319443, Acc: 0.940490186214447\n",
      "Epoch: 876, Loss: 0.0829952210187912, Acc: 0.9367835521697998\n",
      "Epoch: 877, Loss: 0.08770263940095901, Acc: 0.944526731967926\n",
      "Epoch: 878, Loss: 0.0752391442656517, Acc: 0.9356513023376465\n",
      "Epoch: 879, Loss: 0.07065654546022415, Acc: 0.9348859786987305\n",
      "Epoch: 880, Loss: 0.08688575029373169, Acc: 0.9432981610298157\n",
      "Epoch: 881, Loss: 0.0862112045288086, Acc: 0.9534499049186707\n",
      "Epoch: 882, Loss: 0.08731701225042343, Acc: 0.9344998002052307\n",
      "Epoch: 883, Loss: 0.0706259161233902, Acc: 0.9386749863624573\n",
      "Epoch: 884, Loss: 0.07150678336620331, Acc: 0.9357253313064575\n",
      "Epoch: 885, Loss: 0.07445064932107925, Acc: 0.9417641162872314\n",
      "Epoch: 886, Loss: 0.07536258548498154, Acc: 0.9380228519439697\n",
      "Epoch: 887, Loss: 0.0756894052028656, Acc: 0.9369975328445435\n",
      "Epoch: 888, Loss: 0.07488832622766495, Acc: 0.9360936880111694\n",
      "Epoch: 889, Loss: 0.0739661157131195, Acc: 0.9331181645393372\n",
      "Epoch: 890, Loss: 0.06655505299568176, Acc: 0.9420686364173889\n",
      "Epoch: 891, Loss: 0.07021292299032211, Acc: 0.9373503923416138\n",
      "Epoch: 892, Loss: 0.07532550394535065, Acc: 0.939317524433136\n",
      "Epoch: 893, Loss: 0.08343704044818878, Acc: 0.939806342124939\n",
      "Epoch: 894, Loss: 0.08321266621351242, Acc: 0.9480934739112854\n",
      "Epoch: 895, Loss: 0.08506439626216888, Acc: 0.9339228868484497\n",
      "Epoch: 896, Loss: 0.07480104267597198, Acc: 0.9346672296524048\n",
      "Epoch: 897, Loss: 0.06667905300855637, Acc: 0.9391771554946899\n",
      "Epoch: 898, Loss: 0.06654837727546692, Acc: 0.9339733123779297\n",
      "Epoch: 899, Loss: 0.07421021163463593, Acc: 0.9374822974205017\n",
      "Epoch: 900, Loss: 0.06879718601703644, Acc: 0.9362598657608032\n",
      "Epoch: 901, Loss: 0.06947953253984451, Acc: 0.933978796005249\n",
      "Epoch: 902, Loss: 0.09664519131183624, Acc: 0.9450527429580688\n",
      "Epoch: 903, Loss: 0.07954895496368408, Acc: 0.9377512335777283\n",
      "Epoch: 904, Loss: 0.0721287876367569, Acc: 0.9375447034835815\n",
      "Epoch: 905, Loss: 0.08146383613348007, Acc: 0.9445375204086304\n",
      "Epoch: 906, Loss: 0.08140493184328079, Acc: 0.9451348185539246\n",
      "Epoch: 907, Loss: 0.07975497841835022, Acc: 0.9342013001441956\n",
      "Epoch: 908, Loss: 0.06476906687021255, Acc: 0.933172345161438\n",
      "Epoch: 909, Loss: 0.06919912993907928, Acc: 0.9361475706100464\n",
      "Epoch: 910, Loss: 0.09392006695270538, Acc: 0.9485811591148376\n",
      "Epoch: 911, Loss: 0.08217225968837738, Acc: 0.9391311407089233\n",
      "Epoch: 912, Loss: 0.06596527993679047, Acc: 0.9341645240783691\n",
      "Epoch: 913, Loss: 0.06846403330564499, Acc: 0.9341335892677307\n",
      "Epoch: 914, Loss: 0.07375776767730713, Acc: 0.9366528987884521\n",
      "Epoch: 915, Loss: 0.07685939967632294, Acc: 0.9356392025947571\n",
      "Epoch: 916, Loss: 0.0784982293844223, Acc: 0.9364278316497803\n",
      "Epoch: 917, Loss: 0.0686463862657547, Acc: 0.9332675933837891\n",
      "Epoch: 918, Loss: 0.07370170205831528, Acc: 0.9491931796073914\n",
      "Epoch: 919, Loss: 0.07874897867441177, Acc: 0.9353078603744507\n",
      "Epoch: 920, Loss: 0.06670884788036346, Acc: 0.9364528059959412\n",
      "Epoch: 921, Loss: 0.07327371835708618, Acc: 0.9360681772232056\n",
      "Epoch: 922, Loss: 0.06728825718164444, Acc: 0.935268223285675\n",
      "Epoch: 923, Loss: 0.0751141607761383, Acc: 0.9415097236633301\n",
      "Epoch: 924, Loss: 0.07697487622499466, Acc: 0.940660834312439\n",
      "Epoch: 925, Loss: 0.09960941970348358, Acc: 0.9411135315895081\n",
      "Epoch: 926, Loss: 0.0813940167427063, Acc: 0.9440318942070007\n",
      "Epoch: 927, Loss: 0.0670846551656723, Acc: 0.9340940117835999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 928, Loss: 0.07957851886749268, Acc: 0.9389044046401978\n",
      "Epoch: 929, Loss: 0.08044113218784332, Acc: 0.9466609954833984\n",
      "Epoch: 930, Loss: 0.09190697222948074, Acc: 0.9459144473075867\n",
      "Epoch: 931, Loss: 0.06765029579401016, Acc: 0.9356625080108643\n",
      "Epoch: 932, Loss: 0.06513015180826187, Acc: 0.9391251802444458\n",
      "Epoch: 933, Loss: 0.070661760866642, Acc: 0.9374567270278931\n",
      "Epoch: 934, Loss: 0.07015962898731232, Acc: 0.9410449266433716\n",
      "Epoch: 935, Loss: 0.07763822376728058, Acc: 0.9381230473518372\n",
      "Epoch: 936, Loss: 0.09130577743053436, Acc: 0.9447386860847473\n",
      "Epoch: 937, Loss: 0.08262323588132858, Acc: 0.9425159096717834\n",
      "Epoch: 938, Loss: 0.06545271724462509, Acc: 0.944566011428833\n",
      "Epoch: 939, Loss: 0.07530376315116882, Acc: 0.9352693557739258\n",
      "Epoch: 940, Loss: 0.07438850402832031, Acc: 0.9387646317481995\n",
      "Epoch: 941, Loss: 0.06810338795185089, Acc: 0.9351852536201477\n",
      "Epoch: 942, Loss: 0.06946022808551788, Acc: 0.9413973093032837\n",
      "Epoch: 943, Loss: 0.06682602316141129, Acc: 0.9368221163749695\n",
      "Epoch: 944, Loss: 0.06716802716255188, Acc: 0.9340651035308838\n",
      "Epoch: 945, Loss: 0.06487932801246643, Acc: 0.9346745014190674\n",
      "Epoch: 946, Loss: 0.0735207051038742, Acc: 0.9434991478919983\n",
      "Epoch: 947, Loss: 0.08138874173164368, Acc: 0.9338091015815735\n",
      "Epoch: 948, Loss: 0.07617029547691345, Acc: 0.9371774196624756\n",
      "Epoch: 949, Loss: 0.07584458589553833, Acc: 0.9399434924125671\n",
      "Epoch: 950, Loss: 0.06596598774194717, Acc: 0.9351190328598022\n",
      "Epoch: 951, Loss: 0.08598139882087708, Acc: 0.9465749263763428\n",
      "Epoch: 952, Loss: 0.0805891901254654, Acc: 0.9444981217384338\n",
      "Epoch: 953, Loss: 0.0748041644692421, Acc: 0.9411836862564087\n",
      "Epoch: 954, Loss: 0.0767170712351799, Acc: 0.9395523071289062\n",
      "Epoch: 955, Loss: 0.07067645341157913, Acc: 0.937717854976654\n",
      "Epoch: 956, Loss: 0.06759580224752426, Acc: 0.9361473917961121\n",
      "Epoch: 957, Loss: 0.08125294744968414, Acc: 0.9433189630508423\n",
      "Epoch: 958, Loss: 0.06451046466827393, Acc: 0.9394781589508057\n",
      "Epoch: 959, Loss: 0.07427166402339935, Acc: 0.9377112984657288\n",
      "Epoch: 960, Loss: 0.06994818150997162, Acc: 0.9349754452705383\n",
      "Epoch: 961, Loss: 0.06848947703838348, Acc: 0.9345958828926086\n",
      "Epoch: 962, Loss: 0.07533539831638336, Acc: 0.9358105063438416\n",
      "Epoch: 963, Loss: 0.06818389892578125, Acc: 0.9419242143630981\n",
      "Epoch: 964, Loss: 0.0777091234922409, Acc: 0.9400829672813416\n",
      "Epoch: 965, Loss: 0.07402017712593079, Acc: 0.9443066716194153\n",
      "Epoch: 966, Loss: 0.1022743433713913, Acc: 0.9535724520683289\n",
      "Epoch: 967, Loss: 0.0730036199092865, Acc: 0.9369233846664429\n",
      "Epoch: 968, Loss: 0.09413577616214752, Acc: 0.9490233659744263\n",
      "Epoch: 969, Loss: 0.06786919385194778, Acc: 0.9350591897964478\n",
      "Epoch: 970, Loss: 0.07547424733638763, Acc: 0.9456245303153992\n",
      "Epoch: 971, Loss: 0.08186772465705872, Acc: 0.941386878490448\n",
      "Epoch: 972, Loss: 0.07715744525194168, Acc: 0.9351388812065125\n",
      "Epoch: 973, Loss: 0.08483219146728516, Acc: 0.943390965461731\n",
      "Epoch: 974, Loss: 0.08558371663093567, Acc: 0.9456180334091187\n",
      "Epoch: 975, Loss: 0.06432628631591797, Acc: 0.9349465370178223\n",
      "Epoch: 976, Loss: 0.07331755757331848, Acc: 0.9357110261917114\n",
      "Epoch: 977, Loss: 0.08335664868354797, Acc: 0.9418715238571167\n",
      "Epoch: 978, Loss: 0.08234487473964691, Acc: 0.9420683979988098\n",
      "Epoch: 979, Loss: 0.06761997938156128, Acc: 0.9413747787475586\n",
      "Epoch: 980, Loss: 0.07953391969203949, Acc: 0.9419911503791809\n",
      "Epoch: 981, Loss: 0.06678565591573715, Acc: 0.9392891526222229\n",
      "Epoch: 982, Loss: 0.07036955654621124, Acc: 0.9375299215316772\n",
      "Epoch: 983, Loss: 0.07247357815504074, Acc: 0.9407538175582886\n",
      "Epoch: 984, Loss: 0.06220110505819321, Acc: 0.9343132972717285\n",
      "Epoch: 985, Loss: 0.07847540080547333, Acc: 0.9420210123062134\n",
      "Epoch: 986, Loss: 0.08351688086986542, Acc: 0.9457054734230042\n",
      "Epoch: 987, Loss: 0.06333253532648087, Acc: 0.9348118901252747\n",
      "Epoch: 988, Loss: 0.06802484393119812, Acc: 0.9382158517837524\n",
      "Epoch: 989, Loss: 0.07912848889827728, Acc: 0.9468131065368652\n",
      "Epoch: 990, Loss: 0.09217357635498047, Acc: 0.9262083172798157\n",
      "Epoch: 991, Loss: 0.07337868213653564, Acc: 0.9414542317390442\n",
      "Epoch: 992, Loss: 0.07245704531669617, Acc: 0.9401423931121826\n",
      "Epoch: 993, Loss: 0.07020348310470581, Acc: 0.9450796246528625\n",
      "Epoch: 994, Loss: 0.06351833790540695, Acc: 0.9370791912078857\n",
      "Epoch: 995, Loss: 0.07436374574899673, Acc: 0.9422730207443237\n",
      "Epoch: 996, Loss: 0.07515300065279007, Acc: 0.9406325817108154\n",
      "Epoch: 997, Loss: 0.06875582039356232, Acc: 0.9404146075248718\n",
      "Epoch: 998, Loss: 0.07343222200870514, Acc: 0.937428891658783\n",
      "Epoch: 999, Loss: 0.07135877013206482, Acc: 0.9387540221214294\n",
      "Epoch: 1000, Loss: 0.07196333259344101, Acc: 0.9421831965446472\n",
      "Epoch: 1001, Loss: 0.070671945810318, Acc: 0.9389073848724365\n",
      "Epoch: 1002, Loss: 0.08307941257953644, Acc: 0.9454065561294556\n",
      "Epoch: 1003, Loss: 0.07170335948467255, Acc: 0.9481021165847778\n",
      "Epoch: 1004, Loss: 0.06907285004854202, Acc: 0.9364548921585083\n",
      "Epoch: 1005, Loss: 0.07493122667074203, Acc: 0.9386011362075806\n",
      "Epoch: 1006, Loss: 0.07614907622337341, Acc: 0.9388125538825989\n",
      "Epoch: 1007, Loss: 0.07216331362724304, Acc: 0.9356335997581482\n",
      "Epoch: 1008, Loss: 0.07435563951730728, Acc: 0.9413913488388062\n",
      "Epoch: 1009, Loss: 0.07472887635231018, Acc: 0.9495657682418823\n",
      "Epoch: 1010, Loss: 0.0673215240240097, Acc: 0.9404609799385071\n",
      "Epoch: 1011, Loss: 0.0771244466304779, Acc: 0.9423442482948303\n",
      "Epoch: 1012, Loss: 0.08056138455867767, Acc: 0.9393746852874756\n",
      "Epoch: 1013, Loss: 0.0631905272603035, Acc: 0.9381995797157288\n",
      "Epoch: 1014, Loss: 0.06255994737148285, Acc: 0.939288318157196\n",
      "Epoch: 1015, Loss: 0.07324665784835815, Acc: 0.939880907535553\n",
      "Epoch: 1016, Loss: 0.0655483677983284, Acc: 0.9366925954818726\n",
      "Epoch: 1017, Loss: 0.06961265206336975, Acc: 0.9421157240867615\n",
      "Epoch: 1018, Loss: 0.06850479543209076, Acc: 0.9457201361656189\n",
      "Epoch: 1019, Loss: 0.06969709694385529, Acc: 0.9396543502807617\n",
      "Epoch: 1020, Loss: 0.06817301362752914, Acc: 0.9372737407684326\n",
      "Epoch: 1021, Loss: 0.07354940474033356, Acc: 0.9367685914039612\n",
      "Epoch: 1022, Loss: 0.0619877353310585, Acc: 0.9433262348175049\n",
      "Epoch: 1023, Loss: 0.07813214510679245, Acc: 0.9403372406959534\n",
      "Epoch: 1024, Loss: 0.0733518972992897, Acc: 0.9391438364982605\n",
      "Epoch: 1025, Loss: 0.08183728158473969, Acc: 0.9403889179229736\n",
      "Epoch: 1026, Loss: 0.09268207848072052, Acc: 0.9495728015899658\n",
      "Epoch: 1027, Loss: 0.0882033184170723, Acc: 0.9485400915145874\n",
      "Epoch: 1028, Loss: 0.07785484939813614, Acc: 0.9453359842300415\n",
      "Epoch: 1029, Loss: 0.07353351265192032, Acc: 0.9389032125473022\n",
      "Epoch: 1030, Loss: 0.07521416246891022, Acc: 0.9427689909934998\n",
      "Epoch: 1031, Loss: 0.06625348329544067, Acc: 0.938615083694458\n",
      "Epoch: 1032, Loss: 0.06505963206291199, Acc: 0.935884952545166\n",
      "Epoch: 1033, Loss: 0.06784405559301376, Acc: 0.9409105777740479\n",
      "Epoch: 1034, Loss: 0.0796421468257904, Acc: 0.9477987885475159\n",
      "Epoch: 1035, Loss: 0.0801817923784256, Acc: 0.9431871175765991\n",
      "Epoch: 1036, Loss: 0.0716085433959961, Acc: 0.9374472498893738\n",
      "Epoch: 1037, Loss: 0.07494675368070602, Acc: 0.9400069117546082\n",
      "Epoch: 1038, Loss: 0.07390892505645752, Acc: 0.9419456124305725\n",
      "Epoch: 1039, Loss: 0.06238213926553726, Acc: 0.942614734172821\n",
      "Epoch: 1040, Loss: 0.08659035712480545, Acc: 0.9464831948280334\n",
      "Epoch: 1041, Loss: 0.07017142325639725, Acc: 0.9392908215522766\n",
      "Epoch: 1042, Loss: 0.07361236214637756, Acc: 0.9373188018798828\n",
      "Epoch: 1043, Loss: 0.06656350195407867, Acc: 0.9368833899497986\n",
      "Epoch: 1044, Loss: 0.0637810081243515, Acc: 0.9408936500549316\n",
      "Epoch: 1045, Loss: 0.08368506282567978, Acc: 0.9549846649169922\n",
      "Epoch: 1046, Loss: 0.10309411585330963, Acc: 0.9405885934829712\n",
      "Epoch: 1047, Loss: 0.0671466737985611, Acc: 0.9386503100395203\n",
      "Epoch: 1048, Loss: 0.0680546760559082, Acc: 0.9394471645355225\n",
      "Epoch: 1049, Loss: 0.073096364736557, Acc: 0.9456477165222168\n",
      "Epoch: 1050, Loss: 0.07253637909889221, Acc: 0.9390385746955872\n",
      "Epoch: 1051, Loss: 0.07881170511245728, Acc: 0.949266254901886\n",
      "Epoch: 1052, Loss: 0.08287058025598526, Acc: 0.9517242908477783\n",
      "Epoch: 1053, Loss: 0.07411075383424759, Acc: 0.941632091999054\n",
      "Epoch: 1054, Loss: 0.0699712485074997, Acc: 0.9422591924667358\n",
      "Epoch: 1055, Loss: 0.07565107196569443, Acc: 0.9516536593437195\n",
      "Epoch: 1056, Loss: 0.0690908432006836, Acc: 0.9394460916519165\n",
      "Epoch: 1057, Loss: 0.0690002590417862, Acc: 0.9403226971626282\n",
      "Epoch: 1058, Loss: 0.06309211254119873, Acc: 0.9377117156982422\n",
      "Epoch: 1059, Loss: 0.07070783525705338, Acc: 0.9426484704017639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1060, Loss: 0.07605303823947906, Acc: 0.9472310543060303\n",
      "Epoch: 1061, Loss: 0.0731269121170044, Acc: 0.938657283782959\n",
      "Epoch: 1062, Loss: 0.08910025656223297, Acc: 0.9489732980728149\n",
      "Epoch: 1063, Loss: 0.08021293580532074, Acc: 0.945064127445221\n",
      "Epoch: 1064, Loss: 0.07596251368522644, Acc: 0.9366203546524048\n",
      "Epoch: 1065, Loss: 0.07142273336648941, Acc: 0.9407721757888794\n",
      "Epoch: 1066, Loss: 0.06096770614385605, Acc: 0.9359766840934753\n",
      "Epoch: 1067, Loss: 0.06974498927593231, Acc: 0.941669762134552\n",
      "Epoch: 1068, Loss: 0.07207418978214264, Acc: 0.9419873356819153\n",
      "Epoch: 1069, Loss: 0.07346716523170471, Acc: 0.9403966069221497\n",
      "Epoch: 1070, Loss: 0.06885601580142975, Acc: 0.9395121932029724\n",
      "Epoch: 1071, Loss: 0.07336553931236267, Acc: 0.946216881275177\n",
      "Epoch: 1072, Loss: 0.0725182443857193, Acc: 0.9391001462936401\n",
      "Epoch: 1073, Loss: 0.06392046809196472, Acc: 0.9409017562866211\n",
      "Epoch: 1074, Loss: 0.0660596638917923, Acc: 0.9360230565071106\n",
      "Epoch: 1075, Loss: 0.06596393883228302, Acc: 0.9379314184188843\n",
      "Epoch: 1076, Loss: 0.06381257623434067, Acc: 0.9381817579269409\n",
      "Epoch: 1077, Loss: 0.07280773669481277, Acc: 0.9420084953308105\n",
      "Epoch: 1078, Loss: 0.07002558559179306, Acc: 0.9400822520256042\n",
      "Epoch: 1079, Loss: 0.07172562181949615, Acc: 0.9396573305130005\n",
      "Epoch: 1080, Loss: 0.07230974733829498, Acc: 0.9438107013702393\n",
      "Epoch: 1081, Loss: 0.0798409953713417, Acc: 0.9407788515090942\n",
      "Epoch: 1082, Loss: 0.0664772242307663, Acc: 0.9418891072273254\n",
      "Epoch: 1083, Loss: 0.07155202329158783, Acc: 0.9417554140090942\n",
      "Epoch: 1084, Loss: 0.07591947913169861, Acc: 0.9428132176399231\n",
      "Epoch: 1085, Loss: 0.07459509372711182, Acc: 0.9396938681602478\n",
      "Epoch: 1086, Loss: 0.06856709718704224, Acc: 0.9388121962547302\n",
      "Epoch: 1087, Loss: 0.07668710500001907, Acc: 0.9462236762046814\n",
      "Epoch: 1088, Loss: 0.06797479093074799, Acc: 0.9397836327552795\n",
      "Epoch: 1089, Loss: 0.06285645067691803, Acc: 0.9373601078987122\n",
      "Epoch: 1090, Loss: 0.06514514982700348, Acc: 0.9411433935165405\n",
      "Epoch: 1091, Loss: 0.09191566705703735, Acc: 0.9504282474517822\n",
      "Epoch: 1092, Loss: 0.08058726787567139, Acc: 0.9388962388038635\n",
      "Epoch: 1093, Loss: 0.0759422779083252, Acc: 0.942177951335907\n",
      "Epoch: 1094, Loss: 0.08494462072849274, Acc: 0.9438962340354919\n",
      "Epoch: 1095, Loss: 0.0685441642999649, Acc: 0.9427273869514465\n",
      "Epoch: 1096, Loss: 0.078738272190094, Acc: 0.9419621825218201\n",
      "Epoch: 1097, Loss: 0.07361108809709549, Acc: 0.9465019702911377\n",
      "Epoch: 1098, Loss: 0.09884215146303177, Acc: 0.9409878849983215\n",
      "Epoch: 1099, Loss: 0.07552559673786163, Acc: 0.9471724033355713\n",
      "Epoch: 1100, Loss: 0.072414331138134, Acc: 0.9461630582809448\n",
      "Epoch: 1101, Loss: 0.06923931837081909, Acc: 0.9376443028450012\n",
      "Epoch: 1102, Loss: 0.06303077191114426, Acc: 0.9423944354057312\n",
      "Epoch: 1103, Loss: 0.0830790102481842, Acc: 0.9452906250953674\n",
      "Epoch: 1104, Loss: 0.06245163455605507, Acc: 0.9415190815925598\n",
      "Epoch: 1105, Loss: 0.0640595331788063, Acc: 0.9401654005050659\n",
      "Epoch: 1106, Loss: 0.07559787482023239, Acc: 0.9417333602905273\n",
      "Epoch: 1107, Loss: 0.06504043191671371, Acc: 0.9388227462768555\n",
      "Epoch: 1108, Loss: 0.06905670464038849, Acc: 0.9445086121559143\n",
      "Epoch: 1109, Loss: 0.07238297164440155, Acc: 0.938433825969696\n",
      "Epoch: 1110, Loss: 0.07675158977508545, Acc: 0.9440418481826782\n",
      "Epoch: 1111, Loss: 0.06867519021034241, Acc: 0.9373844265937805\n",
      "Epoch: 1112, Loss: 0.06854245066642761, Acc: 0.9386992454528809\n",
      "Epoch: 1113, Loss: 0.07541902363300323, Acc: 0.9376451969146729\n",
      "Epoch: 1114, Loss: 0.07173694670200348, Acc: 0.942969024181366\n",
      "Epoch: 1115, Loss: 0.07994216680526733, Acc: 0.9461965560913086\n",
      "Epoch: 1116, Loss: 0.07558933645486832, Acc: 0.9366015791893005\n",
      "Epoch: 1117, Loss: 0.07383822649717331, Acc: 0.941464364528656\n",
      "Epoch: 1118, Loss: 0.06973198801279068, Acc: 0.9391536116600037\n",
      "Epoch: 1119, Loss: 0.0723143145442009, Acc: 0.9397658109664917\n",
      "Epoch: 1120, Loss: 0.06320091336965561, Acc: 0.9388961791992188\n",
      "Epoch: 1121, Loss: 0.060936860740184784, Acc: 0.9473528861999512\n",
      "Epoch: 1122, Loss: 0.06262009590864182, Acc: 0.9375526309013367\n",
      "Epoch: 1123, Loss: 0.08319922536611557, Acc: 0.9508501291275024\n",
      "Epoch: 1124, Loss: 0.07248951494693756, Acc: 0.9445536732673645\n",
      "Epoch: 1125, Loss: 0.06494678556919098, Acc: 0.9381749033927917\n",
      "Epoch: 1126, Loss: 0.06828733533620834, Acc: 0.9454683065414429\n",
      "Epoch: 1127, Loss: 0.06535227596759796, Acc: 0.938023567199707\n",
      "Epoch: 1128, Loss: 0.063746377825737, Acc: 0.9381248950958252\n",
      "Epoch: 1129, Loss: 0.08646068722009659, Acc: 0.9438818693161011\n",
      "Epoch: 1130, Loss: 0.06987547874450684, Acc: 0.9381484985351562\n",
      "Epoch: 1131, Loss: 0.06289441883563995, Acc: 0.9370847344398499\n",
      "Epoch: 1132, Loss: 0.07312803715467453, Acc: 0.9453984498977661\n",
      "Epoch: 1133, Loss: 0.06595233082771301, Acc: 0.9401369094848633\n",
      "Epoch: 1134, Loss: 0.06600604206323624, Acc: 0.9413697123527527\n",
      "Epoch: 1135, Loss: 0.0730748102068901, Acc: 0.9392618536949158\n",
      "Epoch: 1136, Loss: 0.09204112738370895, Acc: 0.9516217708587646\n",
      "Epoch: 1137, Loss: 0.07256309688091278, Acc: 0.9462673664093018\n",
      "Epoch: 1138, Loss: 0.06812550127506256, Acc: 0.9408099055290222\n",
      "Epoch: 1139, Loss: 0.0721842348575592, Acc: 0.9407530426979065\n",
      "Epoch: 1140, Loss: 0.06500647217035294, Acc: 0.9414231181144714\n",
      "Epoch: 1141, Loss: 0.10810032486915588, Acc: 0.9643967151641846\n",
      "Epoch: 1142, Loss: 0.06279576569795609, Acc: 0.9392753839492798\n",
      "Epoch: 1143, Loss: 0.06844712793827057, Acc: 0.9420633912086487\n",
      "Epoch: 1144, Loss: 0.07194773852825165, Acc: 0.9420728087425232\n",
      "Epoch: 1145, Loss: 0.06435739994049072, Acc: 0.9389451742172241\n",
      "Epoch: 1146, Loss: 0.08002791553735733, Acc: 0.9477391242980957\n",
      "Epoch: 1147, Loss: 0.06804092228412628, Acc: 0.9461760520935059\n",
      "Epoch: 1148, Loss: 0.07913453876972198, Acc: 0.9477845430374146\n",
      "Epoch: 1149, Loss: 0.08504875749349594, Acc: 0.9421564340591431\n",
      "Epoch: 1150, Loss: 0.06592649966478348, Acc: 0.9401564002037048\n",
      "Epoch: 1151, Loss: 0.06851358711719513, Acc: 0.9424803256988525\n",
      "Epoch: 1152, Loss: 0.07467687875032425, Acc: 0.9414605498313904\n",
      "Epoch: 1153, Loss: 0.08194933831691742, Acc: 0.938991367816925\n",
      "Epoch: 1154, Loss: 0.07891099154949188, Acc: 0.9552221298217773\n",
      "Epoch: 1155, Loss: 0.06127142533659935, Acc: 0.9451626539230347\n",
      "Epoch: 1156, Loss: 0.0734190121293068, Acc: 0.9424968957901001\n",
      "Epoch: 1157, Loss: 0.07924404740333557, Acc: 0.9478858709335327\n",
      "Epoch: 1158, Loss: 0.07780306786298752, Acc: 0.9357011914253235\n",
      "Epoch: 1159, Loss: 0.08852097392082214, Acc: 0.948190450668335\n",
      "Epoch: 1160, Loss: 0.09894104301929474, Acc: 0.9494110941886902\n",
      "Epoch: 1161, Loss: 0.07231186330318451, Acc: 0.9413180351257324\n",
      "Epoch: 1162, Loss: 0.07805013656616211, Acc: 0.9318198561668396\n",
      "Epoch: 1163, Loss: 0.0808349996805191, Acc: 0.9430686235427856\n",
      "Epoch: 1164, Loss: 0.07326199114322662, Acc: 0.9443907737731934\n",
      "Epoch: 1165, Loss: 0.06364265084266663, Acc: 0.9393548369407654\n",
      "Epoch: 1166, Loss: 0.07041814923286438, Acc: 0.9387300610542297\n",
      "Epoch: 1167, Loss: 0.06174556910991669, Acc: 0.9378847479820251\n",
      "Epoch: 1168, Loss: 0.06903250515460968, Acc: 0.943790853023529\n",
      "Epoch: 1169, Loss: 0.07536115497350693, Acc: 0.9412569403648376\n",
      "Epoch: 1170, Loss: 0.08295198529958725, Acc: 0.9389541149139404\n",
      "Epoch: 1171, Loss: 0.06448221951723099, Acc: 0.9378647208213806\n",
      "Epoch: 1172, Loss: 0.07347111403942108, Acc: 0.9433605074882507\n",
      "Epoch: 1173, Loss: 0.06522833555936813, Acc: 0.9400007724761963\n",
      "Epoch: 1174, Loss: 0.06269091367721558, Acc: 0.9401106834411621\n",
      "Epoch: 1175, Loss: 0.06629861891269684, Acc: 0.9393815398216248\n",
      "Epoch: 1176, Loss: 0.06615357100963593, Acc: 0.9409533739089966\n",
      "Epoch: 1177, Loss: 0.08322421461343765, Acc: 0.9584680199623108\n",
      "Epoch: 1178, Loss: 0.075400210916996, Acc: 0.9427817463874817\n",
      "Epoch: 1179, Loss: 0.0685044676065445, Acc: 0.9533090591430664\n",
      "Epoch: 1180, Loss: 0.0661110058426857, Acc: 0.9389566779136658\n",
      "Epoch: 1181, Loss: 0.08033403009176254, Acc: 0.9594360589981079\n",
      "Epoch: 1182, Loss: 0.07102710008621216, Acc: 0.9395752549171448\n",
      "Epoch: 1183, Loss: 0.06762591004371643, Acc: 0.944054126739502\n",
      "Epoch: 1184, Loss: 0.07446622848510742, Acc: 0.9406526684761047\n",
      "Epoch: 1185, Loss: 0.0679841935634613, Acc: 0.9423278570175171\n",
      "Epoch: 1186, Loss: 0.07972252368927002, Acc: 0.9535356760025024\n",
      "Epoch: 1187, Loss: 0.07522732764482498, Acc: 0.9416345953941345\n",
      "Epoch: 1188, Loss: 0.06142932549118996, Acc: 0.94009929895401\n",
      "Epoch: 1189, Loss: 0.07492324709892273, Acc: 0.9475088715553284\n",
      "Epoch: 1190, Loss: 0.06298235803842545, Acc: 0.9417387247085571\n",
      "Epoch: 1191, Loss: 0.08331465721130371, Acc: 0.9488328099250793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1192, Loss: 0.06187243387103081, Acc: 0.9487888813018799\n",
      "Epoch: 1193, Loss: 0.07833226025104523, Acc: 0.9450902342796326\n",
      "Epoch: 1194, Loss: 0.06969477236270905, Acc: 0.9385733604431152\n",
      "Epoch: 1195, Loss: 0.07245903462171555, Acc: 0.9417926669120789\n",
      "Epoch: 1196, Loss: 0.07621879875659943, Acc: 0.9483981728553772\n",
      "Epoch: 1197, Loss: 0.0843677893280983, Acc: 0.9405930638313293\n",
      "Epoch: 1198, Loss: 0.05953260511159897, Acc: 0.9398939609527588\n",
      "Epoch: 1199, Loss: 0.06591431051492691, Acc: 0.9452856779098511\n",
      "Epoch: 1200, Loss: 0.06795922666788101, Acc: 0.9418872594833374\n",
      "Epoch: 1201, Loss: 0.06949859857559204, Acc: 0.9471994638442993\n",
      "Epoch: 1202, Loss: 0.0792069286108017, Acc: 0.9428794384002686\n",
      "Epoch: 1203, Loss: 0.07608883827924728, Acc: 0.9493052959442139\n",
      "Epoch: 1204, Loss: 0.07476544380187988, Acc: 0.9431957602500916\n",
      "Epoch: 1205, Loss: 0.07093434035778046, Acc: 0.9431982040405273\n",
      "Epoch: 1206, Loss: 0.06201111152768135, Acc: 0.9388554096221924\n",
      "Epoch: 1207, Loss: 0.06279352307319641, Acc: 0.9385717511177063\n",
      "Epoch: 1208, Loss: 0.07229235023260117, Acc: 0.9495958685874939\n",
      "Epoch: 1209, Loss: 0.09485332667827606, Acc: 0.9426942467689514\n",
      "Epoch: 1210, Loss: 0.06386332958936691, Acc: 0.9423102140426636\n",
      "Epoch: 1211, Loss: 0.06744621694087982, Acc: 0.9414803385734558\n",
      "Epoch: 1212, Loss: 0.06545908749103546, Acc: 0.9440074563026428\n",
      "Epoch: 1213, Loss: 0.06446373462677002, Acc: 0.9416437149047852\n",
      "Epoch: 1214, Loss: 0.06636703759431839, Acc: 0.9386066198348999\n",
      "Epoch: 1215, Loss: 0.06556357443332672, Acc: 0.9463188648223877\n",
      "Epoch: 1216, Loss: 0.06197093054652214, Acc: 0.941335141658783\n",
      "Epoch: 1217, Loss: 0.07118168473243713, Acc: 0.9440478682518005\n",
      "Epoch: 1218, Loss: 0.06220172345638275, Acc: 0.9388435482978821\n",
      "Epoch: 1219, Loss: 0.063687764108181, Acc: 0.9380810260772705\n",
      "Epoch: 1220, Loss: 0.061568789184093475, Acc: 0.9420564770698547\n",
      "Epoch: 1221, Loss: 0.06539328396320343, Acc: 0.9429404139518738\n",
      "Epoch: 1222, Loss: 0.061309508979320526, Acc: 0.9387143850326538\n",
      "Epoch: 1223, Loss: 0.06567016243934631, Acc: 0.9440326690673828\n",
      "Epoch: 1224, Loss: 0.07546113431453705, Acc: 0.9427950382232666\n",
      "Epoch: 1225, Loss: 0.06887995451688766, Acc: 0.9444500207901001\n",
      "Epoch: 1226, Loss: 0.07219740003347397, Acc: 0.9598320722579956\n",
      "Epoch: 1227, Loss: 0.07258830964565277, Acc: 0.9411064386367798\n",
      "Epoch: 1228, Loss: 0.07858917117118835, Acc: 0.948991060256958\n",
      "Epoch: 1229, Loss: 0.0659000426530838, Acc: 0.9421141743659973\n",
      "Epoch: 1230, Loss: 0.07918086647987366, Acc: 0.9452642202377319\n",
      "Epoch: 1231, Loss: 0.07564006745815277, Acc: 0.9448509216308594\n",
      "Epoch: 1232, Loss: 0.09389322996139526, Acc: 0.9595780372619629\n",
      "Epoch: 1233, Loss: 0.06682676076889038, Acc: 0.9390228986740112\n",
      "Epoch: 1234, Loss: 0.07293184101581573, Acc: 0.9498247504234314\n",
      "Epoch: 1235, Loss: 0.06303118169307709, Acc: 0.9398491382598877\n",
      "Epoch: 1236, Loss: 0.06271839141845703, Acc: 0.9420850872993469\n",
      "Epoch: 1237, Loss: 0.06824354827404022, Acc: 0.9440743923187256\n",
      "Epoch: 1238, Loss: 0.0710439383983612, Acc: 0.9400789737701416\n",
      "Epoch: 1239, Loss: 0.06704454123973846, Acc: 0.9446319937705994\n",
      "Epoch: 1240, Loss: 0.06906695663928986, Acc: 0.9471191763877869\n",
      "Epoch: 1241, Loss: 0.0882079005241394, Acc: 0.9544897079467773\n",
      "Epoch: 1242, Loss: 0.0712084174156189, Acc: 0.9430986046791077\n",
      "Epoch: 1243, Loss: 0.07096532732248306, Acc: 0.940712571144104\n",
      "Epoch: 1244, Loss: 0.07391445338726044, Acc: 0.9441763162612915\n",
      "Epoch: 1245, Loss: 0.06861985474824905, Acc: 0.9523606896400452\n",
      "Epoch: 1246, Loss: 0.07119204103946686, Acc: 0.9392263889312744\n",
      "Epoch: 1247, Loss: 0.07696832716464996, Acc: 0.9509413242340088\n",
      "Epoch: 1248, Loss: 0.08265330642461777, Acc: 0.9430356621742249\n",
      "Epoch: 1249, Loss: 0.08037999272346497, Acc: 0.9494139552116394\n",
      "Epoch: 1250, Loss: 0.08657583594322205, Acc: 0.9445114135742188\n",
      "Epoch: 1251, Loss: 0.06651685386896133, Acc: 0.9441252946853638\n",
      "Epoch: 1252, Loss: 0.067377969622612, Acc: 0.9461047649383545\n",
      "Epoch: 1253, Loss: 0.07107014954090118, Acc: 0.9424157738685608\n",
      "Epoch: 1254, Loss: 0.07743048667907715, Acc: 0.9504023790359497\n",
      "Epoch: 1255, Loss: 0.07074986398220062, Acc: 0.9508811235427856\n",
      "Epoch: 1256, Loss: 0.08091853559017181, Acc: 0.9442145228385925\n",
      "Epoch: 1257, Loss: 0.0590694285929203, Acc: 0.9483268857002258\n",
      "Epoch: 1258, Loss: 0.07110736519098282, Acc: 0.9407016634941101\n",
      "Epoch: 1259, Loss: 0.06536747515201569, Acc: 0.9426313042640686\n",
      "Epoch: 1260, Loss: 0.06329755485057831, Acc: 0.9418141841888428\n",
      "Epoch: 1261, Loss: 0.05983082577586174, Acc: 0.9389108419418335\n",
      "Epoch: 1262, Loss: 0.08514859527349472, Acc: 0.9529939889907837\n",
      "Epoch: 1263, Loss: 0.05794890597462654, Acc: 0.9440242052078247\n",
      "Epoch: 1264, Loss: 0.06484754383563995, Acc: 0.9405168294906616\n",
      "Epoch: 1265, Loss: 0.06707354635000229, Acc: 0.9456678032875061\n",
      "Epoch: 1266, Loss: 0.06126265600323677, Acc: 0.9415654540061951\n",
      "Epoch: 1267, Loss: 0.07261129468679428, Acc: 0.95973140001297\n",
      "Epoch: 1268, Loss: 0.07475557178258896, Acc: 0.9402827024459839\n",
      "Epoch: 1269, Loss: 0.06765711307525635, Acc: 0.9435121417045593\n",
      "Epoch: 1270, Loss: 0.06801430881023407, Acc: 0.9483749866485596\n",
      "Epoch: 1271, Loss: 0.07630132138729095, Acc: 0.9437839984893799\n",
      "Epoch: 1272, Loss: 0.07002445310354233, Acc: 0.9461461901664734\n",
      "Epoch: 1273, Loss: 0.06918719410896301, Acc: 0.9431551098823547\n",
      "Epoch: 1274, Loss: 0.06546562910079956, Acc: 0.9461112022399902\n",
      "Epoch: 1275, Loss: 0.0683068186044693, Acc: 0.9503510594367981\n",
      "Epoch: 1276, Loss: 0.06747747212648392, Acc: 0.9471249580383301\n",
      "Epoch: 1277, Loss: 0.0744684487581253, Acc: 0.9401662945747375\n",
      "Epoch: 1278, Loss: 0.06302240490913391, Acc: 0.9406827092170715\n",
      "Epoch: 1279, Loss: 0.07376084476709366, Acc: 0.9474380612373352\n",
      "Epoch: 1280, Loss: 0.06851956248283386, Acc: 0.9449589848518372\n",
      "Epoch: 1281, Loss: 0.06339193135499954, Acc: 0.9403472542762756\n",
      "Epoch: 1282, Loss: 0.06999638676643372, Acc: 0.9449759125709534\n",
      "Epoch: 1283, Loss: 0.07991161942481995, Acc: 0.9466150403022766\n",
      "Epoch: 1284, Loss: 0.07002892345190048, Acc: 0.9468305110931396\n",
      "Epoch: 1285, Loss: 0.06250051409006119, Acc: 0.9410611391067505\n",
      "Epoch: 1286, Loss: 0.06347799301147461, Acc: 0.9412148594856262\n",
      "Epoch: 1287, Loss: 0.06625331938266754, Acc: 0.9401296377182007\n",
      "Epoch: 1288, Loss: 0.07731018215417862, Acc: 0.9494841694831848\n",
      "Epoch: 1289, Loss: 0.06695481389760971, Acc: 0.9463238716125488\n",
      "Epoch: 1290, Loss: 0.07724068313837051, Acc: 0.9379377961158752\n",
      "Epoch: 1291, Loss: 0.0658385381102562, Acc: 0.9456778168678284\n",
      "Epoch: 1292, Loss: 0.08484610915184021, Acc: 0.9538652896881104\n",
      "Epoch: 1293, Loss: 0.07477571070194244, Acc: 0.9411228895187378\n",
      "Epoch: 1294, Loss: 0.08241146802902222, Acc: 0.9493158459663391\n",
      "Epoch: 1295, Loss: 0.10591283440589905, Acc: 0.9545533657073975\n",
      "Epoch: 1296, Loss: 0.0660925954580307, Acc: 0.9419612884521484\n",
      "Epoch: 1297, Loss: 0.06761513650417328, Acc: 0.9427775740623474\n",
      "Epoch: 1298, Loss: 0.06229197978973389, Acc: 0.9418551921844482\n",
      "Epoch: 1299, Loss: 0.06024109944701195, Acc: 0.9486517310142517\n",
      "Epoch: 1300, Loss: 0.062101755291223526, Acc: 0.9603866338729858\n",
      "Epoch: 1301, Loss: 0.07242284715175629, Acc: 0.9425764083862305\n",
      "Epoch: 1302, Loss: 0.07880395650863647, Acc: 0.9431777000427246\n",
      "Epoch: 1303, Loss: 0.06041768938302994, Acc: 0.9431168437004089\n",
      "Epoch: 1304, Loss: 0.07220393419265747, Acc: 0.9488769769668579\n",
      "Epoch: 1305, Loss: 0.07805051654577255, Acc: 0.9514970183372498\n",
      "Epoch: 1306, Loss: 0.07910019904375076, Acc: 0.948495090007782\n",
      "Epoch: 1307, Loss: 0.06874796003103256, Acc: 0.9393733143806458\n",
      "Epoch: 1308, Loss: 0.06259319931268692, Acc: 0.9399812817573547\n",
      "Epoch: 1309, Loss: 0.06548008322715759, Acc: 0.9475030899047852\n",
      "Epoch: 1310, Loss: 0.06198533624410629, Acc: 0.9414861798286438\n",
      "Epoch: 1311, Loss: 0.06731429696083069, Acc: 0.9483776688575745\n",
      "Epoch: 1312, Loss: 0.06316736340522766, Acc: 0.9422690868377686\n",
      "Epoch: 1313, Loss: 0.07200214266777039, Acc: 0.9447013139724731\n",
      "Epoch: 1314, Loss: 0.0656663030385971, Acc: 0.9414829015731812\n",
      "Epoch: 1315, Loss: 0.06836216151714325, Acc: 0.9518592357635498\n",
      "Epoch: 1316, Loss: 0.07372279465198517, Acc: 0.9432562589645386\n",
      "Epoch: 1317, Loss: 0.09334830194711685, Acc: 0.9639090895652771\n",
      "Epoch: 1318, Loss: 0.06280981749296188, Acc: 0.9480199813842773\n",
      "Epoch: 1319, Loss: 0.07317212224006653, Acc: 0.9442610144615173\n",
      "Epoch: 1320, Loss: 0.06898746639490128, Acc: 0.9462723731994629\n",
      "Epoch: 1321, Loss: 0.0665135309100151, Acc: 0.9453347325325012\n",
      "Epoch: 1322, Loss: 0.06162245571613312, Acc: 0.9396314024925232\n",
      "Epoch: 1323, Loss: 0.06448519229888916, Acc: 0.9482756853103638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1324, Loss: 0.06423386931419373, Acc: 0.9556591510772705\n",
      "Epoch: 1325, Loss: 0.06861203163862228, Acc: 0.9394108653068542\n",
      "Epoch: 1326, Loss: 0.07401501387357712, Acc: 0.9505544900894165\n",
      "Epoch: 1327, Loss: 0.06581665575504303, Acc: 0.9506950974464417\n",
      "Epoch: 1328, Loss: 0.0644431933760643, Acc: 0.9490483999252319\n",
      "Epoch: 1329, Loss: 0.06967217475175858, Acc: 0.9452244639396667\n",
      "Epoch: 1330, Loss: 0.07155989110469818, Acc: 0.9475722312927246\n",
      "Epoch: 1331, Loss: 0.07648653537034988, Acc: 0.9434496760368347\n",
      "Epoch: 1332, Loss: 0.07815077155828476, Acc: 0.950038492679596\n",
      "Epoch: 1333, Loss: 0.07325577735900879, Acc: 0.9440172910690308\n",
      "Epoch: 1334, Loss: 0.06786630302667618, Acc: 0.9469270706176758\n",
      "Epoch: 1335, Loss: 0.0655289962887764, Acc: 0.9524824023246765\n",
      "Epoch: 1336, Loss: 0.0726722925901413, Acc: 0.940528392791748\n",
      "Epoch: 1337, Loss: 0.06134014576673508, Acc: 0.9448416233062744\n",
      "Epoch: 1338, Loss: 0.06828860938549042, Acc: 0.9508933424949646\n",
      "Epoch: 1339, Loss: 0.06807921826839447, Acc: 0.9422176480293274\n",
      "Epoch: 1340, Loss: 0.0682811290025711, Acc: 0.943953812122345\n",
      "Epoch: 1341, Loss: 0.071406789124012, Acc: 0.9464774131774902\n",
      "Epoch: 1342, Loss: 0.06838150322437286, Acc: 0.9424787163734436\n",
      "Epoch: 1343, Loss: 0.08745571970939636, Acc: 0.9541113376617432\n",
      "Epoch: 1344, Loss: 0.07365022599697113, Acc: 0.9441078901290894\n",
      "Epoch: 1345, Loss: 0.06166209280490875, Acc: 0.9422771334648132\n",
      "Epoch: 1346, Loss: 0.05969974398612976, Acc: 0.9460327625274658\n",
      "Epoch: 1347, Loss: 0.06379997730255127, Acc: 0.9458185434341431\n",
      "Epoch: 1348, Loss: 0.06257906556129456, Acc: 0.9399116635322571\n",
      "Epoch: 1349, Loss: 0.06847766041755676, Acc: 0.9425570964813232\n",
      "Epoch: 1350, Loss: 0.05978207290172577, Acc: 0.9453620314598083\n",
      "Epoch: 1351, Loss: 0.08783446252346039, Acc: 0.9524388313293457\n",
      "Epoch: 1352, Loss: 0.09219196438789368, Acc: 0.9467779397964478\n",
      "Epoch: 1353, Loss: 0.07289786636829376, Acc: 0.9560431838035583\n",
      "Epoch: 1354, Loss: 0.07064148038625717, Acc: 0.9398261308670044\n",
      "Epoch: 1355, Loss: 0.07247182726860046, Acc: 0.9443121552467346\n",
      "Epoch: 1356, Loss: 0.07769285142421722, Acc: 0.945503294467926\n",
      "Epoch: 1357, Loss: 0.06637302786111832, Acc: 0.9427502155303955\n",
      "Epoch: 1358, Loss: 0.06773459166288376, Acc: 0.9488304853439331\n",
      "Epoch: 1359, Loss: 0.07912808656692505, Acc: 0.9482330679893494\n",
      "Epoch: 1360, Loss: 0.07553400099277496, Acc: 0.9438533186912537\n",
      "Epoch: 1361, Loss: 0.07087591290473938, Acc: 0.9394315481185913\n",
      "Epoch: 1362, Loss: 0.07103166729211807, Acc: 0.9462421536445618\n",
      "Epoch: 1363, Loss: 0.06929421424865723, Acc: 0.9401364326477051\n",
      "Epoch: 1364, Loss: 0.07587546855211258, Acc: 0.9505765438079834\n",
      "Epoch: 1365, Loss: 0.06098167598247528, Acc: 0.940136730670929\n",
      "Epoch: 1366, Loss: 0.06122354418039322, Acc: 0.9437691569328308\n",
      "Epoch: 1367, Loss: 0.05908476933836937, Acc: 0.9424375295639038\n",
      "Epoch: 1368, Loss: 0.07198383659124374, Acc: 0.9456257224082947\n",
      "Epoch: 1369, Loss: 0.06198498606681824, Acc: 0.9407393336296082\n",
      "Epoch: 1370, Loss: 0.07409649342298508, Acc: 0.9486300349235535\n",
      "Epoch: 1371, Loss: 0.06349119544029236, Acc: 0.9434474110603333\n",
      "Epoch: 1372, Loss: 0.06999459117650986, Acc: 0.9460062384605408\n",
      "Epoch: 1373, Loss: 0.06983441859483719, Acc: 0.9549661874771118\n",
      "Epoch: 1374, Loss: 0.06133408471941948, Acc: 0.9416800141334534\n",
      "Epoch: 1375, Loss: 0.08390568941831589, Acc: 0.954892635345459\n",
      "Epoch: 1376, Loss: 0.0642542615532875, Acc: 0.9410794377326965\n",
      "Epoch: 1377, Loss: 0.07237552106380463, Acc: 0.9488258957862854\n",
      "Epoch: 1378, Loss: 0.06570616364479065, Acc: 0.9405040144920349\n",
      "Epoch: 1379, Loss: 0.05990251526236534, Acc: 0.9402197003364563\n",
      "Epoch: 1380, Loss: 0.06912931799888611, Acc: 0.9441537261009216\n",
      "Epoch: 1381, Loss: 0.06245014816522598, Acc: 0.9448623657226562\n",
      "Epoch: 1382, Loss: 0.061041731387376785, Acc: 0.9417717456817627\n",
      "Epoch: 1383, Loss: 0.0654376894235611, Acc: 0.9440891146659851\n",
      "Epoch: 1384, Loss: 0.06523236632347107, Acc: 0.9430081844329834\n",
      "Epoch: 1385, Loss: 0.06321047246456146, Acc: 0.9407105445861816\n",
      "Epoch: 1386, Loss: 0.06830240786075592, Acc: 0.9423211216926575\n",
      "Epoch: 1387, Loss: 0.0773971676826477, Acc: 0.946406364440918\n",
      "Epoch: 1388, Loss: 0.06793475151062012, Acc: 0.9513586759567261\n",
      "Epoch: 1389, Loss: 0.06606125086545944, Acc: 0.9444579482078552\n",
      "Epoch: 1390, Loss: 0.06034117564558983, Acc: 0.9414926171302795\n",
      "Epoch: 1391, Loss: 0.06251160055398941, Acc: 0.9439308643341064\n",
      "Epoch: 1392, Loss: 0.06488654762506485, Acc: 0.9464726448059082\n",
      "Epoch: 1393, Loss: 0.07453818619251251, Acc: 0.9455514550209045\n",
      "Epoch: 1394, Loss: 0.06968402117490768, Acc: 0.9440578818321228\n",
      "Epoch: 1395, Loss: 0.06269947439432144, Acc: 0.9425244927406311\n",
      "Epoch: 1396, Loss: 0.0674082338809967, Acc: 0.9451262354850769\n",
      "Epoch: 1397, Loss: 0.06342387199401855, Acc: 0.9458044171333313\n",
      "Epoch: 1398, Loss: 0.06454002857208252, Acc: 0.9409151077270508\n",
      "Epoch: 1399, Loss: 0.06369781494140625, Acc: 0.942456066608429\n",
      "Epoch: 1400, Loss: 0.06733539700508118, Acc: 0.9452403783798218\n",
      "Epoch: 1401, Loss: 0.06719347089529037, Acc: 0.9422789812088013\n",
      "Epoch: 1402, Loss: 0.05892995744943619, Acc: 0.9431890845298767\n",
      "Epoch: 1403, Loss: 0.060575082898139954, Acc: 0.941990077495575\n",
      "Epoch: 1404, Loss: 0.06945429742336273, Acc: 0.9487286806106567\n",
      "Epoch: 1405, Loss: 0.07425552606582642, Acc: 0.9472193717956543\n",
      "Epoch: 1406, Loss: 0.06572826951742172, Acc: 0.9443245530128479\n",
      "Epoch: 1407, Loss: 0.06291861087083817, Acc: 0.9472447037696838\n",
      "Epoch: 1408, Loss: 0.06420774757862091, Acc: 0.9494093060493469\n",
      "Epoch: 1409, Loss: 0.06943577527999878, Acc: 0.9468820095062256\n",
      "Epoch: 1410, Loss: 0.0718960240483284, Acc: 0.9441964626312256\n",
      "Epoch: 1411, Loss: 0.06430880725383759, Acc: 0.9442800879478455\n",
      "Epoch: 1412, Loss: 0.06354079395532608, Acc: 0.9525196552276611\n",
      "Epoch: 1413, Loss: 0.06655959784984589, Acc: 0.9440110325813293\n",
      "Epoch: 1414, Loss: 0.06796135753393173, Acc: 0.9446588158607483\n",
      "Epoch: 1415, Loss: 0.06956486403942108, Acc: 0.9515344500541687\n",
      "Epoch: 1416, Loss: 0.07605169713497162, Acc: 0.9431201815605164\n",
      "Epoch: 1417, Loss: 0.07855989038944244, Acc: 0.9486510753631592\n",
      "Epoch: 1418, Loss: 0.08906880021095276, Acc: 0.9502829313278198\n",
      "Epoch: 1419, Loss: 0.07104569673538208, Acc: 0.9481049180030823\n",
      "Epoch: 1420, Loss: 0.05890248715877533, Acc: 0.9413363933563232\n",
      "Epoch: 1421, Loss: 0.05944674834609032, Acc: 0.9407489895820618\n",
      "Epoch: 1422, Loss: 0.08140797168016434, Acc: 0.9521074295043945\n",
      "Epoch: 1423, Loss: 0.07297047972679138, Acc: 0.9499198198318481\n",
      "Epoch: 1424, Loss: 0.05722612887620926, Acc: 0.9487633109092712\n",
      "Epoch: 1425, Loss: 0.07090633362531662, Acc: 0.9504683017730713\n",
      "Epoch: 1426, Loss: 0.07462707161903381, Acc: 0.9492908120155334\n",
      "Epoch: 1427, Loss: 0.07058173418045044, Acc: 0.9461856484413147\n",
      "Epoch: 1428, Loss: 0.05910624563694, Acc: 0.9405328631401062\n",
      "Epoch: 1429, Loss: 0.06396415829658508, Acc: 0.9486324787139893\n",
      "Epoch: 1430, Loss: 0.09375547617673874, Acc: 0.9618695974349976\n",
      "Epoch: 1431, Loss: 0.07508978992700577, Acc: 0.9465359449386597\n",
      "Epoch: 1432, Loss: 0.06742475181818008, Acc: 0.9470517039299011\n",
      "Epoch: 1433, Loss: 0.0728568360209465, Acc: 0.9399356842041016\n",
      "Epoch: 1434, Loss: 0.07979661226272583, Acc: 0.9492632150650024\n",
      "Epoch: 1435, Loss: 0.06349791586399078, Acc: 0.9428646564483643\n",
      "Epoch: 1436, Loss: 0.0690397247672081, Acc: 0.9476121664047241\n",
      "Epoch: 1437, Loss: 0.06032335013151169, Acc: 0.9430531859397888\n",
      "Epoch: 1438, Loss: 0.06734158098697662, Acc: 0.9446699619293213\n",
      "Epoch: 1439, Loss: 0.06890606880187988, Acc: 0.9408775568008423\n",
      "Epoch: 1440, Loss: 0.06596983969211578, Acc: 0.9439603686332703\n",
      "Epoch: 1441, Loss: 0.06339335441589355, Acc: 0.9438687562942505\n",
      "Epoch: 1442, Loss: 0.06478989124298096, Acc: 0.9432728290557861\n",
      "Epoch: 1443, Loss: 0.060748860239982605, Acc: 0.9423090219497681\n",
      "Epoch: 1444, Loss: 0.09530100971460342, Acc: 0.973484992980957\n",
      "Epoch: 1445, Loss: 0.09368781745433807, Acc: 0.9405937790870667\n",
      "Epoch: 1446, Loss: 0.05945620313286781, Acc: 0.9535807967185974\n",
      "Epoch: 1447, Loss: 0.07561695575714111, Acc: 0.9502810835838318\n",
      "Epoch: 1448, Loss: 0.07849134504795074, Acc: 0.9493900537490845\n",
      "Epoch: 1449, Loss: 0.0794726312160492, Acc: 0.9436206817626953\n",
      "Epoch: 1450, Loss: 0.059245940297842026, Acc: 0.9423344731330872\n",
      "Epoch: 1451, Loss: 0.06376098096370697, Acc: 0.9496704339981079\n",
      "Epoch: 1452, Loss: 0.06297650188207626, Acc: 0.9439448118209839\n",
      "Epoch: 1453, Loss: 0.07309868186712265, Acc: 0.944172739982605\n",
      "Epoch: 1454, Loss: 0.07622355222702026, Acc: 0.9448015689849854\n",
      "Epoch: 1455, Loss: 0.06117432937026024, Acc: 0.9494138956069946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1456, Loss: 0.08091161400079727, Acc: 0.9515730142593384\n",
      "Epoch: 1457, Loss: 0.06588073074817657, Acc: 0.9518617391586304\n",
      "Epoch: 1458, Loss: 0.06016241014003754, Acc: 0.9411638975143433\n",
      "Epoch: 1459, Loss: 0.0709189623594284, Acc: 0.9435803294181824\n",
      "Epoch: 1460, Loss: 0.07831533253192902, Acc: 0.9518949389457703\n",
      "Epoch: 1461, Loss: 0.0640922337770462, Acc: 0.9509359002113342\n",
      "Epoch: 1462, Loss: 0.08335664868354797, Acc: 0.949124813079834\n",
      "Epoch: 1463, Loss: 0.05840981751680374, Acc: 0.9434393048286438\n",
      "Epoch: 1464, Loss: 0.06380270421504974, Acc: 0.9435601830482483\n",
      "Epoch: 1465, Loss: 0.06956672668457031, Acc: 0.9443579316139221\n",
      "Epoch: 1466, Loss: 0.05828329548239708, Acc: 0.9472980499267578\n",
      "Epoch: 1467, Loss: 0.06821516156196594, Acc: 0.9463791251182556\n",
      "Epoch: 1468, Loss: 0.06534172594547272, Acc: 0.947421133518219\n",
      "Epoch: 1469, Loss: 0.06634065508842468, Acc: 0.9459611177444458\n",
      "Epoch: 1470, Loss: 0.09005077183246613, Acc: 0.948975682258606\n",
      "Epoch: 1471, Loss: 0.05998816341161728, Acc: 0.9448276162147522\n",
      "Epoch: 1472, Loss: 0.07342876493930817, Acc: 0.9480051398277283\n",
      "Epoch: 1473, Loss: 0.05849747732281685, Acc: 0.9501895904541016\n",
      "Epoch: 1474, Loss: 0.07330196350812912, Acc: 0.9468921422958374\n",
      "Epoch: 1475, Loss: 0.061929937452077866, Acc: 0.9513729810714722\n",
      "Epoch: 1476, Loss: 0.06959978491067886, Acc: 0.9481911659240723\n",
      "Epoch: 1477, Loss: 0.07597483694553375, Acc: 0.9455324411392212\n",
      "Epoch: 1478, Loss: 0.0598873607814312, Acc: 0.9423700571060181\n",
      "Epoch: 1479, Loss: 0.06157403439283371, Acc: 0.9528532028198242\n",
      "Epoch: 1480, Loss: 0.06789757311344147, Acc: 0.94179767370224\n",
      "Epoch: 1481, Loss: 0.057724691927433014, Acc: 0.9420239925384521\n",
      "Epoch: 1482, Loss: 0.07438880950212479, Acc: 0.9577988386154175\n",
      "Epoch: 1483, Loss: 0.06667717546224594, Acc: 0.9429407715797424\n",
      "Epoch: 1484, Loss: 0.06299252808094025, Acc: 0.9441012740135193\n",
      "Epoch: 1485, Loss: 0.06521224230527878, Acc: 0.9606121778488159\n",
      "Epoch: 1486, Loss: 0.09270136058330536, Acc: 0.9581458568572998\n",
      "Epoch: 1487, Loss: 0.05963090434670448, Acc: 0.9500884413719177\n",
      "Epoch: 1488, Loss: 0.057523079216480255, Acc: 0.9439572095870972\n",
      "Epoch: 1489, Loss: 0.06849417835474014, Acc: 0.9425718784332275\n",
      "Epoch: 1490, Loss: 0.05867660418152809, Acc: 0.9429536461830139\n",
      "Epoch: 1491, Loss: 0.059440795332193375, Acc: 0.946445882320404\n",
      "Epoch: 1492, Loss: 0.06330902129411697, Acc: 0.9410904049873352\n",
      "Epoch: 1493, Loss: 0.05686848610639572, Acc: 0.9415656924247742\n",
      "Epoch: 1494, Loss: 0.0591583214700222, Acc: 0.9479817152023315\n",
      "Epoch: 1495, Loss: 0.0750318393111229, Acc: 0.9494987726211548\n",
      "Epoch: 1496, Loss: 0.06157281622290611, Acc: 0.9444055557250977\n",
      "Epoch: 1497, Loss: 0.06587475538253784, Acc: 0.945105791091919\n",
      "Epoch: 1498, Loss: 0.06645509600639343, Acc: 0.9462578892707825\n",
      "Epoch: 1499, Loss: 0.06326032429933548, Acc: 0.9465723633766174\n",
      "Epoch: 1500, Loss: 0.08831460773944855, Acc: 0.9585984349250793\n",
      "Epoch: 1501, Loss: 0.08372846245765686, Acc: 0.9569403529167175\n",
      "Epoch: 1502, Loss: 0.07778201252222061, Acc: 0.9531996846199036\n",
      "Epoch: 1503, Loss: 0.0845971629023552, Acc: 0.955139696598053\n",
      "Epoch: 1504, Loss: 0.07050438970327377, Acc: 0.9462040662765503\n",
      "Epoch: 1505, Loss: 0.06343691796064377, Acc: 0.9455657005310059\n",
      "Epoch: 1506, Loss: 0.07453656941652298, Acc: 0.9592382311820984\n",
      "Epoch: 1507, Loss: 0.08190722018480301, Acc: 0.9473433494567871\n",
      "Epoch: 1508, Loss: 0.07334043085575104, Acc: 0.9520131945610046\n",
      "Epoch: 1509, Loss: 0.06925123929977417, Acc: 0.9477545619010925\n",
      "Epoch: 1510, Loss: 0.06358803063631058, Acc: 0.9391116499900818\n",
      "Epoch: 1511, Loss: 0.06593639403581619, Acc: 0.9507884383201599\n",
      "Epoch: 1512, Loss: 0.07635235786437988, Acc: 0.9538453817367554\n",
      "Epoch: 1513, Loss: 0.12469042092561722, Acc: 0.9677896499633789\n",
      "Epoch: 1514, Loss: 0.06713470816612244, Acc: 0.9441429972648621\n",
      "Epoch: 1515, Loss: 0.07624979317188263, Acc: 0.9472679495811462\n",
      "Epoch: 1516, Loss: 0.07559546828269958, Acc: 0.9495493769645691\n",
      "Epoch: 1517, Loss: 0.06463991850614548, Acc: 0.9413357973098755\n",
      "Epoch: 1518, Loss: 0.06606107205152512, Acc: 0.9492109417915344\n",
      "Epoch: 1519, Loss: 0.07023867964744568, Acc: 0.9476065039634705\n",
      "Epoch: 1520, Loss: 0.06163050979375839, Acc: 0.9460707306861877\n",
      "Epoch: 1521, Loss: 0.06748642027378082, Acc: 0.9526163339614868\n",
      "Epoch: 1522, Loss: 0.05957243964076042, Acc: 0.9437010288238525\n",
      "Epoch: 1523, Loss: 0.07023636251688004, Acc: 0.949448823928833\n",
      "Epoch: 1524, Loss: 0.07025004178285599, Acc: 0.949775218963623\n",
      "Epoch: 1525, Loss: 0.06196689233183861, Acc: 0.9438527226448059\n",
      "Epoch: 1526, Loss: 0.06886013597249985, Acc: 0.948158860206604\n",
      "Epoch: 1527, Loss: 0.06134754419326782, Acc: 0.9417697191238403\n",
      "Epoch: 1528, Loss: 0.06115376576781273, Acc: 0.9452025294303894\n",
      "Epoch: 1529, Loss: 0.06911753118038177, Acc: 0.945560097694397\n",
      "Epoch: 1530, Loss: 0.06984078884124756, Acc: 0.9487167596817017\n",
      "Epoch: 1531, Loss: 0.0694539025425911, Acc: 0.9456872344017029\n",
      "Epoch: 1532, Loss: 0.06528647989034653, Acc: 0.9453718066215515\n",
      "Epoch: 1533, Loss: 0.060392849147319794, Acc: 0.9446340203285217\n",
      "Epoch: 1534, Loss: 0.05769012123346329, Acc: 0.9427435994148254\n",
      "Epoch: 1535, Loss: 0.06748229265213013, Acc: 0.9458754658699036\n",
      "Epoch: 1536, Loss: 0.07622499763965607, Acc: 0.9520145654678345\n",
      "Epoch: 1537, Loss: 0.09374871104955673, Acc: 0.946977972984314\n",
      "Epoch: 1538, Loss: 0.06640589982271194, Acc: 0.9483194947242737\n",
      "Epoch: 1539, Loss: 0.06507463753223419, Acc: 0.950114369392395\n",
      "Epoch: 1540, Loss: 0.06131245195865631, Acc: 0.9463264346122742\n",
      "Epoch: 1541, Loss: 0.0731300488114357, Acc: 0.9505410194396973\n",
      "Epoch: 1542, Loss: 0.08095832169055939, Acc: 0.9564124345779419\n",
      "Epoch: 1543, Loss: 0.06995923072099686, Acc: 0.966735303401947\n",
      "Epoch: 1544, Loss: 0.1048412173986435, Acc: 0.9508857727050781\n",
      "Epoch: 1545, Loss: 0.09392472356557846, Acc: 0.9579973220825195\n",
      "Epoch: 1546, Loss: 0.0676337406039238, Acc: 0.9487930536270142\n",
      "Epoch: 1547, Loss: 0.06601311266422272, Acc: 0.9420492053031921\n",
      "Epoch: 1548, Loss: 0.0656859427690506, Acc: 0.9448069930076599\n",
      "Epoch: 1549, Loss: 0.07067748159170151, Acc: 0.9625768065452576\n",
      "Epoch: 1550, Loss: 0.08089195191860199, Acc: 0.9462952613830566\n",
      "Epoch: 1551, Loss: 0.05813531577587128, Acc: 0.9434053301811218\n",
      "Epoch: 1552, Loss: 0.0625971332192421, Acc: 0.9535016417503357\n",
      "Epoch: 1553, Loss: 0.08167356252670288, Acc: 0.9506812691688538\n",
      "Epoch: 1554, Loss: 0.06342080235481262, Acc: 0.9440137147903442\n",
      "Epoch: 1555, Loss: 0.08722878247499466, Acc: 0.955714225769043\n",
      "Epoch: 1556, Loss: 0.08047136664390564, Acc: 0.9479588270187378\n",
      "Epoch: 1557, Loss: 0.06559056043624878, Acc: 0.9504542946815491\n",
      "Epoch: 1558, Loss: 0.06263438612222672, Acc: 0.9448909163475037\n",
      "Epoch: 1559, Loss: 0.0647488683462143, Acc: 0.9515277147293091\n",
      "Epoch: 1560, Loss: 0.06609221547842026, Acc: 0.9439218044281006\n",
      "Epoch: 1561, Loss: 0.06210390850901604, Acc: 0.9453805685043335\n",
      "Epoch: 1562, Loss: 0.07053695619106293, Acc: 0.9477072358131409\n",
      "Epoch: 1563, Loss: 0.06770964711904526, Acc: 0.9497994780540466\n",
      "Epoch: 1564, Loss: 0.06617136299610138, Acc: 0.944517195224762\n",
      "Epoch: 1565, Loss: 0.06777076423168182, Acc: 0.941759467124939\n",
      "Epoch: 1566, Loss: 0.05710717663168907, Acc: 0.9457268714904785\n",
      "Epoch: 1567, Loss: 0.06691323220729828, Acc: 0.9458106756210327\n",
      "Epoch: 1568, Loss: 0.0742514356970787, Acc: 0.957769513130188\n",
      "Epoch: 1569, Loss: 0.06370335072278976, Acc: 0.9470577836036682\n",
      "Epoch: 1570, Loss: 0.06223636865615845, Acc: 0.9458261728286743\n",
      "Epoch: 1571, Loss: 0.08496858924627304, Acc: 0.9675658941268921\n",
      "Epoch: 1572, Loss: 0.06761738657951355, Acc: 0.9465470910072327\n",
      "Epoch: 1573, Loss: 0.09710012376308441, Acc: 0.9503242373466492\n",
      "Epoch: 1574, Loss: 0.0674668699502945, Acc: 0.9472558498382568\n",
      "Epoch: 1575, Loss: 0.0634414330124855, Acc: 0.9410822987556458\n",
      "Epoch: 1576, Loss: 0.06136860325932503, Acc: 0.9458443522453308\n",
      "Epoch: 1577, Loss: 0.06589354574680328, Acc: 0.9516388773918152\n",
      "Epoch: 1578, Loss: 0.059883058071136475, Acc: 0.9418588280677795\n",
      "Epoch: 1579, Loss: 0.05810089036822319, Acc: 0.9451900124549866\n",
      "Epoch: 1580, Loss: 0.07042956352233887, Acc: 0.9516586661338806\n",
      "Epoch: 1581, Loss: 0.06712518632411957, Acc: 0.9474228620529175\n",
      "Epoch: 1582, Loss: 0.07063175737857819, Acc: 0.9449865221977234\n",
      "Epoch: 1583, Loss: 0.0624183714389801, Acc: 0.9511641263961792\n",
      "Epoch: 1584, Loss: 0.06075044721364975, Acc: 0.9463496804237366\n",
      "Epoch: 1585, Loss: 0.058009982109069824, Acc: 0.9441395998001099\n",
      "Epoch: 1586, Loss: 0.06106474995613098, Acc: 0.9504890441894531\n",
      "Epoch: 1587, Loss: 0.057456210255622864, Acc: 0.943297266960144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1588, Loss: 0.06619365513324738, Acc: 0.9494591951370239\n",
      "Epoch: 1589, Loss: 0.08817914873361588, Acc: 0.9551339745521545\n",
      "Epoch: 1590, Loss: 0.058684635907411575, Acc: 0.9473870396614075\n",
      "Epoch: 1591, Loss: 0.06220397725701332, Acc: 0.9428742527961731\n",
      "Epoch: 1592, Loss: 0.06615854054689407, Acc: 0.9484911561012268\n",
      "Epoch: 1593, Loss: 0.05505341291427612, Acc: 0.9428314566612244\n",
      "Epoch: 1594, Loss: 0.08643686771392822, Acc: 0.9633787870407104\n",
      "Epoch: 1595, Loss: 0.0688677728176117, Acc: 0.9507005214691162\n",
      "Epoch: 1596, Loss: 0.0765203982591629, Acc: 0.9450393915176392\n",
      "Epoch: 1597, Loss: 0.067786306142807, Acc: 0.9469969868659973\n",
      "Epoch: 1598, Loss: 0.06894095242023468, Acc: 0.9550069570541382\n",
      "Epoch: 1599, Loss: 0.06651978194713593, Acc: 0.9453091025352478\n",
      "Epoch: 1600, Loss: 0.05772727355360985, Acc: 0.9431808590888977\n",
      "Epoch: 1601, Loss: 0.05474112182855606, Acc: 0.9482323527336121\n",
      "Epoch: 1602, Loss: 0.06355378776788712, Acc: 0.9459208250045776\n",
      "Epoch: 1603, Loss: 0.06105198338627815, Acc: 0.9448660612106323\n",
      "Epoch: 1604, Loss: 0.08050355315208435, Acc: 0.9540337920188904\n",
      "Epoch: 1605, Loss: 0.06596311926841736, Acc: 0.9459096193313599\n",
      "Epoch: 1606, Loss: 0.0649157389998436, Acc: 0.9534395933151245\n",
      "Epoch: 1607, Loss: 0.06975096464157104, Acc: 0.9406103491783142\n",
      "Epoch: 1608, Loss: 0.057991039007902145, Acc: 0.9443941712379456\n",
      "Epoch: 1609, Loss: 0.06385049968957901, Acc: 0.9454056024551392\n",
      "Epoch: 1610, Loss: 0.0706246867775917, Acc: 0.9512327909469604\n",
      "Epoch: 1611, Loss: 0.07015739381313324, Acc: 0.9539561867713928\n",
      "Epoch: 1612, Loss: 0.07505269348621368, Acc: 0.9453862905502319\n",
      "Epoch: 1613, Loss: 0.06624311953783035, Acc: 0.9554259181022644\n",
      "Epoch: 1614, Loss: 0.06520666182041168, Acc: 0.9431565403938293\n",
      "Epoch: 1615, Loss: 0.06713511049747467, Acc: 0.9488309621810913\n",
      "Epoch: 1616, Loss: 0.06126817688345909, Acc: 0.9428703188896179\n",
      "Epoch: 1617, Loss: 0.06548391282558441, Acc: 0.9436950087547302\n",
      "Epoch: 1618, Loss: 0.05670749396085739, Acc: 0.9462955594062805\n",
      "Epoch: 1619, Loss: 0.0659908875823021, Acc: 0.9520586729049683\n",
      "Epoch: 1620, Loss: 0.07337590306997299, Acc: 0.9447370767593384\n",
      "Epoch: 1621, Loss: 0.07296252995729446, Acc: 0.9425479769706726\n",
      "Epoch: 1622, Loss: 0.08503575623035431, Acc: 0.9677156805992126\n",
      "Epoch: 1623, Loss: 0.062144383788108826, Acc: 0.9487078785896301\n",
      "Epoch: 1624, Loss: 0.06355530768632889, Acc: 0.9460304975509644\n",
      "Epoch: 1625, Loss: 0.06254272907972336, Acc: 0.9452837109565735\n",
      "Epoch: 1626, Loss: 0.05819668248295784, Acc: 0.9434792995452881\n",
      "Epoch: 1627, Loss: 0.059715934097766876, Acc: 0.9463160634040833\n",
      "Epoch: 1628, Loss: 0.06255000829696655, Acc: 0.9469872117042542\n",
      "Epoch: 1629, Loss: 0.06645125150680542, Acc: 0.9534294605255127\n",
      "Epoch: 1630, Loss: 0.07893089950084686, Acc: 0.9531038403511047\n",
      "Epoch: 1631, Loss: 0.06465869396924973, Acc: 0.9532620906829834\n",
      "Epoch: 1632, Loss: 0.07001606374979019, Acc: 0.94596928358078\n",
      "Epoch: 1633, Loss: 0.06473124772310257, Acc: 0.9454650282859802\n",
      "Epoch: 1634, Loss: 0.06016627699136734, Acc: 0.9457124471664429\n",
      "Epoch: 1635, Loss: 0.06620290130376816, Acc: 0.9493715763092041\n",
      "Epoch: 1636, Loss: 0.058712586760520935, Acc: 0.9424722790718079\n",
      "Epoch: 1637, Loss: 0.06555572152137756, Acc: 0.9458709955215454\n",
      "Epoch: 1638, Loss: 0.06310606002807617, Acc: 0.9430161118507385\n",
      "Epoch: 1639, Loss: 0.058591704815626144, Acc: 0.9443218111991882\n",
      "Epoch: 1640, Loss: 0.06535615772008896, Acc: 0.9473079442977905\n",
      "Epoch: 1641, Loss: 0.06777577102184296, Acc: 0.9455013871192932\n",
      "Epoch: 1642, Loss: 0.0571209192276001, Acc: 0.9464014768600464\n",
      "Epoch: 1643, Loss: 0.0603741891682148, Acc: 0.9459428787231445\n",
      "Epoch: 1644, Loss: 0.05942068621516228, Acc: 0.945641279220581\n",
      "Epoch: 1645, Loss: 0.07708145678043365, Acc: 0.9595033526420593\n",
      "Epoch: 1646, Loss: 0.0740136057138443, Acc: 0.9492746591567993\n",
      "Epoch: 1647, Loss: 0.0733674168586731, Acc: 0.9502131342887878\n",
      "Epoch: 1648, Loss: 0.0610828623175621, Acc: 0.9469404816627502\n",
      "Epoch: 1649, Loss: 0.06282763183116913, Acc: 0.9472984075546265\n",
      "Epoch: 1650, Loss: 0.05755238980054855, Acc: 0.9460770487785339\n",
      "Epoch: 1651, Loss: 0.05872933566570282, Acc: 0.9466045498847961\n",
      "Epoch: 1652, Loss: 0.056270308792591095, Acc: 0.9450521469116211\n",
      "Epoch: 1653, Loss: 0.06758825480937958, Acc: 0.9516568779945374\n",
      "Epoch: 1654, Loss: 0.0674588531255722, Acc: 0.9426003098487854\n",
      "Epoch: 1655, Loss: 0.059635866433382034, Acc: 0.943422794342041\n",
      "Epoch: 1656, Loss: 0.07187777757644653, Acc: 0.9554314613342285\n",
      "Epoch: 1657, Loss: 0.09655921161174774, Acc: 0.9616586565971375\n",
      "Epoch: 1658, Loss: 0.07451680302619934, Acc: 0.9476020336151123\n",
      "Epoch: 1659, Loss: 0.05467619374394417, Acc: 0.9462135434150696\n",
      "Epoch: 1660, Loss: 0.06546390056610107, Acc: 0.9459287524223328\n",
      "Epoch: 1661, Loss: 0.08865506201982498, Acc: 0.9463210701942444\n",
      "Epoch: 1662, Loss: 0.05782470107078552, Acc: 0.9442344307899475\n",
      "Epoch: 1663, Loss: 0.07889559864997864, Acc: 0.9556376338005066\n",
      "Epoch: 1664, Loss: 0.06428496539592743, Acc: 0.9485486745834351\n",
      "Epoch: 1665, Loss: 0.05739913135766983, Acc: 0.9432058930397034\n",
      "Epoch: 1666, Loss: 0.07902397215366364, Acc: 0.9589990973472595\n",
      "Epoch: 1667, Loss: 0.06459852308034897, Acc: 0.9469066858291626\n",
      "Epoch: 1668, Loss: 0.06368838995695114, Acc: 0.9511002898216248\n",
      "Epoch: 1669, Loss: 0.06528228521347046, Acc: 0.950149655342102\n",
      "Epoch: 1670, Loss: 0.06142989173531532, Acc: 0.9484595060348511\n",
      "Epoch: 1671, Loss: 0.06219281256198883, Acc: 0.944317638874054\n",
      "Epoch: 1672, Loss: 0.0765286237001419, Acc: 0.9498925805091858\n",
      "Epoch: 1673, Loss: 0.07008988410234451, Acc: 0.9512290358543396\n",
      "Epoch: 1674, Loss: 0.08822270482778549, Acc: 0.9571531414985657\n",
      "Epoch: 1675, Loss: 0.0649891197681427, Acc: 0.9482700228691101\n",
      "Epoch: 1676, Loss: 0.06236867606639862, Acc: 0.9603510499000549\n",
      "Epoch: 1677, Loss: 0.056802116334438324, Acc: 0.945609986782074\n",
      "Epoch: 1678, Loss: 0.06663190573453903, Acc: 0.9511300325393677\n",
      "Epoch: 1679, Loss: 0.07412707060575485, Acc: 0.9435566067695618\n",
      "Epoch: 1680, Loss: 0.05727270618081093, Acc: 0.94349205493927\n",
      "Epoch: 1681, Loss: 0.06316134333610535, Acc: 0.9488712549209595\n",
      "Epoch: 1682, Loss: 0.06316939741373062, Acc: 0.9457946419715881\n",
      "Epoch: 1683, Loss: 0.06299583613872528, Acc: 0.9483269453048706\n",
      "Epoch: 1684, Loss: 0.07218031585216522, Acc: 0.9497493505477905\n",
      "Epoch: 1685, Loss: 0.0731225460767746, Acc: 0.9486961364746094\n",
      "Epoch: 1686, Loss: 0.06841882318258286, Acc: 0.9507748484611511\n",
      "Epoch: 1687, Loss: 0.06395942717790604, Acc: 0.952106773853302\n",
      "Epoch: 1688, Loss: 0.055013373494148254, Acc: 0.949794352054596\n",
      "Epoch: 1689, Loss: 0.06263558566570282, Acc: 0.9449636936187744\n",
      "Epoch: 1690, Loss: 0.07131440937519073, Acc: 0.9500476121902466\n",
      "Epoch: 1691, Loss: 0.060415513813495636, Acc: 0.9446092247962952\n",
      "Epoch: 1692, Loss: 0.06652763485908508, Acc: 0.9511559009552002\n",
      "Epoch: 1693, Loss: 0.056936122477054596, Acc: 0.9487294554710388\n",
      "Epoch: 1694, Loss: 0.06172764673829079, Acc: 0.9483832716941833\n",
      "Epoch: 1695, Loss: 0.06640233099460602, Acc: 0.9474290013313293\n",
      "Epoch: 1696, Loss: 0.06830615550279617, Acc: 0.952285647392273\n",
      "Epoch: 1697, Loss: 0.07113286107778549, Acc: 0.9486649036407471\n",
      "Epoch: 1698, Loss: 0.06941059976816177, Acc: 0.9529065489768982\n",
      "Epoch: 1699, Loss: 0.06992103904485703, Acc: 0.9551771283149719\n",
      "Epoch: 1700, Loss: 0.09549853950738907, Acc: 0.9557984471321106\n",
      "Epoch: 1701, Loss: 0.06588350981473923, Acc: 0.9483156204223633\n",
      "Epoch: 1702, Loss: 0.06382260471582413, Acc: 0.9482429623603821\n",
      "Epoch: 1703, Loss: 0.06336937844753265, Acc: 0.9492002725601196\n",
      "Epoch: 1704, Loss: 0.06308732181787491, Acc: 0.9455639123916626\n",
      "Epoch: 1705, Loss: 0.06089141592383385, Acc: 0.9461078643798828\n",
      "Epoch: 1706, Loss: 0.06870289146900177, Acc: 0.9532272815704346\n",
      "Epoch: 1707, Loss: 0.06580705940723419, Acc: 0.9507588744163513\n",
      "Epoch: 1708, Loss: 0.0732971653342247, Acc: 0.950636625289917\n",
      "Epoch: 1709, Loss: 0.07080169767141342, Acc: 0.9462680220603943\n",
      "Epoch: 1710, Loss: 0.06390491127967834, Acc: 0.944181501865387\n",
      "Epoch: 1711, Loss: 0.06138688698410988, Acc: 0.9497358798980713\n",
      "Epoch: 1712, Loss: 0.06371629983186722, Acc: 0.9483098387718201\n",
      "Epoch: 1713, Loss: 0.06623448431491852, Acc: 0.9449928402900696\n",
      "Epoch: 1714, Loss: 0.05873296409845352, Acc: 0.9453368186950684\n",
      "Epoch: 1715, Loss: 0.06315559893846512, Acc: 0.9447482824325562\n",
      "Epoch: 1716, Loss: 0.08020880818367004, Acc: 0.9569871425628662\n",
      "Epoch: 1717, Loss: 0.06533849984407425, Acc: 0.9477547407150269\n",
      "Epoch: 1718, Loss: 0.0692095085978508, Acc: 0.9513166546821594\n",
      "Epoch: 1719, Loss: 0.05555575713515282, Acc: 0.9458814263343811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1720, Loss: 0.06441116333007812, Acc: 0.9468323588371277\n",
      "Epoch: 1721, Loss: 0.05932977795600891, Acc: 0.9459898471832275\n",
      "Epoch: 1722, Loss: 0.05905396118760109, Acc: 0.946528434753418\n",
      "Epoch: 1723, Loss: 0.06266344338655472, Acc: 0.9548414945602417\n",
      "Epoch: 1724, Loss: 0.07458754628896713, Acc: 0.9468401074409485\n",
      "Epoch: 1725, Loss: 0.057225555181503296, Acc: 0.9438809156417847\n",
      "Epoch: 1726, Loss: 0.061381611973047256, Acc: 0.9467592239379883\n",
      "Epoch: 1727, Loss: 0.05992776155471802, Acc: 0.9535143375396729\n",
      "Epoch: 1728, Loss: 0.06787468492984772, Acc: 0.9470467567443848\n",
      "Epoch: 1729, Loss: 0.06670792400836945, Acc: 0.9460217356681824\n",
      "Epoch: 1730, Loss: 0.05934709683060646, Acc: 0.9452874064445496\n",
      "Epoch: 1731, Loss: 0.059167906641960144, Acc: 0.9471917748451233\n",
      "Epoch: 1732, Loss: 0.08178209513425827, Acc: 0.9631893634796143\n",
      "Epoch: 1733, Loss: 0.061553165316581726, Acc: 0.9457614421844482\n",
      "Epoch: 1734, Loss: 0.0682067945599556, Acc: 0.9521753191947937\n",
      "Epoch: 1735, Loss: 0.06253320723772049, Acc: 0.9509918689727783\n",
      "Epoch: 1736, Loss: 0.07066087424755096, Acc: 0.95120769739151\n",
      "Epoch: 1737, Loss: 0.06812741607427597, Acc: 0.9499528408050537\n",
      "Epoch: 1738, Loss: 0.06468681991100311, Acc: 0.9485417604446411\n",
      "Epoch: 1739, Loss: 0.0682695060968399, Acc: 0.9495246410369873\n",
      "Epoch: 1740, Loss: 0.0582183413207531, Acc: 0.9450129270553589\n",
      "Epoch: 1741, Loss: 0.06692393124103546, Acc: 0.9457843899726868\n",
      "Epoch: 1742, Loss: 0.07681313157081604, Acc: 0.9463586807250977\n",
      "Epoch: 1743, Loss: 0.07274135947227478, Acc: 0.9484946727752686\n",
      "Epoch: 1744, Loss: 0.05741285905241966, Acc: 0.9466012120246887\n",
      "Epoch: 1745, Loss: 0.06429416686296463, Acc: 0.9533166289329529\n",
      "Epoch: 1746, Loss: 0.06517070531845093, Acc: 0.9499881863594055\n",
      "Epoch: 1747, Loss: 0.07888583838939667, Acc: 0.9614784121513367\n",
      "Epoch: 1748, Loss: 0.07960595190525055, Acc: 0.9579788446426392\n",
      "Epoch: 1749, Loss: 0.06397905200719833, Acc: 0.9514837265014648\n",
      "Epoch: 1750, Loss: 0.05958705022931099, Acc: 0.9457826018333435\n",
      "Epoch: 1751, Loss: 0.06384004652500153, Acc: 0.9460300207138062\n",
      "Epoch: 1752, Loss: 0.05965675041079521, Acc: 0.950525164604187\n",
      "Epoch: 1753, Loss: 0.06682318449020386, Acc: 0.9547204375267029\n",
      "Epoch: 1754, Loss: 0.05691126734018326, Acc: 0.9434438943862915\n",
      "Epoch: 1755, Loss: 0.06281916797161102, Acc: 0.949698269367218\n",
      "Epoch: 1756, Loss: 0.07080374658107758, Acc: 0.954268217086792\n",
      "Epoch: 1757, Loss: 0.06582003086805344, Acc: 0.9465273022651672\n",
      "Epoch: 1758, Loss: 0.07784682512283325, Acc: 0.9582873582839966\n",
      "Epoch: 1759, Loss: 0.059334270656108856, Acc: 0.9444767832756042\n",
      "Epoch: 1760, Loss: 0.0598069466650486, Acc: 0.9435884356498718\n",
      "Epoch: 1761, Loss: 0.06629233062267303, Acc: 0.9505388140678406\n",
      "Epoch: 1762, Loss: 0.06954375654459, Acc: 0.9507796764373779\n",
      "Epoch: 1763, Loss: 0.07273461669683456, Acc: 0.9511520266532898\n",
      "Epoch: 1764, Loss: 0.06829610466957092, Acc: 0.953955352306366\n",
      "Epoch: 1765, Loss: 0.07628024369478226, Acc: 0.9428269863128662\n",
      "Epoch: 1766, Loss: 0.060208965092897415, Acc: 0.9524169564247131\n",
      "Epoch: 1767, Loss: 0.06819470226764679, Acc: 0.9575408697128296\n",
      "Epoch: 1768, Loss: 0.06747855246067047, Acc: 0.9518968462944031\n",
      "Epoch: 1769, Loss: 0.05746516212821007, Acc: 0.9474086165428162\n",
      "Epoch: 1770, Loss: 0.08547545969486237, Acc: 0.9564874768257141\n",
      "Epoch: 1771, Loss: 0.08052577078342438, Acc: 0.9573559165000916\n",
      "Epoch: 1772, Loss: 0.056252479553222656, Acc: 0.9512702226638794\n",
      "Epoch: 1773, Loss: 0.062800332903862, Acc: 0.9447242617607117\n",
      "Epoch: 1774, Loss: 0.054288946092128754, Acc: 0.9433732628822327\n",
      "Epoch: 1775, Loss: 0.06284020096063614, Acc: 0.9457218050956726\n",
      "Epoch: 1776, Loss: 0.0712243914604187, Acc: 0.952172040939331\n",
      "Epoch: 1777, Loss: 0.060981813818216324, Acc: 0.9486151933670044\n",
      "Epoch: 1778, Loss: 0.06655538082122803, Acc: 0.9494255781173706\n",
      "Epoch: 1779, Loss: 0.05904658883810043, Acc: 0.9472770094871521\n",
      "Epoch: 1780, Loss: 0.06525963544845581, Acc: 0.9446229338645935\n",
      "Epoch: 1781, Loss: 0.05530934780836105, Acc: 0.9465770721435547\n",
      "Epoch: 1782, Loss: 0.0600387379527092, Acc: 0.9615345001220703\n",
      "Epoch: 1783, Loss: 0.059407975524663925, Acc: 0.9452073574066162\n",
      "Epoch: 1784, Loss: 0.08213826268911362, Acc: 0.9585867524147034\n",
      "Epoch: 1785, Loss: 0.08426551520824432, Acc: 0.953584611415863\n",
      "Epoch: 1786, Loss: 0.05571012198925018, Acc: 0.9441344738006592\n",
      "Epoch: 1787, Loss: 0.0650034099817276, Acc: 0.945188581943512\n",
      "Epoch: 1788, Loss: 0.05952909588813782, Acc: 0.9553008675575256\n",
      "Epoch: 1789, Loss: 0.07497529685497284, Acc: 0.9497920870780945\n",
      "Epoch: 1790, Loss: 0.06088104471564293, Acc: 0.942615807056427\n",
      "Epoch: 1791, Loss: 0.05810021236538887, Acc: 0.9499787092208862\n",
      "Epoch: 1792, Loss: 0.0713193416595459, Acc: 0.9416950345039368\n",
      "Epoch: 1793, Loss: 0.07673843204975128, Acc: 0.9504778385162354\n",
      "Epoch: 1794, Loss: 0.07128742337226868, Acc: 0.9525894522666931\n",
      "Epoch: 1795, Loss: 0.060613762587308884, Acc: 0.9455768465995789\n",
      "Epoch: 1796, Loss: 0.05651072785258293, Acc: 0.9451496601104736\n",
      "Epoch: 1797, Loss: 0.06797514855861664, Acc: 0.9523053169250488\n",
      "Epoch: 1798, Loss: 0.07111674547195435, Acc: 0.9508949518203735\n",
      "Epoch: 1799, Loss: 0.0574437640607357, Acc: 0.9455976486206055\n",
      "Epoch: 1800, Loss: 0.06890667229890823, Acc: 0.9508361220359802\n",
      "Epoch: 1801, Loss: 0.054559919983148575, Acc: 0.9444689154624939\n",
      "Epoch: 1802, Loss: 0.05618375167250633, Acc: 0.9470441937446594\n",
      "Epoch: 1803, Loss: 0.060083143413066864, Acc: 0.9468547105789185\n",
      "Epoch: 1804, Loss: 0.06557545065879822, Acc: 0.9489530324935913\n",
      "Epoch: 1805, Loss: 0.06685075908899307, Acc: 0.9511433243751526\n",
      "Epoch: 1806, Loss: 0.06297417730093002, Acc: 0.9513806104660034\n",
      "Epoch: 1807, Loss: 0.05560098588466644, Acc: 0.9520019888877869\n",
      "Epoch: 1808, Loss: 0.06443414837121964, Acc: 0.9570315480232239\n",
      "Epoch: 1809, Loss: 0.10235823690891266, Acc: 0.9470735192298889\n",
      "Epoch: 1810, Loss: 0.08126625418663025, Acc: 0.9489014148712158\n",
      "Epoch: 1811, Loss: 0.06330960243940353, Acc: 0.9481954574584961\n",
      "Epoch: 1812, Loss: 0.0687641054391861, Acc: 0.9486742615699768\n",
      "Epoch: 1813, Loss: 0.06728486716747284, Acc: 0.9510756731033325\n",
      "Epoch: 1814, Loss: 0.06690475344657898, Acc: 0.9560758471488953\n",
      "Epoch: 1815, Loss: 0.057939235121011734, Acc: 0.9448169469833374\n",
      "Epoch: 1816, Loss: 0.05939517170190811, Acc: 0.9478774666786194\n",
      "Epoch: 1817, Loss: 0.054792970418930054, Acc: 0.9453141689300537\n",
      "Epoch: 1818, Loss: 0.07874627411365509, Acc: 0.9588207006454468\n",
      "Epoch: 1819, Loss: 0.06565927714109421, Acc: 0.9496843814849854\n",
      "Epoch: 1820, Loss: 0.06273409724235535, Acc: 0.9579517245292664\n",
      "Epoch: 1821, Loss: 0.07225674390792847, Acc: 0.9476245045661926\n",
      "Epoch: 1822, Loss: 0.062085963785648346, Acc: 0.9564096331596375\n",
      "Epoch: 1823, Loss: 0.0814918726682663, Acc: 0.9512417912483215\n",
      "Epoch: 1824, Loss: 0.06529255956411362, Acc: 0.9535778164863586\n",
      "Epoch: 1825, Loss: 0.05878446251153946, Acc: 0.9472755789756775\n",
      "Epoch: 1826, Loss: 0.05725117027759552, Acc: 0.9493621587753296\n",
      "Epoch: 1827, Loss: 0.0725422203540802, Acc: 0.9470838308334351\n",
      "Epoch: 1828, Loss: 0.07652434706687927, Acc: 0.9423195719718933\n",
      "Epoch: 1829, Loss: 0.06360198557376862, Acc: 0.9624261260032654\n",
      "Epoch: 1830, Loss: 0.06868743896484375, Acc: 0.9528613090515137\n",
      "Epoch: 1831, Loss: 0.07088540494441986, Acc: 0.9576674103736877\n",
      "Epoch: 1832, Loss: 0.07427588850259781, Acc: 0.9566398859024048\n",
      "Epoch: 1833, Loss: 0.06881964206695557, Acc: 0.9528573155403137\n",
      "Epoch: 1834, Loss: 0.07579246163368225, Acc: 0.954753577709198\n",
      "Epoch: 1835, Loss: 0.06346619874238968, Acc: 0.948766827583313\n",
      "Epoch: 1836, Loss: 0.07186390459537506, Acc: 0.9486849308013916\n",
      "Epoch: 1837, Loss: 0.058054521679878235, Acc: 0.950424313545227\n",
      "Epoch: 1838, Loss: 0.07247107475996017, Acc: 0.9586291313171387\n",
      "Epoch: 1839, Loss: 0.06357908993959427, Acc: 0.9464315176010132\n",
      "Epoch: 1840, Loss: 0.06517571955919266, Acc: 0.9573950171470642\n",
      "Epoch: 1841, Loss: 0.06339379400014877, Acc: 0.9514334797859192\n",
      "Epoch: 1842, Loss: 0.0685686394572258, Acc: 0.9516013264656067\n",
      "Epoch: 1843, Loss: 0.057937655597925186, Acc: 0.9474438428878784\n",
      "Epoch: 1844, Loss: 0.06406206637620926, Acc: 0.9530988931655884\n",
      "Epoch: 1845, Loss: 0.07695259898900986, Acc: 0.9522737860679626\n",
      "Epoch: 1846, Loss: 0.055780358612537384, Acc: 0.9475446343421936\n",
      "Epoch: 1847, Loss: 0.06154843419790268, Acc: 0.9603061079978943\n",
      "Epoch: 1848, Loss: 0.06571285426616669, Acc: 0.9475204944610596\n",
      "Epoch: 1849, Loss: 0.05532679334282875, Acc: 0.9470844268798828\n",
      "Epoch: 1850, Loss: 0.055651888251304626, Acc: 0.9450818300247192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1851, Loss: 0.060998450964689255, Acc: 0.9523131847381592\n",
      "Epoch: 1852, Loss: 0.0769614428281784, Acc: 0.9592209458351135\n",
      "Epoch: 1853, Loss: 0.0903887152671814, Acc: 0.94721519947052\n",
      "Epoch: 1854, Loss: 0.06362149864435196, Acc: 0.9456202983856201\n",
      "Epoch: 1855, Loss: 0.06258295476436615, Acc: 0.9532890915870667\n",
      "Epoch: 1856, Loss: 0.05847172066569328, Acc: 0.9473423957824707\n",
      "Epoch: 1857, Loss: 0.07452072948217392, Acc: 0.9559644460678101\n",
      "Epoch: 1858, Loss: 0.06501428037881851, Acc: 0.9495019912719727\n",
      "Epoch: 1859, Loss: 0.06681042164564133, Acc: 0.9521382451057434\n",
      "Epoch: 1860, Loss: 0.05636792629957199, Acc: 0.9465818405151367\n",
      "Epoch: 1861, Loss: 0.06608331948518753, Acc: 0.9489412903785706\n",
      "Epoch: 1862, Loss: 0.06542282551527023, Acc: 0.9505305290222168\n",
      "Epoch: 1863, Loss: 0.06132865324616432, Acc: 0.9512920379638672\n",
      "Epoch: 1864, Loss: 0.06008681654930115, Acc: 0.9484346508979797\n",
      "Epoch: 1865, Loss: 0.08531717956066132, Acc: 0.9663572907447815\n",
      "Epoch: 1866, Loss: 0.06704054027795792, Acc: 0.9521251916885376\n",
      "Epoch: 1867, Loss: 0.060441091656684875, Acc: 0.944082498550415\n",
      "Epoch: 1868, Loss: 0.05650412291288376, Acc: 0.9512145519256592\n",
      "Epoch: 1869, Loss: 0.07397284358739853, Acc: 0.9488753080368042\n",
      "Epoch: 1870, Loss: 0.05807717517018318, Acc: 0.9468026161193848\n",
      "Epoch: 1871, Loss: 0.06575454771518707, Acc: 0.9519755244255066\n",
      "Epoch: 1872, Loss: 0.07043896615505219, Acc: 0.9506251811981201\n",
      "Epoch: 1873, Loss: 0.06678925454616547, Acc: 0.9506646394729614\n",
      "Epoch: 1874, Loss: 0.08417262881994247, Acc: 0.9545639753341675\n",
      "Epoch: 1875, Loss: 0.06504054367542267, Acc: 0.9509119391441345\n",
      "Epoch: 1876, Loss: 0.07725583016872406, Acc: 0.9662405252456665\n",
      "Epoch: 1877, Loss: 0.06639134138822556, Acc: 0.9477803111076355\n",
      "Epoch: 1878, Loss: 0.06497818976640701, Acc: 0.9543881416320801\n",
      "Epoch: 1879, Loss: 0.06761186569929123, Acc: 0.9507123231887817\n",
      "Epoch: 1880, Loss: 0.06667510420084, Acc: 0.9450070261955261\n",
      "Epoch: 1881, Loss: 0.05750623717904091, Acc: 0.9516108632087708\n",
      "Epoch: 1882, Loss: 0.0694870799779892, Acc: 0.9459498524665833\n",
      "Epoch: 1883, Loss: 0.06280427426099777, Acc: 0.9499682188034058\n",
      "Epoch: 1884, Loss: 0.07005786895751953, Acc: 0.9484269022941589\n",
      "Epoch: 1885, Loss: 0.059085771441459656, Acc: 0.949436366558075\n",
      "Epoch: 1886, Loss: 0.05961082875728607, Acc: 0.9453283548355103\n",
      "Epoch: 1887, Loss: 0.05391969531774521, Acc: 0.9443793892860413\n",
      "Epoch: 1888, Loss: 0.05642421916127205, Acc: 0.9471365213394165\n",
      "Epoch: 1889, Loss: 0.0629156082868576, Acc: 0.9441425204277039\n",
      "Epoch: 1890, Loss: 0.06091157719492912, Acc: 0.9474982023239136\n",
      "Epoch: 1891, Loss: 0.05928576737642288, Acc: 0.9468960165977478\n",
      "Epoch: 1892, Loss: 0.05525394156575203, Acc: 0.9451842904090881\n",
      "Epoch: 1893, Loss: 0.06964412331581116, Acc: 0.9535553455352783\n",
      "Epoch: 1894, Loss: 0.08291801065206528, Acc: 0.9477695226669312\n",
      "Epoch: 1895, Loss: 0.09254512190818787, Acc: 0.9478943347930908\n",
      "Epoch: 1896, Loss: 0.05689987167716026, Acc: 0.9481186866760254\n",
      "Epoch: 1897, Loss: 0.06890615820884705, Acc: 0.9529725313186646\n",
      "Epoch: 1898, Loss: 0.0801815539598465, Acc: 0.9548925161361694\n",
      "Epoch: 1899, Loss: 0.07977444678544998, Acc: 0.9518671631813049\n",
      "Epoch: 1900, Loss: 0.07862082123756409, Acc: 0.951968252658844\n",
      "Epoch: 1901, Loss: 0.056578606367111206, Acc: 0.9468326568603516\n",
      "Epoch: 1902, Loss: 0.05929749831557274, Acc: 0.9514652490615845\n",
      "Epoch: 1903, Loss: 0.058229994028806686, Acc: 0.9450939893722534\n",
      "Epoch: 1904, Loss: 0.06127918139100075, Acc: 0.953005850315094\n",
      "Epoch: 1905, Loss: 0.06973626464605331, Acc: 0.9502807259559631\n",
      "Epoch: 1906, Loss: 0.058992087841033936, Acc: 0.9498525857925415\n",
      "Epoch: 1907, Loss: 0.061105020344257355, Acc: 0.9473183155059814\n",
      "Epoch: 1908, Loss: 0.05998166278004646, Acc: 0.9487302303314209\n",
      "Epoch: 1909, Loss: 0.05741390958428383, Acc: 0.9493773579597473\n",
      "Epoch: 1910, Loss: 0.054420627653598785, Acc: 0.9467102885246277\n",
      "Epoch: 1911, Loss: 0.06457559764385223, Acc: 0.9445412755012512\n",
      "Epoch: 1912, Loss: 0.06639929860830307, Acc: 0.9561225175857544\n",
      "Epoch: 1913, Loss: 0.06358997523784637, Acc: 0.9510154128074646\n",
      "Epoch: 1914, Loss: 0.06485376507043839, Acc: 0.9652524590492249\n",
      "Epoch: 1915, Loss: 0.0784928947687149, Acc: 0.9506540894508362\n",
      "Epoch: 1916, Loss: 0.07086145132780075, Acc: 0.9564867615699768\n",
      "Epoch: 1917, Loss: 0.06529773771762848, Acc: 0.9516420364379883\n",
      "Epoch: 1918, Loss: 0.05480380356311798, Acc: 0.9439521431922913\n",
      "Epoch: 1919, Loss: 0.06151976063847542, Acc: 0.9565416574478149\n",
      "Epoch: 1920, Loss: 0.05700000002980232, Acc: 0.9458890557289124\n",
      "Epoch: 1921, Loss: 0.0717051550745964, Acc: 0.9513055682182312\n",
      "Epoch: 1922, Loss: 0.06340077519416809, Acc: 0.9431443810462952\n",
      "Epoch: 1923, Loss: 0.05863834172487259, Acc: 0.9463497400283813\n",
      "Epoch: 1924, Loss: 0.06444553285837173, Acc: 0.9455158710479736\n",
      "Epoch: 1925, Loss: 0.06305646896362305, Acc: 0.9576560854911804\n",
      "Epoch: 1926, Loss: 0.06842180341482162, Acc: 0.958986759185791\n",
      "Epoch: 1927, Loss: 0.059227004647254944, Acc: 0.9461665749549866\n",
      "Epoch: 1928, Loss: 0.05903717130422592, Acc: 0.9469290375709534\n",
      "Epoch: 1929, Loss: 0.05767199024558067, Acc: 0.9536452889442444\n",
      "Epoch: 1930, Loss: 0.07569067180156708, Acc: 0.9494403004646301\n",
      "Epoch: 1931, Loss: 0.08386548608541489, Acc: 0.9519567489624023\n",
      "Epoch: 1932, Loss: 0.06460903584957123, Acc: 0.9670028686523438\n",
      "Epoch: 1933, Loss: 0.0622091181576252, Acc: 0.9460488557815552\n",
      "Epoch: 1934, Loss: 0.05479946732521057, Acc: 0.9454808831214905\n",
      "Epoch: 1935, Loss: 0.06310055404901505, Acc: 0.9546836018562317\n",
      "Epoch: 1936, Loss: 0.05924320966005325, Acc: 0.9484318494796753\n",
      "Epoch: 1937, Loss: 0.07167608290910721, Acc: 0.9556534290313721\n",
      "Epoch: 1938, Loss: 0.06231208145618439, Acc: 0.9515013694763184\n",
      "Epoch: 1939, Loss: 0.06669976562261581, Acc: 0.9526313543319702\n",
      "Epoch: 1940, Loss: 0.06213757023215294, Acc: 0.9467180967330933\n",
      "Epoch: 1941, Loss: 0.05818983167409897, Acc: 0.9476435780525208\n",
      "Epoch: 1942, Loss: 0.05738317593932152, Acc: 0.9458762407302856\n",
      "Epoch: 1943, Loss: 0.06675656884908676, Acc: 0.9551761746406555\n",
      "Epoch: 1944, Loss: 0.05741886794567108, Acc: 0.945209264755249\n",
      "Epoch: 1945, Loss: 0.06015212833881378, Acc: 0.9498041272163391\n",
      "Epoch: 1946, Loss: 0.07640637457370758, Acc: 0.9632782340049744\n",
      "Epoch: 1947, Loss: 0.058490972965955734, Acc: 0.9468048810958862\n",
      "Epoch: 1948, Loss: 0.05938975140452385, Acc: 0.9499596953392029\n",
      "Epoch: 1949, Loss: 0.0754391998052597, Acc: 0.9574371576309204\n",
      "Epoch: 1950, Loss: 0.06038971245288849, Acc: 0.9526289105415344\n",
      "Epoch: 1951, Loss: 0.07259579747915268, Acc: 0.9501208662986755\n",
      "Epoch: 1952, Loss: 0.05778023973107338, Acc: 0.9477463364601135\n",
      "Epoch: 1953, Loss: 0.07226274907588959, Acc: 0.9483914375305176\n",
      "Epoch: 1954, Loss: 0.06568200886249542, Acc: 0.948938250541687\n",
      "Epoch: 1955, Loss: 0.0783548504114151, Acc: 0.9520127177238464\n",
      "Epoch: 1956, Loss: 0.05727788433432579, Acc: 0.9475682377815247\n",
      "Epoch: 1957, Loss: 0.08633384853601456, Acc: 0.9585637450218201\n",
      "Epoch: 1958, Loss: 0.0650675892829895, Acc: 0.9523188471794128\n",
      "Epoch: 1959, Loss: 0.05508770793676376, Acc: 0.9535950422286987\n",
      "Epoch: 1960, Loss: 0.05699038505554199, Acc: 0.9462428689002991\n",
      "Epoch: 1961, Loss: 0.06596609205007553, Acc: 0.953572154045105\n",
      "Epoch: 1962, Loss: 0.06011001020669937, Acc: 0.9501556754112244\n",
      "Epoch: 1963, Loss: 0.07490118592977524, Acc: 0.9556782245635986\n",
      "Epoch: 1964, Loss: 0.05855116993188858, Acc: 0.952186644077301\n",
      "Epoch: 1965, Loss: 0.05563659220933914, Acc: 0.9475928544998169\n",
      "Epoch: 1966, Loss: 0.06490539759397507, Acc: 0.9629190564155579\n",
      "Epoch: 1967, Loss: 0.07406464219093323, Acc: 0.9521676898002625\n",
      "Epoch: 1968, Loss: 0.05378347262740135, Acc: 0.9609404802322388\n",
      "Epoch: 1969, Loss: 0.07471971213817596, Acc: 0.9525939226150513\n",
      "Epoch: 1970, Loss: 0.06584639102220535, Acc: 0.950835108757019\n",
      "Epoch: 1971, Loss: 0.06938538700342178, Acc: 0.9495054483413696\n",
      "Epoch: 1972, Loss: 0.053138334304094315, Acc: 0.9542121887207031\n",
      "Epoch: 1973, Loss: 0.06201356649398804, Acc: 0.9464284181594849\n",
      "Epoch: 1974, Loss: 0.05420812964439392, Acc: 0.9469832181930542\n",
      "Epoch: 1975, Loss: 0.06157315522432327, Acc: 0.9487149119377136\n",
      "Epoch: 1976, Loss: 0.06584982573986053, Acc: 0.9494257569313049\n",
      "Epoch: 1977, Loss: 0.06072087958455086, Acc: 0.9537217617034912\n",
      "Epoch: 1978, Loss: 0.06096011772751808, Acc: 0.9520882964134216\n",
      "Epoch: 1979, Loss: 0.06761135905981064, Acc: 0.9505996108055115\n",
      "Epoch: 1980, Loss: 0.06237468123435974, Acc: 0.9515875577926636\n",
      "Epoch: 1981, Loss: 0.06385990977287292, Acc: 0.9597641229629517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1982, Loss: 0.074106365442276, Acc: 0.9519757628440857\n",
      "Epoch: 1983, Loss: 0.0607646219432354, Acc: 0.9520595669746399\n",
      "Epoch: 1984, Loss: 0.06779377162456512, Acc: 0.9531943202018738\n",
      "Epoch: 1985, Loss: 0.06250821799039841, Acc: 0.9551153779029846\n",
      "Epoch: 1986, Loss: 0.08405079692602158, Acc: 0.96986323595047\n",
      "Epoch: 1987, Loss: 0.07077643275260925, Acc: 0.9504133462905884\n",
      "Epoch: 1988, Loss: 0.05296489968895912, Acc: 0.9502553939819336\n",
      "Epoch: 1989, Loss: 0.07489162683486938, Acc: 0.9531658887863159\n",
      "Epoch: 1990, Loss: 0.06905008852481842, Acc: 0.9563327431678772\n",
      "Epoch: 1991, Loss: 0.07327908277511597, Acc: 0.9594866633415222\n",
      "Epoch: 1992, Loss: 0.062250152230262756, Acc: 0.9473877549171448\n",
      "Epoch: 1993, Loss: 0.06102791428565979, Acc: 0.9525148868560791\n",
      "Epoch: 1994, Loss: 0.07068145275115967, Acc: 0.9487137794494629\n",
      "Epoch: 1995, Loss: 0.05620795115828514, Acc: 0.9451818466186523\n",
      "Epoch: 1996, Loss: 0.06664737313985825, Acc: 0.9573493003845215\n",
      "Epoch: 1997, Loss: 0.08254057168960571, Acc: 0.9503793716430664\n",
      "Epoch: 1998, Loss: 0.0761820375919342, Acc: 0.9485480189323425\n",
      "Epoch: 1999, Loss: 0.05793627351522446, Acc: 0.9445642828941345\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "epochs = 2000\n",
    "history_acc, history_loss = train_model(epochs, model, packed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
    "fig.suptitle('Training Metrics')\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(history_loss)\n",
    "\n",
    "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "axes[1].plot(history_acc)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
