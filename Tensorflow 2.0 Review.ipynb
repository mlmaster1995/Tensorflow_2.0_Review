{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow 2.0.0 Basics Review\n",
    "## tf variables\n",
    "## tf operations\n",
    "## tf keras modeling\n",
    "## tf data pipelines\n",
    "## tf layers and activation functions\n",
    "## tf gradients and loss functions\n",
    "## tf custom model training I\n",
    "## tf custom model training II "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow GPU Version: 2.0.0\n",
      "Eager Execution is: True\n",
      "Keras Version: 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- check tensorflow and keras version\n",
    "'''\n",
    "print(f'Tensorflow GPU Version: {tf.__version__}')\n",
    "print(f'Eager Execution is: {tf.executing_eagerly()}')\n",
    "print(f'Keras Version: {tf.keras.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- check GPU \n",
    "'''\n",
    "var = tf.Variable([3,3])\n",
    "if tf.test.is_gpu_available():\n",
    "    print('Running on GPU')\n",
    "else:\n",
    "    print('Runing on CPU')\n",
    "\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf1:  <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.6>\n",
      "tf2:  <tf.Variable 'Variable:0' shape=(3, 3) dtype=int32, numpy=\n",
      "array([[0, 4, 5],\n",
      "       [4, 2, 7],\n",
      "       [7, 8, 9]], dtype=int32)>\n",
      "tf1 iwth Numpy: 5.599999904632568\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- Declare variables\n",
    "'''\n",
    "var = 56\n",
    "tf1 = tf.Variable(var, dtype=tf.float32)\n",
    "tf1.assign(5.6)\n",
    "tf2 = tf.Variable([[0,4,5],[4,2,7],[7,8,9]])\n",
    "print('tf1: ',tf1)\n",
    "print('tf2: ',tf2)\n",
    "print(f'tf1 iwth Numpy: {tf1.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(568, shape=(), dtype=int16)\n",
      "568\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- Declare Constants\n",
    "'''\n",
    "constantVar = tf.constant(568, dtype = tf.int16)\n",
    "print(constantVar)\n",
    "print(constantVar.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var shape: (3, 4)\n",
      "var1 shape: (2, 6)\n",
      "var2 shape: (1, 12)\n",
      "var3 shape: (4, 3)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- Reshape a tensor\n",
    "'''\n",
    "var = tf.Variable([[2,3,4,5],[5,2,9,0],[3,1,2,4]], dtype=tf.float32)\n",
    "print('var shape:',var.shape)\n",
    "var1 = tf.reshape(var,(2,6))\n",
    "print('var1 shape:',var1.shape)\n",
    "var2 = tf.reshape(var, (1,12))\n",
    "print('var2 shape:',var2.shape)\n",
    "var3 = tf.reshape(var, (4,3))\n",
    "print('var3 shape:',var3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var rank: tf.Tensor(3, shape=(), dtype=int32)\n",
      "var2 slice: tf.Tensor(\n",
      "[[[ 2.  3.  4.  5.]\n",
      "  [ 5.  2.  9.  0.]\n",
      "  [ 3.  1.  2.  4.]]\n",
      "\n",
      " [[ 0. 30. 40. 50.]\n",
      "  [ 5.  2.  9.  0.]\n",
      "  [ 3.  1.  2.  4.]]], shape=(2, 3, 4), dtype=float32)\n",
      "var2 rank: 3\n",
      "var2 size: 24\n",
      "var2 dtype: <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tensor dimention\n",
    "- tensor slice\n",
    "- tensor cast numpy\n",
    "- tensor slice\n",
    "- tensor dtype\n",
    "'''\n",
    "var = tf.Variable([[[2,3,4,5],[5,2,9,0],[3,1,2,4]],[[0,30,40,50],[5,2,9,0],[3,1,2,4]],[[12,13,14,15],[5,2,9,0],[3,1,2,4]]], dtype=tf.float32)\n",
    "print('var rank:',tf.rank(var))\n",
    "var2 = var[0:2]\n",
    "print('var2 slice:',var2)\n",
    "print('var2 rank:',tf.rank(var2).numpy())\n",
    "print('var2 size:',tf.size(var2).numpy())\n",
    "print('var2 dtype:', var2.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var tensor:\n",
      " tf.Tensor(\n",
      "[[[  4.   6.   8.  10.]\n",
      "  [ 10.   4.  18.   0.]\n",
      "  [  6.   2.   4.   8.]]\n",
      "\n",
      " [[  0.  60.  80. 100.]\n",
      "  [ 10.   4.  18.   0.]\n",
      "  [  6.   2.   4.   8.]]\n",
      "\n",
      " [[ 24.  26.  28.  30.]\n",
      "  [ 10.   4.  18.   0.]\n",
      "  [  6.   2.   4.   8.]]], shape=(3, 3, 4), dtype=float32)\n",
      "\n",
      "var numpy cast:\n",
      " [[[  4.   6.   8.  10.]\n",
      "  [ 10.   4.  18.   0.]\n",
      "  [  6.   2.   4.   8.]]\n",
      "\n",
      " [[  0.  60.  80. 100.]\n",
      "  [ 10.   4.  18.   0.]\n",
      "  [  6.   2.   4.   8.]]\n",
      "\n",
      " [[ 24.  26.  28.  30.]\n",
      "  [ 10.   4.  18.   0.]\n",
      "  [  6.   2.   4.   8.]]]\n",
      "\n",
      "var*4:\n",
      " tf.Tensor(\n",
      "[[[ 16.  24.  32.  40.]\n",
      "  [ 40.  16.  72.   0.]\n",
      "  [ 24.   8.  16.  32.]]\n",
      "\n",
      " [[  0. 240. 320. 400.]\n",
      "  [ 40.  16.  72.   0.]\n",
      "  [ 24.   8.  16.  32.]]\n",
      "\n",
      " [[ 96. 104. 112. 120.]\n",
      "  [ 40.  16.  72.   0.]\n",
      "  [ 24.   8.  16.  32.]]], shape=(3, 3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tensor element-wise primitive tensor operations\n",
    "- tensor broadcasting\n",
    "'''\n",
    "var1 = tf.Variable([[[2,3,4,5],[5,2,9,0],[3,1,2,4]],[[0,30,40,50],[5,2,9,0],[3,1,2,4]],[[12,13,14,15],[5,2,9,0],[3,1,2,4]]], dtype=tf.float32)\n",
    "var2 = tf.Variable([[[2,2,2,2],[2,2,2,2],[2,2,2,2]],[[2,2,2,2],[2,2,2,2],[2,2,2,2]],[[2,2,2,2],[2,2,2,2],[2,2,2,2]]], dtype=tf.float32)\n",
    "var = var1 * var2\n",
    "print('var tensor:\\n',var)\n",
    "print('\\nvar numpy cast:\\n',var.numpy())\n",
    "print('\\nvar*4:\\n',var*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var:\n",
      " tf.Tensor(\n",
      "[[14 23]\n",
      " [23 50]], shape=(2, 2), dtype=int32)\n",
      "\n",
      "var dtype: <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tensor multiplication\n",
    "- tensor constant\n",
    "- tensor constant cast\n",
    "'''\n",
    "var1 = tf.constant([[1,2,3],[4,5,3]])\n",
    "var2 = tf.constant([[1,2,3],[4,5,3]])\n",
    "var = tf.matmul(var1,tf.transpose(var2))\n",
    "print('var:\\n',var)\n",
    "var = tf.cast(var, dtype=tf.float32)\n",
    "print('\\nvar dtype:', var.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[1.0, 2.0, 4.0], [], [3.0, 4.0], [1.0]]>\n",
      "tf.Tensor([1. 2. 4.], shape=(3,), dtype=float32)\n",
      "tf.Tensor([1.], shape=(1,), dtype=float32)\n",
      "<tf.RaggedTensor [[1, 2, 4], [], [5, 6], [7], [8]]>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- ragged tensors by constant\n",
    "- ragged tensors by split_row\n",
    "'''\n",
    "ragged_tensor = tf.ragged.constant([[1,2,4],[],[3,4],[1]], dtype=tf.float32, name='ragged_tensor')\n",
    "print(ragged_tensor)\n",
    "print(ragged_tensor[0])\n",
    "print(ragged_tensor[3])\n",
    "ragged_tensor2 = tf.RaggedTensor.from_row_splits(values=[1,2,4,5,6,7,8], row_splits=[0,3,3,5,6,7])\n",
    "print(ragged_tensor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_t1:\n",
      " tf.Tensor(\n",
      "[[0.23302452]\n",
      " [0.33036613]\n",
      " [0.07002912]], shape=(3, 1), dtype=float32)\n",
      "_t2:\n",
      " tf.Tensor(\n",
      "[[-1.1006709 ]\n",
      " [-0.40500164]\n",
      " [-1.2100329 ]], shape=(3, 1), dtype=float32)\n",
      "_t:\n",
      " tf.Tensor(\n",
      "[[1.7787435 ]\n",
      " [0.54076576]\n",
      " [1.638559  ]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- calculate square differences\n",
    "'''\n",
    "t1 = tf.random.normal((3,1))\n",
    "t2 = tf.random.normal((3,1))\n",
    "t = tf.math.squared_difference(t1,t2,name='square_difference')\n",
    "print('_t1:\\n',t1)\n",
    "print('_t2:\\n',t2)\n",
    "print('_t:\\n',t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_var:\n",
      " tf.Tensor(\n",
      "[[8.844145   8.424911   5.7227373 ]\n",
      " [4.715581   8.562659   7.411667  ]\n",
      " [0.57933927 8.567917   4.0922523 ]], shape=(3, 3), dtype=float32)\n",
      "\n",
      "_cross_mean:\n",
      " tf.Tensor(6.324579, shape=(), dtype=float32)\n",
      "\n",
      "_x_mean:\n",
      " tf.Tensor([4.7130218 8.518496  5.7422185], shape=(3,), dtype=float32)\n",
      "\n",
      "_x_mean_same_dim:\n",
      " tf.Tensor([[4.7130218 8.518496  5.7422185]], shape=(1, 3), dtype=float32)\n",
      "\n",
      "_y_mean:\n",
      " tf.Tensor([7.6639304 6.8966355 4.4131694], shape=(3,), dtype=float32)\n",
      "\n",
      "_y_mean_same_dim:\n",
      " tf.Tensor(\n",
      "[[7.6639304]\n",
      " [6.8966355]\n",
      " [4.4131694]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- calculate tensor mean\n",
    "'''\n",
    "var = tf.constant(tf.random.uniform((3,3),minval=0, maxval= 10, dtype=tf.float32))\n",
    "cross_mean = tf.reduce_mean(var, axis=None)\n",
    "x_mean = tf.reduce_mean(var, axis=0)\n",
    "y_mean = tf.reduce_mean(var, axis=1)\n",
    "x_mean_dim = tf.reduce_mean(var, axis=0,keepdims=True)\n",
    "y_mean_dim = tf.reduce_mean(var, axis=1, keepdims=True)\n",
    "print('_var:\\n',var)\n",
    "print('\\n_cross_mean:\\n',cross_mean)\n",
    "print('\\n_x_mean:\\n',x_mean)\n",
    "print('\\n_x_mean_same_dim:\\n',x_mean_dim)\n",
    "print('\\n_y_mean:\\n',y_mean)\n",
    "print('\\n_y_mean_same_dim:\\n',y_mean_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_random_var1:\n",
      " tf.Tensor(\n",
      "[[ 0.43616885]\n",
      " [-1.9093795 ]\n",
      " [ 1.3789066 ]\n",
      " [-1.0405852 ]], shape=(4, 1), dtype=float32)\n",
      "_random_var2:\n",
      " tf.Tensor(\n",
      "[[ 4.6377807]\n",
      " [14.660629 ]\n",
      " [-6.065405 ]\n",
      " [ 7.9940577]], shape=(4, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tensors with random normal values\n",
    "'''\n",
    "tf.random.set_seed(2)\n",
    "var_random1 = tf.random.normal((4,1),mean=0,stddev=1)\n",
    "var_random2 = tf.random.normal((4,1),mean=5,stddev=10)\n",
    "print('_random_var1:\\n',var_random1)\n",
    "print('_random_var2:\\n',var_random2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_var1: tf.Tensor(\n",
      "[[3]\n",
      " [1]\n",
      " [3]\n",
      " [1]], shape=(4, 1), dtype=int32)\n",
      "_var2: tf.Tensor(\n",
      "[[7]\n",
      " [5]\n",
      " [7]\n",
      " [7]], shape=(4, 1), dtype=int32)\n",
      "_var_concat_x: tf.Tensor(\n",
      "[[3]\n",
      " [1]\n",
      " [3]\n",
      " [1]\n",
      " [7]\n",
      " [5]\n",
      " [7]\n",
      " [7]], shape=(8, 1), dtype=int32)\n",
      "_var_concat_y: tf.Tensor(\n",
      "[[3 7]\n",
      " [1 5]\n",
      " [3 7]\n",
      " [1 7]], shape=(4, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tensors with random uniform values\n",
    "'''\n",
    "tf.random.set_seed(2)\n",
    "var1 = tf.random.uniform((4,1),minval=1, maxval=4, dtype=tf.int32)\n",
    "var2 = tf.random.uniform((4,1),minval=5, maxval=8, dtype=tf.int32)\n",
    "var_concat_x = tf.concat(values=[var1,var2],axis=0)\n",
    "var_concat_y = tf.concat(values=[var1,var2],axis=1)\n",
    "print('_var1:',var1)\n",
    "print('_var2:',var2)\n",
    "print('_var_concat_x:',var_concat_x)\n",
    "print('_var_concat_y:',var_concat_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_var:\n",
      " tf.Tensor(\n",
      "[[ 4  7  1  3  4  8  8  5  2  0]\n",
      " [ 6  4  0  9  4  3 -2  2  5 -1]\n",
      " [ 3  4  8  0 -1  5  0  8  4  7]\n",
      " [ 8  2  9  7  0  5  7  8  1  2]\n",
      " [ 3  1 -1  9 -2  5  2  7 -2  0]\n",
      " [ 3  8 -1 -2  8  1  5  7  5  6]\n",
      " [ 0  9 -2  3  8  3  4  0  4  0]\n",
      " [ 0  9  2  3  8  1  1  3  1  4]\n",
      " [-2  5  4  6 -2 -1  0  2  0  7]\n",
      " [-2  1  8  6 -2  4  3  6  4  0]], shape=(10, 10), dtype=int32)\n",
      "_max_index_x:\n",
      " [3 6 3 1 5 0 0 2 1 2]\n",
      "_min_index_y:\n",
      " [9 6 4 4 4 3 2 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- indexing tensor values\n",
    "- defualt axis=None, it's across cols to search max or min\n",
    "'''\n",
    "tf.random.set_seed(1)\n",
    "var = tf.constant(tf.random.uniform((10,10), minval=-2, maxval=10, dtype=tf.int32))\n",
    "_max_index_x = tf.argmax(input = var, axis=0, output_type=tf.int32)\n",
    "_min_index_y = tf.argmin(input = var, axis=1, output_type=tf.int32)\n",
    "print('_var:\\n', var)\n",
    "print('_max_index_x:\\n',_max_index_x.numpy())\n",
    "print('_min_index_y:\\n',_min_index_y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original var: \n",
      " [[4 5 6]\n",
      " [4 5 6]\n",
      " [4 5 6]]\n",
      "new var: \n",
      " [[0 0 6]\n",
      " [4 0 0]\n",
      " [0 5 0]]\n",
      "restore var: \n",
      " [[4 5 6]\n",
      " [4 5 6]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- Checkpoint to restore and save tensors\n",
    "- Checkpoint's constructor accepts keyword arguments whose values are types that contain trackable state, such as \n",
    "  ~`tf.keras.optimizers.Optimizer` implementations, \n",
    "  ~`tf.Variable`, \n",
    "  ~`tf.keras.Layer` implementations,\n",
    "  ~`tf.keras.Model` implementations. \n",
    "  It saves these values with a checkpoint and maintains a `save_counter` for numbering checkpoints\n",
    "'''\n",
    "var = tf.Variable([[4,5,6],[4,5,6],[4,5,6]])\n",
    "print('original var: \\n',var.numpy())\n",
    "savePoint = tf.train.Checkpoint(var=var)\n",
    "savePath = savePoint.save('./tk_ckpts/vars')\n",
    "var.assign([[0,0,6],[4,0,0],[0,5,0]])\n",
    "print('new var: \\n',var.numpy())\n",
    "savePoint.restore(savePath)\n",
    "print('restore var: \\n',var.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(23, shape=(), dtype=int32)\n",
      "tf.Tensor(22, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tf.function(\n",
    "    func=None, input_signature=None, autograph=True, experimental_implements=None,\n",
    "    experimental_autograph_options=None, experimental_relax_shapes=False,\n",
    "    experimental_compile=None\n",
    ")\n",
    "\n",
    "'''\n",
    "def calc(x,y):\n",
    "    return x**2*5+y\n",
    "f1 = tf.function(test)\n",
    "print(f1(2,3))\n",
    "\n",
    "@tf.function\n",
    "def calc_2(x,y):\n",
    "    return x*6+y\n",
    "print(calc_2(3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack_x: \n",
      " tf.Tensor(\n",
      "[[1 2 3 4]\n",
      " [1 2 3 4]\n",
      " [1 2 3 4]], shape=(3, 4), dtype=int32)\n",
      "\n",
      "stack_y: \n",
      " tf.Tensor(\n",
      "[[1 1 1]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [4 4 4]], shape=(4, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tf.stack\n",
    "- axis=0, take each item in the list and stack from top to bottom\n",
    "- axis=1, take values of each item in the list and make a row and stack rows from top to bottom \n",
    "'''\n",
    "var1 = tf.constant([1,2,3,4])\n",
    "var2 = tf.constant([1,2,3,4])\n",
    "var3 = tf.constant([1,2,3,4])\n",
    "stack_x = tf.stack([var1,var2,var3], axis=0)\n",
    "stack_y = tf.stack([var1,var2,var3], axis=1)\n",
    "print('stack_x: \\n', stack_x)\n",
    "print('\\nstack_y: \\n',stack_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf keras modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=4445, shape=(2, 2), dtype=float16, numpy=\n",
       "array([[1., 2.],\n",
       "       [3., 4.]], dtype=float16)>"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- declare a variable with backend\n",
    "- Keras is a model-level library, providing high-level building blocks for developing deep learning models. It does not handle low-level operations such as tensor products, convolutions and so on itself. \n",
    "- Instead, it relies on a specialized, well optimized tensor manipulation library to do so, serving as the \"backend engine\" of Keras. Rather than picking one single tensor library and making the \n",
    "implementation of Keras tied to that library, Keras handles the problem in a modular way, and several different backend engines can be plugged seamlessly into Keras.\n",
    "'''\n",
    "var = K.constant([[1,2],[3,4]],dtype=tf.float16)\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- load minist data\n",
    "'''\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_x, train_y),(test_x, test_y)=mnist.load_data()\n",
    "train_x = train_x.astype('float32')/255\n",
    "test_x =test_x.astype('float32')/255\n",
    "train_x = train_x.reshape(train_x.shape[0],28,28,1)\n",
    "test_x = test_x.reshape(test_x.shape[0],28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 6, 6, 64)          18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 87,562\n",
      "Trainable params: 87,370\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tf.keras Functional API\n",
    "'''\n",
    "ipt = tf.keras.Input(shape=(28,28,1))\n",
    "opt = tf.keras.layers.Conv2D(32,3)(ipt)\n",
    "opt = tf.keras.layers.LeakyReLU()(opt)\n",
    "opt = tf.keras.layers.BatchNormalization()(opt)\n",
    "opt = tf.keras.layers.MaxPool2D((3,3))(opt)\n",
    "\n",
    "opt = tf.keras.layers.Conv2D(64,3)(opt)\n",
    "opt = tf.keras.layers.LeakyReLU()(opt)\n",
    "opt = tf.keras.layers.BatchNormalization()(opt)\n",
    "opt = tf.keras.layers.MaxPool2D((3,3))(opt)\n",
    "\n",
    "opt = tf.keras.layers.Flatten()(opt)\n",
    "opt = tf.keras.layers.Dense(256, activation='relu')(opt)\n",
    "opt = tf.keras.layers.Dense(10, activation='softmax')(opt)\n",
    "\n",
    "model = tf.keras.models.Model(ipt, opt)\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=['acc']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.1646 - acc: 0.9532 - val_loss: 0.2478 - val_acc: 0.9220\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.0489 - acc: 0.9844 - val_loss: 0.0399 - val_acc: 0.9858\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0335 - acc: 0.9891 - val_loss: 0.0390 - val_acc: 0.9874\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0295 - acc: 0.9905 - val_loss: 0.0350 - val_acc: 0.9889\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0227 - acc: 0.9929 - val_loss: 0.0549 - val_acc: 0.9819\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0202 - acc: 0.9931 - val_loss: 0.0441 - val_acc: 0.9860\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.0162 - acc: 0.9948 - val_loss: 0.0338 - val_acc: 0.9894\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0144 - acc: 0.9952 - val_loss: 0.0409 - val_acc: 0.9887\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.0131 - acc: 0.9958 - val_loss: 0.0397 - val_acc: 0.9893\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0121 - acc: 0.9959 - val_loss: 0.0354 - val_acc: 0.9905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f67f063e350>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_x, \n",
    "    train_y,\n",
    "    batch_size = 128,\n",
    "    epochs = 10,\n",
    "    validation_data=(test_x,test_y),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- tf.keras.Model class\n",
    "- tf.keras.callbacks.Callback class\n",
    "- tf.keras.callbacks.EarlyStopping class\n",
    "'''\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32,(3,3), padding='same')\n",
    "        self.act1 = tf.keras.layers.LeakyReLU()\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.pool1 = tf.keras.layers.MaxPooling2D((3,3))\n",
    "        \n",
    "        self.conv2 = tf.keras.layers.Conv2D(32,(3,3), padding='same')\n",
    "        self.act2 = tf.keras.layers.LeakyReLU()\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.pool2 = tf.keras.layers.MaxPooling2D((3,3))\n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(512, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.act1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "    \n",
    "class customCallback(tf.keras.callbacks.Callback):\n",
    "    # constructor\n",
    "    def __init__(self):\n",
    "        # parent constructor\n",
    "        super(customCallback,self).__init__()\n",
    "        \n",
    "    # call at the training end    \n",
    "    def on_epoch_end(self, batch, logs=None):\n",
    "        print('\\n train_acc-val_acc:',logs['acc']-logs['val_acc'])\n",
    "        print(self.params)\n",
    "\n",
    "'''\n",
    "ValAccEarlyStopping Class:\n",
    "- val_acc_base is to define the expected val_acc at the end of each epoch traning\n",
    "- if val_acc >= val_acc_base, model will stop training\n",
    "- if early stopping is not triggered by the end of training, the model with best \n",
    "val_acc will be restored\n",
    "'''\n",
    "class ValAccEarlyStopping(tf.keras.callbacks.EarlyStopping):\n",
    "    # constructor\n",
    "    def __init__(self, val_acc_base):\n",
    "        # parent constructor\n",
    "        super(ValAccEarlyStopping,self).__init__(monitor='val_acc', verbose=1, baseline=val_acc_base, restore_best_weights=True)\n",
    "        # fields\n",
    "        self.__best_weights=None\n",
    "        self.__bestWeightEpoch=None\n",
    "        self.__weights =[]\n",
    "        self.__val_acc=[]\n",
    "        \n",
    "    # early stopping method\n",
    "    def on_epoch_end(self,epoch,logs=None): \n",
    "        # restore best model weights\n",
    "        if self.restore_best_weights:\n",
    "            # save weights & val_acc for each epoch\n",
    "            self.__weights.append(self.model.get_weights())\n",
    "            self.__val_acc.append(logs['val_acc'])\n",
    "            \n",
    "            # update the best weights\n",
    "            self.__bestWeightEpoch = self.__val_acc.index(max(self.__val_acc))\n",
    "            self.__best_weights = self.__weights[self.__bestWeightEpoch]           \n",
    "        \n",
    "        # early stopping check\n",
    "        if logs[self.monitor]>=self.baseline:\n",
    "            self.model.stop_training = True\n",
    "            self.stopped_epoch = epoch+1\n",
    "        \n",
    "    # update early stopping training end method         \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0 and self.verbose > 0:\n",
    "            print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))\n",
    "        else:\n",
    "            self.model.set_weights(self.__best_weights)\n",
    "            print(f'Early stopping is not triggered, but best model is restored at epoch {self.__bestWeightEpoch+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.1619 - acc: 0.9514 - val_loss: 0.1679 - val_acc: 0.9587\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.0498 - acc: 0.9843 - val_loss: 0.0428 - val_acc: 0.9859\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6614d91290>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- build model \n",
    "- compile model\n",
    "- train model with custom early stopping class\n",
    "'''\n",
    "model = MyModel()\n",
    "model.compile(\n",
    "    optimizer= tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics= ['acc'],\n",
    ")\n",
    "model.fit(\n",
    "    train_x, \n",
    "    train_y,\n",
    "    batch_size = 128,\n",
    "    epochs = 10,\n",
    "    validation_data = (test_x,test_y),\n",
    "    callbacks = [ValAccEarlyStopping(val_acc_base=0.98)],\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf data pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (60000, 28, 28, 1) , x_train data type:  float32\n",
      "x_test shape:  (10000, 28, 28, 1) , x_test data type:  float32\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "- load fashin_minist dataset\n",
    "- 10 classes 0-9\n",
    "'''\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0],28,28,1).astype('float32')/255\n",
    "x_test = x_test.reshape(x_test.shape[0],28,28,1).astype('float32')/255\n",
    "print('x_train shape: ',x_train.shape,', x_train data type: ', x_train.dtype)\n",
    "print('x_test shape: ',x_test.shape,', x_test data type: ',x_test.dtype)\n",
    " \n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(x_train[258].reshape((28,28)).astype('float32'))\n",
    "plt.title('fashion_mnist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- data pipeline from Numpy Arrays\n",
    "- tf.data.Dataset.from_tensor_slices\n",
    "'''\n",
    "batch_size = 128\n",
    "shuffle_buffer_size = 10000\n",
    "tf.random.set_seed(5)\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(batch_size).shuffle(shuffle_buffer_size)\n",
    "train_data = train_data.repeat()\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test,y_test)).batch(batch_size).shuffle(shuffle_buffer_size)\n",
    "test_data = test_data.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (128, 28, 28, 1)\n",
      "y shape: (128,)\n",
      "x shape: (128, 28, 28, 1)\n",
      "y shape: (128,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- data pipline from Numpy with iterator\n",
    "- tf.data.Dataset.from_tensor_slices\n",
    "- tf.compat.v1.data.make_one_shot_iteraor\n",
    "'''\n",
    "batch_size = 128\n",
    "shuffle_buffer_size = 10000\n",
    "tf.random.set_seed(5)\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(batch_size).shuffle(shuffle_buffer_size)\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test,y_test)).batch(batch_size).shuffle(shuffle_buffer_size)\n",
    "train_iterator = tf.compat.v1.data.make_one_shot_iterator(train_data)\n",
    "test_iterator = tf.compat.v1.data.make_one_shot_iterator(test_data)\n",
    "\n",
    "tr_x,tr_y = train_iterator.get_next()\n",
    "print('x shape:',tr_x.shape)\n",
    "print('y shape:',tr_y.shape)\n",
    "te_x,te_y = test_iterator.get_next()\n",
    "print('x shape:',te_x.shape)\n",
    "print('y shape:',te_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- data pipline from CSV file \n",
    "- tf.data.experimental.make_csv_dataset()\n",
    "'''\n",
    "TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
    "TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n",
    "\n",
    "train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\n",
    "test_file_path = tf.keras.utils.get_file(\"eval.csv\", TEST_DATA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 627 entries, 0 to 626\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   survived            627 non-null    int64  \n",
      " 1   sex                 627 non-null    object \n",
      " 2   age                 627 non-null    float64\n",
      " 3   n_siblings_spouses  627 non-null    int64  \n",
      " 4   parch               627 non-null    int64  \n",
      " 5   fare                627 non-null    float64\n",
      " 6   class               627 non-null    object \n",
      " 7   deck                627 non-null    object \n",
      " 8   embark_town         627 non-null    object \n",
      " 9   alone               627 non-null    object \n",
      "dtypes: float64(2), int64(3), object(5)\n",
      "memory usage: 49.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train_dt = pd.read_csv(train_file_path)\n",
    "train_dt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 264 entries, 0 to 263\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   survived            264 non-null    int64  \n",
      " 1   sex                 264 non-null    object \n",
      " 2   age                 264 non-null    float64\n",
      " 3   n_siblings_spouses  264 non-null    int64  \n",
      " 4   parch               264 non-null    int64  \n",
      " 5   fare                264 non-null    float64\n",
      " 6   class               264 non-null    object \n",
      " 7   deck                264 non-null    object \n",
      " 8   embark_town         264 non-null    object \n",
      " 9   alone               264 non-null    object \n",
      "dtypes: float64(2), int64(3), object(5)\n",
      "memory usage: 20.8+ KB\n"
     ]
    }
   ],
   "source": [
    "test_dt = pd.read_csv(test_file_path)\n",
    "test_dt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMN = 'survived'\n",
    "LABELS = [0,1]\n",
    "\n",
    "def get_csv_data(file_path, batch=3, **kwargs):\n",
    "    csvData = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern = file_path,\n",
    "        batch_size = batch,\n",
    "        label_name = LABEL_COLUMN,\n",
    "        na_value = \"?\",\n",
    "        num_epochs= 1,\n",
    "        ignore_errors = True,\n",
    "        **kwargs\n",
    "    )\n",
    "    return csvData\n",
    "\n",
    "# load csv data\n",
    "train_data = get_csv_data(train_file_path)\n",
    "test_data = get_csv_data(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'male' b'female' b'female']\n",
      "age                 : [32. 28. 39.]\n",
      "n_siblings_spouses  : [0 1 1]\n",
      "parch               : [0 0 1]\n",
      "fare                : [ 56.4958 133.65    83.1583]\n",
      "class               : [b'Third' b'First' b'First']\n",
      "deck                : [b'unknown' b'unknown' b'E']\n",
      "embark_town         : [b'Southampton' b'Southampton' b'Cherbourg']\n",
      "alone               : [b'y' b'n' b'n']\n",
      "\n",
      "labels              : [1 1 1]\n"
     ]
    }
   ],
   "source": [
    "def show_batch(dataset):\n",
    "    '''\n",
    "    - take only one batch of data for the display -> dataset.take(1)\n",
    "    - two batches data will be taken -> dataset.take(2)\n",
    "    - {:20s} -> 20 spaces \n",
    "    ''' \n",
    "    for batch, label in dataset.take(1):\n",
    "        for key, value in batch.items():\n",
    "            print(\"{:20s}: {}\".format(key,value.numpy()))\n",
    "        print(\"\\n{:20s}: {}\".format('labels',label.numpy()))\n",
    "            \n",
    "show_batch(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column_names example data: \n",
      "sex                 : [b'male' b'female' b'female']\n",
      "age                 : [24. 28. 45.]\n",
      "n_siblings_spouses  : [0 3 1]\n",
      "parch               : [0 1 1]\n",
      "fare                : [ 13.      25.4667 164.8667]\n",
      "class               : [b'Second' b'Third' b'First']\n",
      "deck                : [b'unknown' b'unknown' b'unknown']\n",
      "embark_town         : [b'Southampton' b'Southampton' b'Southampton']\n",
      "alone               : [b'y' b'n' b'n']\n",
      "\n",
      "labels              : [0 0 1]\n",
      "\n",
      "select_columns example data: \n",
      "age                 : [28. 40. 33.]\n",
      "parch               : [0 1 0]\n",
      "\n",
      "labels              : [0 1 0]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- column_names -> to name the columes if the dataset has no column names \n",
    "- select_columns -> to select which column of data will be extraced from csv\n",
    "'''\n",
    "temp_data = get_csv_data(train_file_path, column_names = ['survived', 'sex', 'age', 'n_siblings_spouses', 'parch', 'fare', 'class', 'deck', 'embark_town', 'alone'])\n",
    "print('column_names example data: ')\n",
    "show_batch(temp_data)\n",
    "\n",
    "temp_data = get_csv_data(train_file_path, select_columns = ['survived','age','parch'])\n",
    "print('\\nselect_columns example data: ')\n",
    "show_batch(temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                 : [ 3. 28. 29. 29. 28. 28. 22. 22. 45. 18.]\n",
      "n_siblings_spouses  : [4. 1. 1. 1. 0. 0. 0. 0. 1. 0.]\n",
      "parch               : [2. 2. 1. 0. 0. 0. 0. 1. 1. 1.]\n",
      "fare                : [ 31.3875  23.45    10.4625  26.       7.75     8.05     7.2292  55.\n",
      " 164.8667  23.    ]\n",
      "\n",
      "labels              : [1 0 0 1 1 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- load selected column data in one batch size\n",
    "'''\n",
    "SELECT_COLUMNS = ['survived', 'age', 'n_siblings_spouses', 'parch', 'fare']\n",
    "DEFAULTS = [0, 0.0, 0.0, 0.0, 0.0]\n",
    "temp_dataset = get_csv_data(train_file_path, batch=10, select_columns=SELECT_COLUMNS, column_defaults = DEFAULTS)\n",
    "\n",
    "show_batch(temp_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one batch data and labels:\n",
      "age            : [32. 28. 21. 18. 18. 29.  4.  1. 40. 18.]\n",
      "n_siblings_spouses: [0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      "parch          : [0. 0. 0. 0. 0. 0. 2. 2. 0. 1.]\n",
      "fare           : [ 8.3625  7.8792 77.9583  7.775  13.      9.5    22.025  20.575  31.\n",
      "  7.8542]\n",
      "labels              : [0 1 1 0 0 1 1 1 1 0]\n",
      "\n",
      "Stack data rows:\n",
      "[[32.     28.     21.     18.     18.     29.      4.      1.     40.\n",
      "  18.    ]\n",
      " [ 0.      0.      0.      0.      0.      0.      0.      1.      0.\n",
      "   1.    ]\n",
      " [ 0.      0.      0.      0.      0.      0.      2.      2.      0.\n",
      "   1.    ]\n",
      " [ 8.3625  7.8792 77.9583  7.775  13.      9.5    22.025  20.575  31.\n",
      "   7.8542]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- take one example batch\n",
    "- The Python iter() function returns an iterator for the given object.\n",
    "- iter(temp_datset) returns a tensorflow iterator\n",
    "- example_batch -> tensor dictionary\n",
    "- labels_batch -> tensor\n",
    "'''\n",
    "features, labels = next(iter(temp_dataset)) \n",
    "print('one batch data and labels:')\n",
    "for key,values in features.items():\n",
    "     print(\"{:15s}: {}\".format(key,values.numpy()))\n",
    "print(\"{:20s}: {}\".format('labels',labels.numpy()))\n",
    "\n",
    "'''\n",
    "- stack rows from top to bottom to make a matrix \n",
    "'''\n",
    "print('\\nStack data rows:')\n",
    "print(tf.stack(list(features.values()),axis=0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 48.      22.      35.      26.      28.      31.      17.      40.\n",
      "   30.      19.    ]\n",
      " [  1.       0.       0.       0.       1.       1.       0.       0.\n",
      "    0.       0.    ]\n",
      " [  0.       0.       0.       0.       0.       0.       0.       0.\n",
      "    0.       2.    ]\n",
      " [ 39.6      9.35     7.05    56.4958  15.5    113.275    8.6625  31.\n",
      "   10.5     26.2833]]\n",
      "\n",
      "[1 0 0 1 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- define pack function \n",
    "- take features and labels in one batch of data\n",
    "- return a stacked matrix and label pair\n",
    "'''\n",
    "def pack_rows (features, labels):\n",
    "    return tf.stack(list(features.values()),axis=1),labels\n",
    "\n",
    "packed_dataset = temp_dataset.map(pack_rows)\n",
    "\n",
    "for features, labels in packed_dataset.take(1):\n",
    "    print(features.numpy())\n",
    "    print()\n",
    "    print(labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- data pipline for TFRecord\n",
    "'''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following functions can be used to convert a value to a type compatible\n",
    "# with tf.Example.\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "# generate serilize example of row data such as [False, 3, b'chicken', 589] in this function\n",
    "def serialize_example(feature0, feature1, feature2, feature3):\n",
    "    \"\"\"\n",
    "    Creates a tf.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    # Create a dictionary mapping the feature name to the tf.Example-compatible\n",
    "    feature = {\n",
    "        'feature0': _int64_feature(feature0),\n",
    "        'feature1': _int64_feature(feature1),\n",
    "        'feature2': _bytes_feature(feature2),\n",
    "        'feature3': _float_feature(feature3),\n",
    "    }\n",
    "    \n",
    "    # Create a Features message using tf.train.Example.\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes_list {\n",
      "  value: \"test_string\"\n",
      "}\n",
      "\n",
      "bytes_list {\n",
      "  value: \"test_bytes\"\n",
      "}\n",
      "\n",
      "float_list {\n",
      "  value: 2.7182817459106445\n",
      "}\n",
      "\n",
      "int64_list {\n",
      "  value: 1\n",
      "}\n",
      "\n",
      "int64_list {\n",
      "  value: 1\n",
      "}\n",
      "\n",
      "\n",
      "tf.train.feature outputs: \n",
      " bytes_list {\n",
      "  value: \"this is a string...\"\n",
      "}\n",
      "\n",
      "tf.train.feature in string outputs: \n",
      " b'\\n\\x15\\n\\x13this is a string...'\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- generate data compatible with tf.Example\n",
    "- convert opt of tf.train.feature to a string\n",
    "'''\n",
    "# bytes_list\n",
    "print(_bytes_feature(b'test_string'))\n",
    "print(_bytes_feature(u'test_bytes'.encode('utf-8')))\n",
    "# float_list\n",
    "print(_float_feature(np.exp(1)))\n",
    "# int64_list\n",
    "print(_int64_feature(True))\n",
    "print(_int64_feature(1))\n",
    "# convert to string\n",
    "print('\\ntf.train.feature outputs: \\n',_bytes_feature(b'this is a string...'))\n",
    "print('tf.train.feature in string outputs: \\n',_bytes_feature(b'this is a string...').SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, 4, b'goat',0.9876]: \n",
      " b'\\nR\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\n\\x14\\n\\x08feature2\\x12\\x08\\n\\x06\\n\\x04goat\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04[\\xd3|?'\n",
      "\n",
      "[False, 4, b'goat',0.9876]: \n",
      " features {\n",
      "  feature {\n",
      "    key: \"feature0\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"feature1\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 4\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"feature2\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"goat\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"feature3\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 0.9876000285148621\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tf.Example messages\n",
    "- create a numpy dataset\n",
    "'''\n",
    "# The number of observations in the dataset.\n",
    "n_observations = int(1e4)\n",
    "# Boolean feature, encoded as False or True.\n",
    "feature_0 = np.random.choice([False, True], n_observations)\n",
    "# Integer feature, random from 0 to 4.\n",
    "feature_1 = np.random.randint(0, 5, n_observations)\n",
    "# String feature\n",
    "strings = np.array([b'cat', b'dog', b'chicken', b'horse', b'goat'])\n",
    "feature_2 = strings[feature_1]\n",
    "# Float feature, from a standard normal distribution\n",
    "feature_3 = np.random.randn(n_observations)\n",
    "\n",
    "# generate serilized_example\n",
    "serialized_example = serialize_example(False, 4, b'goat', 0.9876)\n",
    "print('''[False, 4, b'goat',0.9876]: \\n''',serialized_example)\n",
    "\n",
    "example_proto = tf.train.Example.FromString(serialized_example)\n",
    "print('''\\n[False, 4, b'goat',0.9876]: \\n''',example_proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-78-99056d6660ed>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-78-99056d6660ed>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    example_features = tf.train.Example(features=tf.train.Features(feature=features))??????????????????\u001b[0m\n\u001b[0m                                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tf.Example with cora dataset: https://relational.fit.cvut.cz/dataset/CORA\n",
    "'''\n",
    "\n",
    "with open(in_file, 'rU') as cora_content:\n",
    "    for line in cora_content:\n",
    "        entries = line.rstrip('\\n').split('\\t')\n",
    "\n",
    "        # entries contains [ID, Word1, Word2, ..., Label]; 'Words' are 0/1 values.\n",
    "        words = map(int, entries[1:-1])\n",
    "        example_id = entries[0]\n",
    "        features = {\n",
    "            'id': _bytes_feature(example_id),\n",
    "            'words': _int64_feature(*words),\n",
    "            'label': _int64_feature(label_index[entries[-1]]),\n",
    "        }\n",
    "        example_features = tf.train.Example(features=tf.train.Features(feature=features))??????????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=8413, shape=(60000, 10), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- one hot encoding\n",
    "'''\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "y_train = tf.one_hot(y_train, depth=10)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.keras.layers.Layer and activation functions\n",
    "## - tf.keras.layers.Layer custom definition (https://www.tensorflow.org/guide/keras/custom_layers_and_models#layers_encapsulate_a_state_weights_and_some_computation) use row matrix multiplication;\n",
    "## - row[...] * mat [...] = row[...]\n",
    "## - weight matrix shape is input_shape * units and this cannot be changed in the custom layer as it has to be consistentcy for further calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial weights:  tf.Tensor(\n",
      "[[ 0.40749726  0.02219561 -2.200683   -0.8212301 ]\n",
      " [-0.36420798 -2.177051   -0.8709116  -0.40788257]\n",
      " [ 0.37394255 -0.49891925 -0.00245201 -0.14541708]\n",
      " [ 0.56160647 -0.88500816 -0.12318895  0.7797051 ]], shape=(4, 4), dtype=float32)\n",
      "initial zeros:  tf.Tensor(\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]], shape=(4, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tf.random_normal_initializer\n",
    "- tf.zeros_initializer\n",
    "'''\n",
    "tf.random.set_seed(15)\n",
    "input_dim = (4,4)\n",
    "init = tf.random_normal_initializer(mean=0.0, stddev=1)\n",
    "init_zeros = tf.zeros_initializer()\n",
    "print('initial weights: ',init(shape=input_dim, dtype=tf.float32))\n",
    "print('initial zeros: ', init_zeros(shape=input_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- single linear combination layer with self-defined weights, units -> output channels, input_shape -> input shape\n",
    "- computation is max[...]*col[...] = col[...]\n",
    "- **kwargs-> allowed_kwargs = {\n",
    "                                'input_shape',\n",
    "                                'batch_input_shape',\n",
    "                                'batch_size',\n",
    "                                'weights',\n",
    "                                'activity_regularizer',\n",
    "                                'autocast'\n",
    "                            }\n",
    "- \n",
    "'''\n",
    "class customLayer(tf.keras.layers.Layer):\n",
    "    #constructor\n",
    "    def __init__(self, units = 3, input_shape=5, trainable=True, name=None, dtype=tf.float32, **kwargs):\n",
    "        # parent constructor\n",
    "        super(customLayer, self).__init__(name=name, dtype= dtype, trainable=trainable, **kwargs)\n",
    "        # fields\n",
    "        init_w = tf.random_normal_initializer(mean=0, stddev=1)\n",
    "        init_b = tf.zeros_initializer()\n",
    "        self.w = tf.Variable(init_w(shape=(units, input_shape),dtype=dtype), trainable=trainable)\n",
    "        self.b = tf.Variable(init_b(shape=(units,)),dtype=dtype, trainable=trainable)       \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        if tf.rank(inputs)==1:\n",
    "            inputs = tf.keras.backend.expand_dims(inputs, axis=1)\n",
    "        return tf.matmul(self.w, inputs)+self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape is at 4 * 1: \n",
      "[1. 1. 1. 1.]\n",
      "\n",
      "layer1 trainable weights is at 5*4: \n",
      "  [[ 2.1618998   0.07567143  1.8947576   1.5536673 ]\n",
      " [-1.2869931  -0.81316155  0.6367824  -1.7842115 ]\n",
      " [ 0.32601485 -0.3001915  -0.73505306  0.06373952]\n",
      " [-0.6815776  -0.5778277   1.0000818   0.6779952 ]\n",
      " [ 0.38209513 -0.7653013  -0.3519354   0.03938751]]\n",
      "\n",
      "output channels is at 5*1: \n",
      " [[ 5.685996    5.685996    5.685996    5.685996    5.685996  ]\n",
      " [-3.2475839  -3.2475839  -3.2475839  -3.2475839  -3.2475839 ]\n",
      " [-0.64549017 -0.64549017 -0.64549017 -0.64549017 -0.64549017]\n",
      " [ 0.4186716   0.4186716   0.4186716   0.4186716   0.4186716 ]\n",
      " [-0.695754   -0.695754   -0.695754   -0.695754   -0.695754  ]]\n",
      "\n",
      "layer1 trainable bias: \n",
      " [0. 0. 0. 0. 0.]\n",
      "\n",
      "layer1 non-trainable weights:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "input_shape=4\n",
    "units= 5\n",
    "ipt = tf.ones((input_shape))\n",
    "layer1 = customLayer(units=units, input_shape=input_shape, name='test')\n",
    "print(f'input shape is at {input_shape} * 1: \\n{ipt}')\n",
    "print(f'\\nlayer1 trainable weights is at {units}*{input_shape}: \\n  {layer1.weights[0].numpy()}',)\n",
    "print(f'\\noutput channels is at {units}*1: \\n {layer1(ipt).numpy()}')\n",
    "print('\\nlayer1 trainable bias: \\n', layer1.weights[1].numpy())\n",
    "print('\\nlayer1 non-trainable weights:\\n', layer1.non_trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- Multi-layer Perceptron(MLP) layer\n",
    "- subclass of tf.keras.layers.Layer class\n",
    "- this layer has no batch size feature\n",
    "- this layer use build() to identify the input shape on the runtime\n",
    "- this layer use row multiplication caculation on weights and inputs\n",
    "- add_weight(\n",
    "        name=None, shape=None, dtype=None, initializer=None, regularizer=None,\n",
    "        trainable=None, constraint=None, partitioner=None, use_resource=None,\n",
    "        synchronization=tf.VariableSynchronization.AUTO,\n",
    "        aggregation=tf.compat.v1.VariableAggregation.NONE, **kwargs\n",
    "    )\n",
    "- **kwargs: `getter`, 'collections`, `experimental_autocast` and `caching_device`.\n",
    "\n",
    "'''\n",
    "class MLP(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=3, activation=None, trainable=True, name=None, dtype=tf.float32, **kwargs):\n",
    "        super(MLP, self).__init__( name=name, trainable = trainable, dtype=dtype, **kwargs)\n",
    "        self.units = units;\n",
    "        self.__activation_name = activation\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),initializer='random_normal')\n",
    "        self.b = self.add_weight(shape=(self.units,),initializer='random_normal')\n",
    "    \n",
    "    def get_config(self):\n",
    "        config_dic ={\n",
    "            'units':self.units,\n",
    "            'activation': self.__activation_name, \n",
    "            'trainable_weights & bias':self.trainable_weights,\n",
    "            'non-trainable_weights & bias':self.non_trainable_weights,\n",
    "        }\n",
    "        config = super(MLP,self).get_config()\n",
    "        config.update(config_dic)\n",
    "        return config\n",
    "    \n",
    "    def call(self, inputs, training = None):\n",
    "        if tf.rank(inputs)==1:\n",
    "            inputs = tf.keras.backend.expand_dims(inputs, axis=0)\n",
    "        linear_combination = tf.matmul(inputs, self.w)+self.b \n",
    "        return self.activation(linear_combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'mlp_single_layer',\n",
       " 'trainable': True,\n",
       " 'dtype': 'float32',\n",
       " 'units': 5,\n",
       " 'activation': None,\n",
       " 'trainable_weights & bias': [<tf.Variable 'mlp_single_layer/Variable:0' shape=(4, 5) dtype=float32, numpy=\n",
       "  array([[-0.04268041, -0.0240161 , -0.00559446,  0.00667279,  0.05542039],\n",
       "         [ 0.0149584 ,  0.04895762,  0.00866133, -0.03208294, -0.02934596],\n",
       "         [ 0.12082821, -0.01217135,  0.06881073, -0.04737529, -0.01472751],\n",
       "         [ 0.09234401, -0.05215368,  0.03285414, -0.01964135,  0.09214985]],\n",
       "        dtype=float32)>,\n",
       "  <tf.Variable 'mlp_single_layer/Variable:0' shape=(5,) dtype=float32, numpy=\n",
       "  array([ 0.06843295,  0.02584788,  0.0653073 , -0.0641969 , -0.0261974 ],\n",
       "        dtype=float32)>],\n",
       " 'non-trainable_weights & bias': []}"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- get_config() function\n",
    "- override get_config() in the tf.keras.layers.Layer\n",
    "- \"input_shape\" in kwards is to generate \"batch_input_shape\" and it should be avoided in the custom layer\n",
    "- instead, use build() to get the real input shape in the custom layer\n",
    "'''\n",
    "input_shape=4\n",
    "units= 5\n",
    "ipt = tf.ones((input_shape))\n",
    "mlp = MLP(units=units, activation=None, name='mlp_single_layer', trainable=True)\n",
    "opt = mlp(ipt)\n",
    "mlp.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape is at 4: \n",
      "[1. 1. 1. 1.]\n",
      "\n",
      "layer1 trainable weights is at 4*5: \n",
      "  [[-0.02856542  0.03515685 -0.06862309  0.07264601 -0.02461529]\n",
      " [-0.00546568 -0.04800047  0.15303433 -0.06815054  0.03404353]\n",
      " [-0.02024032  0.07377522  0.01931716  0.03163869  0.01544571]\n",
      " [-0.05890731 -0.13744293 -0.08588818 -0.01069717 -0.04543935]]\n",
      "\n",
      "layer1 trainable bias is at 5: \n",
      "  [ 0.06411647  0.01416687  0.05103198 -0.03926305 -0.04594219]\n",
      "\n",
      "output channels is at 5: \n",
      " [[-0.04906226 -0.06234445  0.06887219 -0.01382606 -0.06650759]]\n",
      "\n",
      "layer1 non-trainable weights:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- single MLP layer computation\n",
    "- without activation functions\n",
    "- with build() to acquire input_shape\n",
    "'''\n",
    "input_shape=4\n",
    "units= 5\n",
    "ipt = tf.ones((input_shape))\n",
    "mlp = MLP(units=units,activation=None, name='mlp_single_layer')\n",
    "opt = mlp(ipt)\n",
    "print(f'input shape is at {input_shape}: \\n{ipt}')\n",
    "print(f'\\nlayer1 trainable weights is at {input_shape}*{units}: \\n  {mlp.weights[0].numpy()}',)\n",
    "print(f'\\nlayer1 trainable bias is at {units}: \\n  {mlp.weights[1].numpy()}',)\n",
    "print(f'\\noutput channels is at {units}: \\n {mlp(ipt).numpy()}')\n",
    "print('\\nlayer1 non-trainable weights:\\n', mlp.non_trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: [ 0.40749726  0.02219561 -2.200683   -0.8212301  -0.36420798]\n",
      "\n",
      "final output:  [[0.00949545]]\n",
      "\n",
      "mlp1 layer weights and bias: \n",
      " [array([[-0.05752216, -0.01777615, -0.0443813 , -0.07142202,  0.03639229],\n",
      "       [ 0.05079982,  0.01979304, -0.01862826, -0.01179774,  0.03318491],\n",
      "       [-0.0077181 , -0.0696383 , -0.09101368, -0.00102651,  0.03367503],\n",
      "       [-0.07211521,  0.02843316, -0.05534288, -0.08601289,  0.10155316],\n",
      "       [-0.00349029, -0.05710344,  0.02195667,  0.02711194, -0.05647339]],\n",
      "      dtype=float32), array([ 0.01557087,  0.06733529, -0.02465851, -0.09773596,  0.00385595],\n",
      "      dtype=float32)]\n",
      "\n",
      "mlp2 layer weights and bias: \n",
      " [array([[-0.02640516],\n",
      "       [ 0.02764253],\n",
      "       [-0.0596687 ],\n",
      "       [-0.02553965],\n",
      "       [ 0.04818435]], dtype=float32), array([0.02116098], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- multiple MLP layers computation\n",
    "- without activation functions\n",
    "- with input_shape acquired by build() function\n",
    "'''\n",
    "tf.random.set_seed(15)\n",
    "input_shape=5\n",
    "ipt = tf.random.normal(shape=(input_shape,))\n",
    "mlp1 = MLP(units=5, activation= None, name='mlp_layer1')\n",
    "mlp2 = MLP(units=1, activation=None,  name='mlp_layer2')\n",
    "opt = mlp1(ipt)\n",
    "opt = mlp2(opt)\n",
    "print('input:', ipt.numpy())\n",
    "print('\\nfinal output: ', opt.numpy())\n",
    "print('\\nmlp1 layer weights and bias: \\n',[val.numpy() for val in mlp1.weights])\n",
    "print('\\nmlp2 layer weights and bias: \\n',[val.numpy() for val in mlp2.weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: \n",
      " [[0.08908382 0.09991569 0.10659099 0.10593536 0.09802915 0.08910162\n",
      "  0.09953129 0.11073688 0.09931216 0.10176301]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- multiple MLP with 'sigmoid' activation function\n",
    "'''\n",
    "tf.random.set_seed(15)\n",
    "input_shape=5\n",
    "ipt = tf.random.normal(shape=(input_shape,))\n",
    "opt = MLP(units = 256, activation = 'relu')(ipt)\n",
    "opt = MLP(units = 128, activation = 'relu')(ipt)\n",
    "opt = MLP(units =10, activation= 'softmax')(opt)\n",
    "print('output: \\n',opt.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input is: \n",
      " [[ 0.19024122 -0.4606145  -0.13381146 -2.562121   -0.72492903  0.613187\n",
      "  -0.15034355 -1.5807154   0.6264738   0.9457944 ]]\n",
      "\n",
      "default linear activation output: \n",
      " [[ 0.19024122 -0.4606145  -0.13381146 -2.562121   -0.72492903  0.613187\n",
      "  -0.15034355 -1.5807154   0.6264738   0.9457944 ]]\n",
      "\n",
      "sigmoid activation output: \n",
      " [[0.5474174  0.38684008 0.46659696 0.07161635 0.32630855 0.64866745\n",
      "  0.46248475 0.17069417 0.65168947 0.72026867]]\n",
      "\n",
      "tanh activation output: \n",
      " [[ 0.18797891 -0.43058494 -0.1330185  -0.98816895 -0.61995316  0.54636663\n",
      "  -0.14922096 -0.91871357  0.5556194   0.7378732 ]]\n",
      "\n",
      "relu activation output: \n",
      " [[0.19024122 0.         0.         0.         0.         0.613187\n",
      "  0.         0.         0.6264738  0.9457944 ]]\n",
      "\n",
      "softmax activation output: \n",
      " [[0.11373109 0.05932205 0.08225171 0.00725343 0.04554344 0.17360501\n",
      "  0.08090309 0.01935363 0.17592703 0.24210948]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- tf activation functions\n",
    "- tf.keras.activations module\n",
    "'''\n",
    "ipt = tf.random.normal(shape=(1,10))\n",
    "print('input is: \\n', ipt.numpy())\n",
    "\n",
    "linear_res = tf.keras.activations.get(None)(ipt)\n",
    "print('\\ndefault linear activation output: \\n',linear_res.numpy())\n",
    "\n",
    "sigmoid_res = tf.keras.activations.get('sigmoid')(ipt)\n",
    "print('\\nsigmoid activation output: \\n',sigmoid_res.numpy())\n",
    "\n",
    "tanh_res = tf.keras.activations.tanh(ipt)\n",
    "print('\\ntanh activation output: \\n',tanh_res.numpy())\n",
    "\n",
    "relu_res = tf.keras.activations.relu(ipt)\n",
    "print('\\nrelu activation output: \\n',relu_res.numpy())\n",
    "\n",
    "softmax_res = tf.keras.activations.softmax(ipt)\n",
    "print('\\nsoftmax activation output: \\n',softmax_res.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf gradients and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy_dw: 2.0,\n",
      "dy_db: 1.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- batch = 1\n",
    "- x is tf.constant and it is not trainable so tape.gradient() on x is not working, \n",
    "- weight and bias is tf.Variable and it is trainalbe\n",
    "'''\n",
    "x = tf.constant(2.0) \n",
    "weight = tf.Variable(2.0)\n",
    "bias = tf.Variable(2.0)\n",
    "with tf.GradientTape(persistent = True, watch_accessed_variables=True) as tape:\n",
    "    y = weight * x + bias\n",
    "    \n",
    "dy_dw = tape.gradient(y, weight)\n",
    "dy_db = tape.gradient(y, bias)\n",
    "    \n",
    "print(f'dy_dw: {dy_dw},\\ndy_db: {dy_db}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[6.]\n",
      " [6.]\n",
      " [6.]], shape=(3, 1), dtype=float32)\n",
      "dy_dw: 6.0,\n",
      "dy_db: 3.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- btach = 3\n",
    "- x is tf.constant and it is not trainable so tape.gradient() on x is not working, \n",
    "- weight and bias is tf.Variable and it is trainalbe\n",
    "'''\n",
    "x = tf.constant([[2],[2],[2]], dtype=tf.float32) \n",
    "weight = tf.Variable(2, dtype=tf.float32)\n",
    "bias = tf.Variable(2, dtype=tf.float32)\n",
    "with tf.GradientTape(persistent = True, watch_accessed_variables=True) as tape:\n",
    "    y = weight * x + bias\n",
    "    \n",
    "dy_dw = tape.gradient(y, weight)\n",
    "dy_db = tape.gradient(y, bias)\n",
    "\n",
    "print(y)\n",
    "print(f'dy_dw: {dy_dw},\\ndy_db: {dy_db}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy_dw: 48.0,\n",
      "dy_dw2:36.0,\n",
      "dy_db: 24.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- batch =1\n",
    "- gradients for nested calculation function \n",
    "'''\n",
    "x = tf.constant(2.0) \n",
    "weight = tf.Variable(2, dtype=tf.float32)\n",
    "weight2 = tf.Variable(2, dtype=tf.float32)\n",
    "bias = tf.Variable(2.0, dtype=tf.float32)\n",
    "\n",
    "def y1 (weight, bias, x):\n",
    "    return weight * x + bias\n",
    "\n",
    "def y1_square(x):\n",
    "    return weight2*x**2\n",
    "\n",
    "with tf.GradientTape(persistent = True, watch_accessed_variables=True) as tape:\n",
    "    y = y1_square(y1(weight, bias, x))   \n",
    "    \n",
    "dy_dw = tape.gradient(y, weight)\n",
    "dy_dw2 = tape.gradient(y, weight2)\n",
    "dy_db = tape.gradient(y, bias)\n",
    "    \n",
    "print(f'dy_dw: {dy_dw},\\ndy_dw2:{dy_dw2},\\ndy_db: {dy_db}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy_dw:\n",
      "[[1.]\n",
      " [2.]\n",
      " [3.]],\n",
      "dy_db:\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- btach = 1\n",
    "- gradients on linear combination function with 1-dim weights\n",
    "'''\n",
    "ipt = tf.Variable([[1,2,3]],dtype=tf.float32) \n",
    "w = tf.Variable([[1],[2],[3]],dtype=tf.float32)\n",
    "b = tf.Variable([1],dtype=tf.float32)\n",
    "\n",
    "def linear_combination(ipt,weight,bias):\n",
    "    return tf.matmul(ipt,weight)+bias\n",
    "\n",
    "with tf.GradientTape(persistent = True, watch_accessed_variables=True) as tape:\n",
    "    y = linear_combination(ipt,w,b)\n",
    "dy_dw = tape.gradient(y, w)\n",
    "dy_db = tape.gradient(y, b)\n",
    "    \n",
    "print(f'dy_dw:\\n{dy_dw},\\ndy_db:\\n{dy_db}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy_dw:\n",
      "[[ 4.0688735e-01  1.2480556e-02  1.3418210e-01]\n",
      " [-1.7811962e+00 -5.4635078e-02 -5.8739763e-01]\n",
      " [ 1.2863358e+00  3.9456099e-02  4.2420402e-01]\n",
      " [-9.7072709e-01 -2.9775353e-02 -3.2012349e-01]\n",
      " [-8.9167268e-04 -2.7350497e-05 -2.9405317e-04]],\n",
      "dy_db:\n",
      "[[0.9328666  0.02861405 0.30763796]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- multiple inputs on a logistic regression calculation with multiple outpus\n",
    "- binary_cross_entropy for the loss\n",
    "'''\n",
    "tf.random.set_seed(2)\n",
    "init = tf.random_normal_initializer(mean=0.0, stddev=1)\n",
    "ipt = tf.Variable(initial_value=init(shape=(1,5),dtype=tf.float32)) \n",
    "w = tf.Variable(initial_value= init(shape=(5,3),dtype=tf.float32))\n",
    "b = tf.Variable(initial_value= init(shape=(1,3),dtype=tf.float32))\n",
    "target = tf.Variable(initial_value = init(shape=(1,3),dtype= tf.float32))\n",
    "\n",
    "def logistic_regression(ipt,weight,bias):\n",
    "    return tf.keras.activations.sigmoid(tf.matmul(ipt,weight)+bias)\n",
    "\n",
    "def cross_entropy_loss(label,predict):\n",
    "    loss =  tf.keras.losses.BinaryCrossentropy()\n",
    "    return loss(label,predict)\n",
    "\n",
    "with tf.GradientTape(persistent = True, watch_accessed_variables=True) as tape:\n",
    "    loss = cross_entropy_loss(target,logistic_regression(ipt,w,b))\n",
    "dy_dw = tape.gradient(loss, w)\n",
    "dy_db = tape.gradient(loss, b)\n",
    "    \n",
    "print(f'dy_dw:\\n{dy_dw},\\ndy_db:\\n{dy_db}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.keras custom model training I\n",
    "### - training a small dataset with stochastic gradient decent\n",
    "### - loss function is mean least square\n",
    "### - MLP model to simulate a linear combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- MLP layer\n",
    "'''\n",
    "class MLP(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=3, activation=None, trainable=True, name=None, dtype=tf.float32, **kwargs):\n",
    "        super(MLP, self).__init__( name=name, trainable = trainable, dtype=dtype, **kwargs)\n",
    "        self.units = units\n",
    "        self.__activation_name = activation\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),initializer='random_normal',)\n",
    "        self.b = self.add_weight(shape=(self.units,),initializer='random_normal')\n",
    "        \n",
    "    def get_config(self):\n",
    "        config_dic ={\n",
    "            'units':self.units,\n",
    "            'activation': self.__activation_name, \n",
    "            'trainable_weights & bias':self.trainable_weights,\n",
    "            'non-trainable_weights & bias':self.non_trainable_weights,\n",
    "        }\n",
    "        config = super(MLP,self).get_config()\n",
    "        config.update(config_dic)\n",
    "        return config\n",
    "    \n",
    "    def call(self, inputs, training = None):\n",
    "        if tf.rank(inputs)==1:\n",
    "            inputs = tf.keras.backend.expand_dims(inputs, axis=0)\n",
    "        linear_combination = tf.matmul(inputs, self.w)+self.b \n",
    "        return self.activation(linear_combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- subclass model\n",
    "'''\n",
    "class MLP_Model(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MLP_Model, self).__init__(self, **kwargs)\n",
    "        self.mlp1 = MLP(5)\n",
    "        self.mlp2 = MLP(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        opt = self.mlp1(inputs)\n",
    "        opt = self.mlp2(opt)\n",
    "        return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "- generate random data \n",
    "'''\n",
    "# define random data\n",
    "w = 6\n",
    "b = 1\n",
    "batch = 1000\n",
    "inputs  = tf.random.normal(shape=(batch,1), mean=0, stddev=1)\n",
    "noise   = tf.random.normal(shape=(batch,1), mean=0, stddev=1)\n",
    "outputs = inputs * w + b + noise\n",
    "\n",
    "# plotting \n",
    "model = MLP_Model()\n",
    "plt.scatter(inputs,outputs, c='b')\n",
    "plt.scatter(inputs, model(inputs), c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final epoch: 50, loss: 0.9573383331298828\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "- custom model training with SGD and mean square loss function\n",
    "'''\n",
    "# define model\n",
    "model = MLP_Model()\n",
    "\n",
    "# define loss\n",
    "mean_square_loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# gradient function -> return loss -> return gradients\n",
    "def get_grad(target, model, data):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = mean_square_loss(target, model(data))\n",
    "    return loss, tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "# define optimizer\n",
    "SGD = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# training process\n",
    "num_epochs = 50\n",
    "for _ in range(num_epochs):\n",
    "    loss, grad = get_grad(outputs, model, inputs)\n",
    "    SGD.apply_gradients(zip(grad, model.trainable_variables))\n",
    "print(f'final epoch: {SGD.iterations.numpy()}, loss: {loss}')\n",
    "\n",
    "# plotting \n",
    "plt.scatter(inputs,outputs, c='b')\n",
    "plt.scatter(inputs, model(inputs), c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.keras custom model training II\n",
    "## - training model a formal dataset with batches \n",
    "## - loss function is categorial cross entropy\n",
    "## - optimizer is SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>120</th>\n",
       "      <th>4</th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   120    4  setosa  versicolor  virginica\n",
       "0  6.4  2.8     5.6         2.2          2\n",
       "1  5.0  2.3     3.3         1.0          1\n",
       "2  4.9  2.5     4.5         1.7          2\n",
       "3  4.9  3.1     1.5         0.1          0\n",
       "4  5.7  3.8     1.7         0.3          0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tensorflow built in dataset\n",
    "train_dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\"\n",
    "train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),\n",
    "                                           origin=train_dataset_url)\n",
    "\n",
    "data = pd.read_csv(train_dataset_url)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120 entries, 0 to 119\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   120         120 non-null    float64\n",
      " 1   4           120 non-null    float64\n",
      " 2   setosa      120 non-null    float64\n",
      " 3   versicolor  120 non-null    float64\n",
      " 4   virginica   120 non-null    int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 4.8 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "Label: species\n",
      "Label classes:['Iris setosa', 'Iris versicolor', 'Iris virginica']\n"
     ]
    }
   ],
   "source": [
    "# column order in CSV file\n",
    "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "class_names = ['Iris setosa', 'Iris versicolor', 'Iris virginica']\n",
    "\n",
    "feature_names = column_names[:-1]\n",
    "label_name = column_names[-1]\n",
    "\n",
    "print(\"Features: {}\".format(feature_names))\n",
    "print(\"Label: {}\".format(label_name))\n",
    "print(f\"Label classes:{class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  species\n",
       "0           6.4          2.8           5.6          2.2        2\n",
       "1           5.0          2.3           3.3          1.0        1\n",
       "2           4.9          2.5           4.5          1.7        2\n",
       "3           4.9          3.1           1.5          0.1        0\n",
       "4           5.7          3.8           1.7          0.3        0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename the dataset column name\n",
    "data = data.rename(columns={'120': \"sepal_length\", '4': \"sepal_width\", 'setosa': \"petal_length\",'versicolor':'petal_width','virginica':'species'})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# count label classes\n",
    "data['species'].value_counts().sort_values(ascending=False).plot.bar(figsize=(8,5))\n",
    "plt.title('Label Class Count')\n",
    "plt.xlabel('0-Iris setosa, 1- Iris versicolor, 2-Iris virginica')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal_length\n",
      "5.845000000000001\n",
      "0.8685784774150068\n",
      "sepal_width\n",
      "3.064999999999999\n",
      "0.4271559257155984\n",
      "petal_length\n",
      "3.739166666666667\n",
      "1.822100359444549\n",
      "petal_width\n",
      "1.196666666666666\n",
      "0.7820392791757137\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- data normalization -> Z score\n",
    "'''\n",
    "for name, value in data.iteritems():\n",
    "    if name is not 'species':\n",
    "        print(name)\n",
    "        print(value.mean())\n",
    "        print(value.std())\n",
    "        data[name] = (data[name]-value.mean())/ value.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.638975</td>\n",
       "      <td>-0.620382</td>\n",
       "      <td>1.021257</td>\n",
       "      <td>1.282971</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.972854</td>\n",
       "      <td>-1.790915</td>\n",
       "      <td>-0.241022</td>\n",
       "      <td>-0.251479</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.087985</td>\n",
       "      <td>-1.322702</td>\n",
       "      <td>0.417558</td>\n",
       "      <td>0.643616</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.087985</td>\n",
       "      <td>0.081937</td>\n",
       "      <td>-1.228893</td>\n",
       "      <td>-1.402317</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.166939</td>\n",
       "      <td>1.720683</td>\n",
       "      <td>-1.119130</td>\n",
       "      <td>-1.146575</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  species\n",
       "0      0.638975    -0.620382      1.021257     1.282971        2\n",
       "1     -0.972854    -1.790915     -0.241022    -0.251479        1\n",
       "2     -1.087985    -1.322702      0.417558     0.643616        2\n",
       "3     -1.087985     0.081937     -1.228893    -1.402317        0\n",
       "4     -0.166939     1.720683     -1.119130    -1.146575        0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.to_csv('./processed_data.csv', index=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (OrderedDict([(sepal_length, (None,)), (sepal_width, (None,)), (petal_length, (None,)), (petal_width, (None,))]), (None,)), types: (OrderedDict([(sepal_length, tf.float32), (sepal_width, tf.float32), (petal_length, tf.float32), (petal_width, tf.float32)]), tf.int32)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- generate csv file data pipeline with tf.data.Dataset\n",
    "'''\n",
    "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "\n",
    "dataCSV = tf.data.experimental.make_csv_dataset(\n",
    "    file_pattern = './processed_data.csv', \n",
    "    batch_size = 32,\n",
    "    column_names= column_names, \n",
    "    select_columns= None,\n",
    "    label_name= 'species', \n",
    "    num_epochs = 1,\n",
    "    shuffle=True,\n",
    "    shuffle_buffer_size=100, \n",
    ")\n",
    "dataCSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal_length :\n",
      " [ 0.06332185 -1.3182459   2.1356735   1.5600202  -0.05180879  0.984367\n",
      " -0.97285396 -0.8577233  -0.97285396 -0.51233137 -0.39720073 -1.0879846\n",
      "  0.86923635 -1.4333766   2.1356735  -0.16693944 -0.97285396 -0.8577233\n",
      " -0.51233137 -0.05180879  1.0994977  -0.39720073 -1.4333766  -0.8577233\n",
      "  1.2146283   0.5238444   0.29358315 -0.05180879 -1.0879846  -0.39720073\n",
      "  0.29358315 -0.16693944]\n",
      "sepal_width :\n",
      " [ 0.31604385  0.31604385 -0.15216926  0.31604385 -0.8544889  -0.15216926\n",
      " -1.7909151   1.7206831   0.31604385 -0.15216926  1.0183635  -1.5568086\n",
      " -0.3862758   0.08193729 -0.62038237  1.7206831  -2.4932349   1.7206831\n",
      "  1.9547896  -1.0885955   0.31604385 -1.0885955   1.25247     1.7206831\n",
      "  0.08193729  0.5501504  -0.62038237 -0.8544889  -1.322702   -1.5568086\n",
      " -0.15216926 -0.62038237]\n",
      "petal_length :\n",
      " [ 0.58220357 -1.3386565   1.2956659   1.2407842   0.7468487   0.691967\n",
      " -0.24102221 -1.2288932  -1.3935384   0.4175584  -1.3386565  -0.24102221\n",
      "  0.47244012 -1.2288932   1.6249563  -1.1191298  -0.13125877 -1.0093663\n",
      " -1.3386565   0.14314982  1.1859025   0.36267668 -1.5033017  -1.1740115\n",
      "  0.7468487   1.2407842   0.5273219   0.7468487   0.4175584  -0.02149534\n",
      "  0.63708526  0.19803153]\n",
      "petal_width :\n",
      " [ 0.77148724 -1.2744458   1.4108413   0.77148724  0.8993581   0.64361644\n",
      " -0.25147927 -1.146575   -1.2744458   0.3878748  -1.2744458  -0.25147927\n",
      "  0.13213317 -1.2744458   1.0272288  -1.146575   -0.25147927 -1.0187042\n",
      " -1.0187042   0.00426236  1.4108413   0.00426236 -1.2744458  -1.2744458\n",
      "  1.4108413   1.666583    0.00426236  0.8993581   0.64361644 -0.25147927\n",
      "  0.77148724  0.13213317]\n",
      "labels:\n",
      " [1 0 2 2 2 1 1 0 0 1 0 1 1 0 2 0 1 0 0 1 2 1 0 0 2 2 1 2 2 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- print one batch data and labels\n",
    "'''\n",
    "features, labels = next(iter(dataCSV))\n",
    "for key, value in features.items():\n",
    "    print(key,\":\\n\",value.numpy())\n",
    "print('labels:\\n',labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- pack data into tensor \n",
    "'''\n",
    "def pack_rows (features, labels):\n",
    "    labels = tf.one_hot(labels, 3)\n",
    "    return tf.stack(list(features.values()),axis=1),labels\n",
    "\n",
    "packed_dataset = dataCSV.map(pack_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: \n",
      " tf.Tensor(\n",
      "[[ 2.0205429  -0.15216926  1.5700746   1.1550997 ]\n",
      " [ 0.06332185 -0.15216926  0.7468487   0.77148724]\n",
      " [-1.0879846  -1.5568086  -0.24102221 -0.25147927]\n",
      " [ 0.6389751   0.31604385  0.85661215  1.4108413 ]\n",
      " [ 1.6751509  -0.3862758   1.4054294   0.77148724]\n",
      " [ 1.0994977  -0.15216926  0.9663756   1.1550997 ]\n",
      " [-0.05180879 -0.62038237  0.7468487   1.5387121 ]\n",
      " [-0.51233137  0.78425694 -1.2288932  -1.0187042 ]\n",
      " [-0.8577233   1.7206831  -1.0093663  -1.0187042 ]\n",
      " [ 0.17845249 -0.8544889   0.7468487   0.51574564]], shape=(10, 4), dtype=float32)\n",
      "labels: \n",
      " tf.Tensor(\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]], shape=(10, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "features, labels = next(iter(packed_dataset))\n",
    "print('features: \\n',features[:10])\n",
    "print('labels: \\n',labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 739\n",
      "Trainable params: 739\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- build model with tf.kers API\n",
    "'''\n",
    "def get_model():\n",
    "    ipt = tf.keras.Input(shape=(4,))\n",
    "    opt = tf.keras.layers.Dense(32, activation='relu')(ipt)\n",
    "    opt = tf.keras.layers.Dense(16, activation='relu')(opt)\n",
    "    opt = tf.keras.layers.Dense(3, activation='softmax')(opt)\n",
    "    model = tf.keras.Model(ipt, opt)\n",
    "    return model\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- custom training process\n",
    "- batch_size -> 32\n",
    "- num_epochs -> 1 -> dataset will be repeated 1 time per epoch\n",
    "- loss -> categorical_cross_entroty\n",
    "- optimizer -> SGD\n",
    "'''\n",
    "loss_func = tf.keras.losses.CategoricalCrossentropy()# define optimizer\n",
    "SGD = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "def grad(target, model, data):\n",
    "    with tf.GradientTape() as tape: \n",
    "        loss = loss_func(target, model(data))\n",
    "    return loss, tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "def train_model(epochs, model, packed_dataset):\n",
    "    history_acc = []\n",
    "    history_loss = []\n",
    "    for _ in range(epochs):\n",
    "        step_loss = tf.keras.metrics.Mean()\n",
    "        step_acc = tf.keras.metrics.CategoricalCrossentropy()\n",
    "        '''\n",
    "        - train model for steps per epoch\n",
    "        '''\n",
    "        for x,y in packed_dataset: \n",
    "            '''\n",
    "            - calculate loss and grad\n",
    "            - update weights with grad\n",
    "            '''\n",
    "            loss, gradient = grad(y, model, x)\n",
    "            SGD.apply_gradients(zip(gradient, model.trainable_variables))\n",
    "            '''\n",
    "            - record each step loss and acc\n",
    "            '''\n",
    "            step_loss.update_state(loss)\n",
    "            step_acc.update_state(y, model(x, training = True))\n",
    "        '''\n",
    "        - record training loss and acc per epochs\n",
    "        '''\n",
    "        history_loss.append(step_loss.result())\n",
    "        history_acc.append(1-step_acc.result())\n",
    "        \n",
    "        print(f'Epoch: {_}, Loss: {step_loss.result()}, Acc: {1-step_acc.result()}')\n",
    "    return history_acc, history_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.1046162843704224, Acc: -0.09449243545532227\n",
      "Epoch: 1, Loss: 1.0682268142700195, Acc: -0.06209743022918701\n",
      "Epoch: 2, Loss: 1.0405062437057495, Acc: -0.03126382827758789\n",
      "Epoch: 3, Loss: 1.0094778537750244, Acc: -0.0031818151473999023\n",
      "Epoch: 4, Loss: 0.9870929718017578, Acc: 0.023600757122039795\n",
      "Epoch: 5, Loss: 0.9603358507156372, Acc: 0.04712611436843872\n",
      "Epoch: 6, Loss: 0.9367320537567139, Acc: 0.07090336084365845\n",
      "Epoch: 7, Loss: 0.9129051566123962, Acc: 0.09260743856430054\n",
      "Epoch: 8, Loss: 0.8922386765480042, Acc: 0.11384975910186768\n",
      "Epoch: 9, Loss: 0.8714143633842468, Acc: 0.13338381052017212\n",
      "Epoch: 10, Loss: 0.8540022373199463, Acc: 0.15213745832443237\n",
      "Epoch: 11, Loss: 0.8340117335319519, Acc: 0.16998648643493652\n",
      "Epoch: 12, Loss: 0.8134689927101135, Acc: 0.18735593557357788\n",
      "Epoch: 13, Loss: 0.800219714641571, Acc: 0.20406723022460938\n",
      "Epoch: 14, Loss: 0.7856563329696655, Acc: 0.21973997354507446\n",
      "Epoch: 15, Loss: 0.7673563957214355, Acc: 0.23575764894485474\n",
      "Epoch: 16, Loss: 0.752836287021637, Acc: 0.2512776255607605\n",
      "Epoch: 17, Loss: 0.7407625317573547, Acc: 0.26645153760910034\n",
      "Epoch: 18, Loss: 0.7217832207679749, Acc: 0.28080713748931885\n",
      "Epoch: 19, Loss: 0.7064163088798523, Acc: 0.2948635220527649\n",
      "Epoch: 20, Loss: 0.6931714415550232, Acc: 0.30892831087112427\n",
      "Epoch: 21, Loss: 0.6827701926231384, Acc: 0.3225574493408203\n",
      "Epoch: 22, Loss: 0.668108344078064, Acc: 0.3355482816696167\n",
      "Epoch: 23, Loss: 0.6596384644508362, Acc: 0.3481552004814148\n",
      "Epoch: 24, Loss: 0.6416897177696228, Acc: 0.3603135943412781\n",
      "Epoch: 25, Loss: 0.6272833347320557, Acc: 0.372303307056427\n",
      "Epoch: 26, Loss: 0.615005373954773, Acc: 0.3838042616844177\n",
      "Epoch: 27, Loss: 0.6052185297012329, Acc: 0.39546430110931396\n",
      "Epoch: 28, Loss: 0.5968157052993774, Acc: 0.40608352422714233\n",
      "Epoch: 29, Loss: 0.5829683542251587, Acc: 0.4165269136428833\n",
      "Epoch: 30, Loss: 0.5747837424278259, Acc: 0.42697352170944214\n",
      "Epoch: 31, Loss: 0.5663217306137085, Acc: 0.43682968616485596\n",
      "Epoch: 32, Loss: 0.5473902821540833, Acc: 0.44680899381637573\n",
      "Epoch: 33, Loss: 0.5506529808044434, Acc: 0.4558826684951782\n",
      "Epoch: 34, Loss: 0.5369558334350586, Acc: 0.46487683057785034\n",
      "Epoch: 35, Loss: 0.5274726152420044, Acc: 0.4732859134674072\n",
      "Epoch: 36, Loss: 0.5225726366043091, Acc: 0.4818347692489624\n",
      "Epoch: 37, Loss: 0.5081075429916382, Acc: 0.4896770119667053\n",
      "Epoch: 38, Loss: 0.5088344812393188, Acc: 0.497355580329895\n",
      "Epoch: 39, Loss: 0.4998592734336853, Acc: 0.5048108100891113\n",
      "Epoch: 40, Loss: 0.4958069920539856, Acc: 0.5122548937797546\n",
      "Epoch: 41, Loss: 0.48086386919021606, Acc: 0.5192019939422607\n",
      "Epoch: 42, Loss: 0.46951380372047424, Acc: 0.5256547331809998\n",
      "Epoch: 43, Loss: 0.46890631318092346, Acc: 0.5322586297988892\n",
      "Epoch: 44, Loss: 0.4664902091026306, Acc: 0.5383590459823608\n",
      "Epoch: 45, Loss: 0.45988982915878296, Acc: 0.5445284843444824\n",
      "Epoch: 46, Loss: 0.4530905783176422, Acc: 0.5503057241439819\n",
      "Epoch: 47, Loss: 0.44334733486175537, Acc: 0.5562562346458435\n",
      "Epoch: 48, Loss: 0.45069581270217896, Acc: 0.5613319277763367\n",
      "Epoch: 49, Loss: 0.4327753782272339, Acc: 0.5667355060577393\n",
      "Epoch: 50, Loss: 0.42473405599594116, Acc: 0.5715236663818359\n",
      "Epoch: 51, Loss: 0.43203818798065186, Acc: 0.576507568359375\n",
      "Epoch: 52, Loss: 0.41096144914627075, Acc: 0.5814208984375\n",
      "Epoch: 53, Loss: 0.42075222730636597, Acc: 0.5862479209899902\n",
      "Epoch: 54, Loss: 0.40990808606147766, Acc: 0.5902374982833862\n",
      "Epoch: 55, Loss: 0.4050435721874237, Acc: 0.5946485996246338\n",
      "Epoch: 56, Loss: 0.40683743357658386, Acc: 0.5987542867660522\n",
      "Epoch: 57, Loss: 0.3989136517047882, Acc: 0.6029990911483765\n",
      "Epoch: 58, Loss: 0.39124491810798645, Acc: 0.6069830656051636\n",
      "Epoch: 59, Loss: 0.3845899701118469, Acc: 0.6107833385467529\n",
      "Epoch: 60, Loss: 0.3872997462749481, Acc: 0.6143269538879395\n",
      "Epoch: 61, Loss: 0.38070133328437805, Acc: 0.6177787184715271\n",
      "Epoch: 62, Loss: 0.37659427523612976, Acc: 0.6216642260551453\n",
      "Epoch: 63, Loss: 0.37653928995132446, Acc: 0.6246905326843262\n",
      "Epoch: 64, Loss: 0.3795727789402008, Acc: 0.6282498240470886\n",
      "Epoch: 65, Loss: 0.3703117072582245, Acc: 0.6313834190368652\n",
      "Epoch: 66, Loss: 0.36495476961135864, Acc: 0.6344994306564331\n",
      "Epoch: 67, Loss: 0.3660556375980377, Acc: 0.6375252604484558\n",
      "Epoch: 68, Loss: 0.363944411277771, Acc: 0.6407285332679749\n",
      "Epoch: 69, Loss: 0.3586210012435913, Acc: 0.6433991193771362\n",
      "Epoch: 70, Loss: 0.3607374131679535, Acc: 0.6461790204048157\n",
      "Epoch: 71, Loss: 0.348800927400589, Acc: 0.6489377021789551\n",
      "Epoch: 72, Loss: 0.3486844599246979, Acc: 0.6515830755233765\n",
      "Epoch: 73, Loss: 0.34589383006095886, Acc: 0.6543107032775879\n",
      "Epoch: 74, Loss: 0.3456680476665497, Acc: 0.6568500995635986\n",
      "Epoch: 75, Loss: 0.3379938006401062, Acc: 0.6596565842628479\n",
      "Epoch: 76, Loss: 0.34488096833229065, Acc: 0.6618419885635376\n",
      "Epoch: 77, Loss: 0.33567601442337036, Acc: 0.6642699241638184\n",
      "Epoch: 78, Loss: 0.3266378939151764, Acc: 0.666765570640564\n",
      "Epoch: 79, Loss: 0.3280697762966156, Acc: 0.6690926551818848\n",
      "Epoch: 80, Loss: 0.33302679657936096, Acc: 0.6713107824325562\n",
      "Epoch: 81, Loss: 0.32492947578430176, Acc: 0.6735614538192749\n",
      "Epoch: 82, Loss: 0.3225574195384979, Acc: 0.6759622693061829\n",
      "Epoch: 83, Loss: 0.3210201859474182, Acc: 0.678210973739624\n",
      "Epoch: 84, Loss: 0.31651753187179565, Acc: 0.680117130279541\n",
      "Epoch: 85, Loss: 0.3274305462837219, Acc: 0.6821016073226929\n",
      "Epoch: 86, Loss: 0.31117093563079834, Acc: 0.6841703653335571\n",
      "Epoch: 87, Loss: 0.3169248402118683, Acc: 0.6864805817604065\n",
      "Epoch: 88, Loss: 0.31578436493873596, Acc: 0.6880722045898438\n",
      "Epoch: 89, Loss: 0.3067830801010132, Acc: 0.6901092529296875\n",
      "Epoch: 90, Loss: 0.3085419535636902, Acc: 0.6917917132377625\n",
      "Epoch: 91, Loss: 0.3018380105495453, Acc: 0.6938685178756714\n",
      "Epoch: 92, Loss: 0.30435726046562195, Acc: 0.6954992413520813\n",
      "Epoch: 93, Loss: 0.30252042412757874, Acc: 0.6975905299186707\n",
      "Epoch: 94, Loss: 0.3033013939857483, Acc: 0.6997466087341309\n",
      "Epoch: 95, Loss: 0.30271482467651367, Acc: 0.700813889503479\n",
      "Epoch: 96, Loss: 0.29773852229118347, Acc: 0.7027558088302612\n",
      "Epoch: 97, Loss: 0.2959718704223633, Acc: 0.7043687701225281\n",
      "Epoch: 98, Loss: 0.29024738073349, Acc: 0.7061358690261841\n",
      "Epoch: 99, Loss: 0.3050256371498108, Acc: 0.7083812355995178\n",
      "Epoch: 100, Loss: 0.29163625836372375, Acc: 0.7095965147018433\n",
      "Epoch: 101, Loss: 0.28836655616760254, Acc: 0.7110356688499451\n",
      "Epoch: 102, Loss: 0.29361492395401, Acc: 0.7127309441566467\n",
      "Epoch: 103, Loss: 0.28482240438461304, Acc: 0.7142212390899658\n",
      "Epoch: 104, Loss: 0.2844250500202179, Acc: 0.7157554030418396\n",
      "Epoch: 105, Loss: 0.28691866993904114, Acc: 0.7172176837921143\n",
      "Epoch: 106, Loss: 0.28851044178009033, Acc: 0.719075083732605\n",
      "Epoch: 107, Loss: 0.2781471014022827, Acc: 0.7203044891357422\n",
      "Epoch: 108, Loss: 0.27686625719070435, Acc: 0.7220143675804138\n",
      "Epoch: 109, Loss: 0.2791832685470581, Acc: 0.7233405113220215\n",
      "Epoch: 110, Loss: 0.2806818187236786, Acc: 0.7248839735984802\n",
      "Epoch: 111, Loss: 0.27200430631637573, Acc: 0.7261258363723755\n",
      "Epoch: 112, Loss: 0.2758907079696655, Acc: 0.7279918789863586\n",
      "Epoch: 113, Loss: 0.2680881917476654, Acc: 0.7289566993713379\n",
      "Epoch: 114, Loss: 0.27007848024368286, Acc: 0.730412483215332\n",
      "Epoch: 115, Loss: 0.2726173400878906, Acc: 0.7318730354309082\n",
      "Epoch: 116, Loss: 0.2652917206287384, Acc: 0.7334933280944824\n",
      "Epoch: 117, Loss: 0.2630133330821991, Acc: 0.7344820499420166\n",
      "Epoch: 118, Loss: 0.2549399733543396, Acc: 0.7358731031417847\n",
      "Epoch: 119, Loss: 0.2639755606651306, Acc: 0.7371630668640137\n",
      "Epoch: 120, Loss: 0.26214420795440674, Acc: 0.7386013269424438\n",
      "Epoch: 121, Loss: 0.266267865896225, Acc: 0.740175724029541\n",
      "Epoch: 122, Loss: 0.2557167112827301, Acc: 0.7417418360710144\n",
      "Epoch: 123, Loss: 0.2540959119796753, Acc: 0.742411196231842\n",
      "Epoch: 124, Loss: 0.2627256512641907, Acc: 0.7438841462135315\n",
      "Epoch: 125, Loss: 0.2557404637336731, Acc: 0.7455641031265259\n",
      "Epoch: 126, Loss: 0.2615465223789215, Acc: 0.746237576007843\n",
      "Epoch: 127, Loss: 0.2544543147087097, Acc: 0.7474620342254639\n",
      "Epoch: 128, Loss: 0.2533760964870453, Acc: 0.7486333250999451\n",
      "Epoch: 129, Loss: 0.24751414358615875, Acc: 0.7498587369918823\n",
      "Epoch: 130, Loss: 0.2499033510684967, Acc: 0.7515209913253784\n",
      "Epoch: 131, Loss: 0.25392946600914, Acc: 0.7523970603942871\n",
      "Epoch: 132, Loss: 0.24535192549228668, Acc: 0.7534983158111572\n",
      "Epoch: 133, Loss: 0.2473326027393341, Acc: 0.7548041343688965\n",
      "Epoch: 134, Loss: 0.2471681535243988, Acc: 0.7564735412597656\n",
      "Epoch: 135, Loss: 0.24297362565994263, Acc: 0.7571214437484741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 136, Loss: 0.2509969174861908, Acc: 0.7584960460662842\n",
      "Epoch: 137, Loss: 0.23826153576374054, Acc: 0.7593212723731995\n",
      "Epoch: 138, Loss: 0.2415366768836975, Acc: 0.7604374289512634\n",
      "Epoch: 139, Loss: 0.2435564547777176, Acc: 0.7618557214736938\n",
      "Epoch: 140, Loss: 0.24261361360549927, Acc: 0.762946367263794\n",
      "Epoch: 141, Loss: 0.23557449877262115, Acc: 0.7640283107757568\n",
      "Epoch: 142, Loss: 0.2351302206516266, Acc: 0.7651127576828003\n",
      "Epoch: 143, Loss: 0.23241187632083893, Acc: 0.7668710350990295\n",
      "Epoch: 144, Loss: 0.24052685499191284, Acc: 0.7674344182014465\n",
      "Epoch: 145, Loss: 0.22950895130634308, Acc: 0.7686755061149597\n",
      "Epoch: 146, Loss: 0.23090487718582153, Acc: 0.7698234915733337\n",
      "Epoch: 147, Loss: 0.22773055732250214, Acc: 0.7707839012145996\n",
      "Epoch: 148, Loss: 0.22364528477191925, Acc: 0.7718387842178345\n",
      "Epoch: 149, Loss: 0.22400619089603424, Acc: 0.772962212562561\n",
      "Epoch: 150, Loss: 0.2256188839673996, Acc: 0.774070143699646\n",
      "Epoch: 151, Loss: 0.22358618676662445, Acc: 0.7752547264099121\n",
      "Epoch: 152, Loss: 0.22732284665107727, Acc: 0.7764843702316284\n",
      "Epoch: 153, Loss: 0.22466178238391876, Acc: 0.7773497104644775\n",
      "Epoch: 154, Loss: 0.21787291765213013, Acc: 0.7783215045928955\n",
      "Epoch: 155, Loss: 0.22312398254871368, Acc: 0.779833197593689\n",
      "Epoch: 156, Loss: 0.22194179892539978, Acc: 0.7804642915725708\n",
      "Epoch: 157, Loss: 0.2221464067697525, Acc: 0.7816252708435059\n",
      "Epoch: 158, Loss: 0.22107040882110596, Acc: 0.7829848527908325\n",
      "Epoch: 159, Loss: 0.21401908993721008, Acc: 0.7838280200958252\n",
      "Epoch: 160, Loss: 0.2114264816045761, Acc: 0.7846735715866089\n",
      "Epoch: 161, Loss: 0.22004863619804382, Acc: 0.7858044505119324\n",
      "Epoch: 162, Loss: 0.21094195544719696, Acc: 0.7867428064346313\n",
      "Epoch: 163, Loss: 0.21277762949466705, Acc: 0.7879080176353455\n",
      "Epoch: 164, Loss: 0.21628031134605408, Acc: 0.7887385487556458\n",
      "Epoch: 165, Loss: 0.2090434432029724, Acc: 0.789812445640564\n",
      "Epoch: 166, Loss: 0.21597054600715637, Acc: 0.7910398244857788\n",
      "Epoch: 167, Loss: 0.2021394819021225, Acc: 0.7918657064437866\n",
      "Epoch: 168, Loss: 0.2096516340970993, Acc: 0.793175220489502\n",
      "Epoch: 169, Loss: 0.20928655564785004, Acc: 0.794014573097229\n",
      "Epoch: 170, Loss: 0.20615388453006744, Acc: 0.7950569987297058\n",
      "Epoch: 171, Loss: 0.20387586951255798, Acc: 0.7959550619125366\n",
      "Epoch: 172, Loss: 0.20859107375144958, Acc: 0.7972714900970459\n",
      "Epoch: 173, Loss: 0.20082469284534454, Acc: 0.7978947758674622\n",
      "Epoch: 174, Loss: 0.20490097999572754, Acc: 0.7988681793212891\n",
      "Epoch: 175, Loss: 0.20384515821933746, Acc: 0.8001503944396973\n",
      "Epoch: 176, Loss: 0.19615048170089722, Acc: 0.800855815410614\n",
      "Epoch: 177, Loss: 0.19676366448402405, Acc: 0.8018787503242493\n",
      "Epoch: 178, Loss: 0.1944233775138855, Acc: 0.8027619123458862\n",
      "Epoch: 179, Loss: 0.1977279782295227, Acc: 0.8037780523300171\n",
      "Epoch: 180, Loss: 0.1981428861618042, Acc: 0.804777979850769\n",
      "Epoch: 181, Loss: 0.19574983417987823, Acc: 0.806130051612854\n",
      "Epoch: 182, Loss: 0.19958153367042542, Acc: 0.8069645762443542\n",
      "Epoch: 183, Loss: 0.19504082202911377, Acc: 0.8074835538864136\n",
      "Epoch: 184, Loss: 0.19353297352790833, Acc: 0.8087037205696106\n",
      "Epoch: 185, Loss: 0.18649275600910187, Acc: 0.8095908164978027\n",
      "Epoch: 186, Loss: 0.18700730800628662, Acc: 0.8104950785636902\n",
      "Epoch: 187, Loss: 0.1894587129354477, Acc: 0.8111904859542847\n",
      "Epoch: 188, Loss: 0.18997012078762054, Acc: 0.8121942281723022\n",
      "Epoch: 189, Loss: 0.1871279776096344, Acc: 0.8132928013801575\n",
      "Epoch: 190, Loss: 0.18476562201976776, Acc: 0.814310610294342\n",
      "Epoch: 191, Loss: 0.1867361217737198, Acc: 0.8149474859237671\n",
      "Epoch: 192, Loss: 0.18417847156524658, Acc: 0.815826952457428\n",
      "Epoch: 193, Loss: 0.18223638832569122, Acc: 0.816991925239563\n",
      "Epoch: 194, Loss: 0.18350452184677124, Acc: 0.8175989389419556\n",
      "Epoch: 195, Loss: 0.1885284185409546, Acc: 0.8188480734825134\n",
      "Epoch: 196, Loss: 0.1788461059331894, Acc: 0.8201190233230591\n",
      "Epoch: 197, Loss: 0.17617279291152954, Acc: 0.8201869130134583\n",
      "Epoch: 198, Loss: 0.18093101680278778, Acc: 0.8211787343025208\n",
      "Epoch: 199, Loss: 0.18273815512657166, Acc: 0.8223238587379456\n",
      "Epoch: 200, Loss: 0.17669075727462769, Acc: 0.8228300213813782\n",
      "Epoch: 201, Loss: 0.1770959496498108, Acc: 0.8237406611442566\n",
      "Epoch: 202, Loss: 0.17506752908229828, Acc: 0.8256022930145264\n",
      "Epoch: 203, Loss: 0.17110151052474976, Acc: 0.8252213597297668\n",
      "Epoch: 204, Loss: 0.17535780370235443, Acc: 0.8262789845466614\n",
      "Epoch: 205, Loss: 0.1712576001882553, Acc: 0.8270713090896606\n",
      "Epoch: 206, Loss: 0.1685577630996704, Acc: 0.8280774354934692\n",
      "Epoch: 207, Loss: 0.1733182966709137, Acc: 0.8284785747528076\n",
      "Epoch: 208, Loss: 0.1765197217464447, Acc: 0.8302780389785767\n",
      "Epoch: 209, Loss: 0.17622411251068115, Acc: 0.8303624391555786\n",
      "Epoch: 210, Loss: 0.16803115606307983, Acc: 0.8309308290481567\n",
      "Epoch: 211, Loss: 0.16585586965084076, Acc: 0.8318465948104858\n",
      "Epoch: 212, Loss: 0.17067168653011322, Acc: 0.8325344324111938\n",
      "Epoch: 213, Loss: 0.16701628267765045, Acc: 0.8333876729011536\n",
      "Epoch: 214, Loss: 0.17330586910247803, Acc: 0.8343088626861572\n",
      "Epoch: 215, Loss: 0.16188058257102966, Acc: 0.8350460529327393\n",
      "Epoch: 216, Loss: 0.1648881435394287, Acc: 0.8361430168151855\n",
      "Epoch: 217, Loss: 0.16285207867622375, Acc: 0.836673378944397\n",
      "Epoch: 218, Loss: 0.1588430553674698, Acc: 0.8374214172363281\n",
      "Epoch: 219, Loss: 0.1615973711013794, Acc: 0.8384764194488525\n",
      "Epoch: 220, Loss: 0.16249284148216248, Acc: 0.839799165725708\n",
      "Epoch: 221, Loss: 0.16378790140151978, Acc: 0.8397730588912964\n",
      "Epoch: 222, Loss: 0.15666867792606354, Acc: 0.8404077291488647\n",
      "Epoch: 223, Loss: 0.15993519127368927, Acc: 0.8416820764541626\n",
      "Epoch: 224, Loss: 0.15805107355117798, Acc: 0.8420398235321045\n",
      "Epoch: 225, Loss: 0.15598943829536438, Acc: 0.843359112739563\n",
      "Epoch: 226, Loss: 0.15546952188014984, Acc: 0.8435776233673096\n",
      "Epoch: 227, Loss: 0.15996555984020233, Acc: 0.8446633219718933\n",
      "Epoch: 228, Loss: 0.1552935689687729, Acc: 0.8453322649002075\n",
      "Epoch: 229, Loss: 0.15285131335258484, Acc: 0.8460460901260376\n",
      "Epoch: 230, Loss: 0.15094192326068878, Acc: 0.846406877040863\n",
      "Epoch: 231, Loss: 0.1516231894493103, Acc: 0.8473972082138062\n",
      "Epoch: 232, Loss: 0.15572546422481537, Acc: 0.8481596112251282\n",
      "Epoch: 233, Loss: 0.15607045590877533, Acc: 0.8486530780792236\n",
      "Epoch: 234, Loss: 0.14996273815631866, Acc: 0.8497746586799622\n",
      "Epoch: 235, Loss: 0.15053264796733856, Acc: 0.8501660823822021\n",
      "Epoch: 236, Loss: 0.1455049365758896, Acc: 0.8506858348846436\n",
      "Epoch: 237, Loss: 0.1481015980243683, Acc: 0.8517875075340271\n",
      "Epoch: 238, Loss: 0.148410364985466, Acc: 0.8522855043411255\n",
      "Epoch: 239, Loss: 0.14515455067157745, Acc: 0.853052020072937\n",
      "Epoch: 240, Loss: 0.14863918721675873, Acc: 0.8536642789840698\n",
      "Epoch: 241, Loss: 0.14381468296051025, Acc: 0.8540736436843872\n",
      "Epoch: 242, Loss: 0.14676329493522644, Acc: 0.8547112345695496\n",
      "Epoch: 243, Loss: 0.14312413334846497, Acc: 0.8553265929222107\n",
      "Epoch: 244, Loss: 0.14459756016731262, Acc: 0.8562998175621033\n",
      "Epoch: 245, Loss: 0.14198361337184906, Acc: 0.8568503856658936\n",
      "Epoch: 246, Loss: 0.14563150703907013, Acc: 0.8576419353485107\n",
      "Epoch: 247, Loss: 0.14176325500011444, Acc: 0.8584836721420288\n",
      "Epoch: 248, Loss: 0.14026020467281342, Acc: 0.8586905002593994\n",
      "Epoch: 249, Loss: 0.1393839418888092, Acc: 0.8593297004699707\n",
      "Epoch: 250, Loss: 0.14373965561389923, Acc: 0.8604288101196289\n",
      "Epoch: 251, Loss: 0.13831037282943726, Acc: 0.860710084438324\n",
      "Epoch: 252, Loss: 0.1378345787525177, Acc: 0.8613026142120361\n",
      "Epoch: 253, Loss: 0.1400107741355896, Acc: 0.8620797991752625\n",
      "Epoch: 254, Loss: 0.1361921727657318, Acc: 0.8623655438423157\n",
      "Epoch: 255, Loss: 0.13825584948062897, Acc: 0.8631895184516907\n",
      "Epoch: 256, Loss: 0.13685239851474762, Acc: 0.8638059496879578\n",
      "Epoch: 257, Loss: 0.13602083921432495, Acc: 0.8645358085632324\n",
      "Epoch: 258, Loss: 0.1331501305103302, Acc: 0.8649699687957764\n",
      "Epoch: 259, Loss: 0.1377745419740677, Acc: 0.8654186725616455\n",
      "Epoch: 260, Loss: 0.13153503835201263, Acc: 0.8661512732505798\n",
      "Epoch: 261, Loss: 0.13566435873508453, Acc: 0.8665569424629211\n",
      "Epoch: 262, Loss: 0.13153119385242462, Acc: 0.867232084274292\n",
      "Epoch: 263, Loss: 0.13494841754436493, Acc: 0.8676704168319702\n",
      "Epoch: 264, Loss: 0.128861203789711, Acc: 0.8684614300727844\n",
      "Epoch: 265, Loss: 0.13179486989974976, Acc: 0.8693035244941711\n",
      "Epoch: 266, Loss: 0.13632315397262573, Acc: 0.8697692155838013\n",
      "Epoch: 267, Loss: 0.13232126832008362, Acc: 0.8699413537979126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 268, Loss: 0.13027983903884888, Acc: 0.870894730091095\n",
      "Epoch: 269, Loss: 0.12864431738853455, Acc: 0.8712366819381714\n",
      "Epoch: 270, Loss: 0.13162153959274292, Acc: 0.8717479705810547\n",
      "Epoch: 271, Loss: 0.127543106675148, Acc: 0.8721824288368225\n",
      "Epoch: 272, Loss: 0.12682588398456573, Acc: 0.8731070160865784\n",
      "Epoch: 273, Loss: 0.12718833982944489, Acc: 0.8736627101898193\n",
      "Epoch: 274, Loss: 0.13120177388191223, Acc: 0.8738045692443848\n",
      "Epoch: 275, Loss: 0.1227780431509018, Acc: 0.8743082284927368\n",
      "Epoch: 276, Loss: 0.12584787607192993, Acc: 0.8752442598342896\n",
      "Epoch: 277, Loss: 0.12310853600502014, Acc: 0.8754944205284119\n",
      "Epoch: 278, Loss: 0.12296003103256226, Acc: 0.876136064529419\n",
      "Epoch: 279, Loss: 0.12341451644897461, Acc: 0.8762497305870056\n",
      "Epoch: 280, Loss: 0.12218321114778519, Acc: 0.8767693042755127\n",
      "Epoch: 281, Loss: 0.12139838188886642, Acc: 0.8771736025810242\n",
      "Epoch: 282, Loss: 0.1250283569097519, Acc: 0.8780462145805359\n",
      "Epoch: 283, Loss: 0.11974052339792252, Acc: 0.8783363103866577\n",
      "Epoch: 284, Loss: 0.12246142327785492, Acc: 0.8787389397621155\n",
      "Epoch: 285, Loss: 0.1251220852136612, Acc: 0.8794128894805908\n",
      "Epoch: 286, Loss: 0.12221908569335938, Acc: 0.8795356154441833\n",
      "Epoch: 287, Loss: 0.11895956099033356, Acc: 0.880134642124176\n",
      "Epoch: 288, Loss: 0.11813431233167648, Acc: 0.88042151927948\n",
      "Epoch: 289, Loss: 0.1204359233379364, Acc: 0.8809058666229248\n",
      "Epoch: 290, Loss: 0.11675813794136047, Acc: 0.8816973567008972\n",
      "Epoch: 291, Loss: 0.12198896706104279, Acc: 0.8823292255401611\n",
      "Epoch: 292, Loss: 0.12414944171905518, Acc: 0.8822722434997559\n",
      "Epoch: 293, Loss: 0.1179056391119957, Acc: 0.8826253414154053\n",
      "Epoch: 294, Loss: 0.11986963450908661, Acc: 0.8834716081619263\n",
      "Epoch: 295, Loss: 0.11304621398448944, Acc: 0.8844899535179138\n",
      "Epoch: 296, Loss: 0.11523662507534027, Acc: 0.8842481374740601\n",
      "Epoch: 297, Loss: 0.11554457992315292, Acc: 0.884647786617279\n",
      "Epoch: 298, Loss: 0.11592080444097519, Acc: 0.8852492570877075\n",
      "Epoch: 299, Loss: 0.11401908844709396, Acc: 0.885481059551239\n",
      "Epoch: 300, Loss: 0.11245127767324448, Acc: 0.8859184980392456\n",
      "Epoch: 301, Loss: 0.11673842370510101, Acc: 0.8863956928253174\n",
      "Epoch: 302, Loss: 0.11920306086540222, Acc: 0.8865822553634644\n",
      "Epoch: 303, Loss: 0.11226743459701538, Acc: 0.8871012926101685\n",
      "Epoch: 304, Loss: 0.11348432302474976, Acc: 0.8875190615653992\n",
      "Epoch: 305, Loss: 0.11013981699943542, Acc: 0.8879104256629944\n",
      "Epoch: 306, Loss: 0.1089758649468422, Acc: 0.8883734941482544\n",
      "Epoch: 307, Loss: 0.11444695293903351, Acc: 0.8891883492469788\n",
      "Epoch: 308, Loss: 0.11691513657569885, Acc: 0.8889578580856323\n",
      "Epoch: 309, Loss: 0.1116938367486, Acc: 0.8893520832061768\n",
      "Epoch: 310, Loss: 0.11069280654191971, Acc: 0.8899078369140625\n",
      "Epoch: 311, Loss: 0.11637470126152039, Acc: 0.890251636505127\n",
      "Epoch: 312, Loss: 0.10631959140300751, Acc: 0.8903601765632629\n",
      "Epoch: 313, Loss: 0.11044160276651382, Acc: 0.8909415006637573\n",
      "Epoch: 314, Loss: 0.10629076510667801, Acc: 0.891322135925293\n",
      "Epoch: 315, Loss: 0.10667087137699127, Acc: 0.8916416764259338\n",
      "Epoch: 316, Loss: 0.10845351964235306, Acc: 0.8918795585632324\n",
      "Epoch: 317, Loss: 0.10881240665912628, Acc: 0.8921486735343933\n",
      "Epoch: 318, Loss: 0.10556584596633911, Acc: 0.8925808072090149\n",
      "Epoch: 319, Loss: 0.10426370799541473, Acc: 0.8931753039360046\n",
      "Epoch: 320, Loss: 0.10597488284111023, Acc: 0.8933982849121094\n",
      "Epoch: 321, Loss: 0.1103297621011734, Acc: 0.8937479257583618\n",
      "Epoch: 322, Loss: 0.10756802558898926, Acc: 0.8941503763198853\n",
      "Epoch: 323, Loss: 0.10782825201749802, Acc: 0.8951401114463806\n",
      "Epoch: 324, Loss: 0.10627526044845581, Acc: 0.8948795795440674\n",
      "Epoch: 325, Loss: 0.10397166013717651, Acc: 0.8950705528259277\n",
      "Epoch: 326, Loss: 0.10876820981502533, Acc: 0.8954777717590332\n",
      "Epoch: 327, Loss: 0.104404017329216, Acc: 0.8957419991493225\n",
      "Epoch: 328, Loss: 0.1026654839515686, Acc: 0.8964491486549377\n",
      "Epoch: 329, Loss: 0.10461108386516571, Acc: 0.8971647620201111\n",
      "Epoch: 330, Loss: 0.10726878046989441, Acc: 0.8970761299133301\n",
      "Epoch: 331, Loss: 0.10117630660533905, Acc: 0.8974834680557251\n",
      "Epoch: 332, Loss: 0.1016724482178688, Acc: 0.8975381851196289\n",
      "Epoch: 333, Loss: 0.1030321717262268, Acc: 0.8978509902954102\n",
      "Epoch: 334, Loss: 0.10085161030292511, Acc: 0.8983445763587952\n",
      "Epoch: 335, Loss: 0.10363030433654785, Acc: 0.8985438346862793\n",
      "Epoch: 336, Loss: 0.10511179268360138, Acc: 0.8991151452064514\n",
      "Epoch: 337, Loss: 0.10300134867429733, Acc: 0.89960777759552\n",
      "Epoch: 338, Loss: 0.10530343651771545, Acc: 0.8997489809989929\n",
      "Epoch: 339, Loss: 0.10118449479341507, Acc: 0.899937093257904\n",
      "Epoch: 340, Loss: 0.10038819164037704, Acc: 0.9000422954559326\n",
      "Epoch: 341, Loss: 0.10023833066225052, Acc: 0.9005120396614075\n",
      "Epoch: 342, Loss: 0.0999540239572525, Acc: 0.9009304642677307\n",
      "Epoch: 343, Loss: 0.09653040021657944, Acc: 0.900863766670227\n",
      "Epoch: 344, Loss: 0.09715954959392548, Acc: 0.9012351036071777\n",
      "Epoch: 345, Loss: 0.09707246720790863, Acc: 0.9020138382911682\n",
      "Epoch: 346, Loss: 0.09582609683275223, Acc: 0.9019737839698792\n",
      "Epoch: 347, Loss: 0.10412348806858063, Acc: 0.9020175933837891\n",
      "Epoch: 348, Loss: 0.1018005758523941, Acc: 0.9028981924057007\n",
      "Epoch: 349, Loss: 0.09840875118970871, Acc: 0.9032878875732422\n",
      "Epoch: 350, Loss: 0.09714026749134064, Acc: 0.9030439853668213\n",
      "Epoch: 351, Loss: 0.09755371510982513, Acc: 0.9033423662185669\n",
      "Epoch: 352, Loss: 0.09461181610822678, Acc: 0.9038737416267395\n",
      "Epoch: 353, Loss: 0.09913092851638794, Acc: 0.9040035605430603\n",
      "Epoch: 354, Loss: 0.10609745234251022, Acc: 0.9041978716850281\n",
      "Epoch: 355, Loss: 0.09420570731163025, Acc: 0.9044798612594604\n",
      "Epoch: 356, Loss: 0.09945288300514221, Acc: 0.9048669338226318\n",
      "Epoch: 357, Loss: 0.09539712965488434, Acc: 0.9051750302314758\n",
      "Epoch: 358, Loss: 0.09235066175460815, Acc: 0.905147135257721\n",
      "Epoch: 359, Loss: 0.09266551584005356, Acc: 0.9053963422775269\n",
      "Epoch: 360, Loss: 0.09633245319128036, Acc: 0.9057599902153015\n",
      "Epoch: 361, Loss: 0.09454306960105896, Acc: 0.9061686396598816\n",
      "Epoch: 362, Loss: 0.09384037554264069, Acc: 0.9061656594276428\n",
      "Epoch: 363, Loss: 0.09747059643268585, Acc: 0.9064326286315918\n",
      "Epoch: 364, Loss: 0.09186854213476181, Acc: 0.9070760607719421\n",
      "Epoch: 365, Loss: 0.09031925350427628, Acc: 0.907048225402832\n",
      "Epoch: 366, Loss: 0.09398717433214188, Acc: 0.9071714878082275\n",
      "Epoch: 367, Loss: 0.09448248893022537, Acc: 0.9075099229812622\n",
      "Epoch: 368, Loss: 0.09418842941522598, Acc: 0.907735288143158\n",
      "Epoch: 369, Loss: 0.09001155942678452, Acc: 0.9079511165618896\n",
      "Epoch: 370, Loss: 0.09082023054361343, Acc: 0.9081098437309265\n",
      "Epoch: 371, Loss: 0.09461093693971634, Acc: 0.9085001349449158\n",
      "Epoch: 372, Loss: 0.09364842623472214, Acc: 0.908626914024353\n",
      "Epoch: 373, Loss: 0.09178493171930313, Acc: 0.9092235565185547\n",
      "Epoch: 374, Loss: 0.08808756619691849, Acc: 0.9093074202537537\n",
      "Epoch: 375, Loss: 0.09403121471405029, Acc: 0.9095101356506348\n",
      "Epoch: 376, Loss: 0.09433206915855408, Acc: 0.9096595048904419\n",
      "Epoch: 377, Loss: 0.09182138741016388, Acc: 0.9100422263145447\n",
      "Epoch: 378, Loss: 0.09224934875965118, Acc: 0.9106109142303467\n",
      "Epoch: 379, Loss: 0.08720187097787857, Acc: 0.9103757739067078\n",
      "Epoch: 380, Loss: 0.09166327863931656, Acc: 0.9108571410179138\n",
      "Epoch: 381, Loss: 0.08878491073846817, Acc: 0.9110797643661499\n",
      "Epoch: 382, Loss: 0.08787760138511658, Acc: 0.911212146282196\n",
      "Epoch: 383, Loss: 0.09493421018123627, Acc: 0.9118791222572327\n",
      "Epoch: 384, Loss: 0.08890563249588013, Acc: 0.9116283059120178\n",
      "Epoch: 385, Loss: 0.08739462494850159, Acc: 0.9117324352264404\n",
      "Epoch: 386, Loss: 0.09130662679672241, Acc: 0.9119561910629272\n",
      "Epoch: 387, Loss: 0.08936860412359238, Acc: 0.912520706653595\n",
      "Epoch: 388, Loss: 0.08653312921524048, Acc: 0.9124475121498108\n",
      "Epoch: 389, Loss: 0.08442205935716629, Acc: 0.912998616695404\n",
      "Epoch: 390, Loss: 0.08567997813224792, Acc: 0.9129692316055298\n",
      "Epoch: 391, Loss: 0.08780186623334885, Acc: 0.913224458694458\n",
      "Epoch: 392, Loss: 0.08427150547504425, Acc: 0.9136372208595276\n",
      "Epoch: 393, Loss: 0.08851906657218933, Acc: 0.9135459065437317\n",
      "Epoch: 394, Loss: 0.08834514021873474, Acc: 0.9143041372299194\n",
      "Epoch: 395, Loss: 0.08894900977611542, Acc: 0.9143099784851074\n",
      "Epoch: 396, Loss: 0.08670622855424881, Acc: 0.9140324592590332\n",
      "Epoch: 397, Loss: 0.08783705532550812, Acc: 0.9147249460220337\n",
      "Epoch: 398, Loss: 0.08746694028377533, Acc: 0.9151197075843811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 399, Loss: 0.0863490030169487, Acc: 0.9148293733596802\n",
      "Epoch: 400, Loss: 0.09210142493247986, Acc: 0.9150031208992004\n",
      "Epoch: 401, Loss: 0.08320498466491699, Acc: 0.9154606461524963\n",
      "Epoch: 402, Loss: 0.08460366725921631, Acc: 0.9154173135757446\n",
      "Epoch: 403, Loss: 0.08457329869270325, Acc: 0.9160114526748657\n",
      "Epoch: 404, Loss: 0.08369464427232742, Acc: 0.9157159328460693\n",
      "Epoch: 405, Loss: 0.08228704333305359, Acc: 0.9160581231117249\n",
      "Epoch: 406, Loss: 0.08453172445297241, Acc: 0.9162968397140503\n",
      "Epoch: 407, Loss: 0.08315912634134293, Acc: 0.916325569152832\n",
      "Epoch: 408, Loss: 0.0867934599518776, Acc: 0.9176750183105469\n",
      "Epoch: 409, Loss: 0.08338426798582077, Acc: 0.9167978763580322\n",
      "Epoch: 410, Loss: 0.08731330931186676, Acc: 0.9174618721008301\n",
      "Epoch: 411, Loss: 0.08346523344516754, Acc: 0.91765958070755\n",
      "Epoch: 412, Loss: 0.07982691377401352, Acc: 0.9174597859382629\n",
      "Epoch: 413, Loss: 0.08418747037649155, Acc: 0.9180116057395935\n",
      "Epoch: 414, Loss: 0.08183805644512177, Acc: 0.9182501435279846\n",
      "Epoch: 415, Loss: 0.08052578568458557, Acc: 0.9184887409210205\n",
      "Epoch: 416, Loss: 0.08075518906116486, Acc: 0.9182705879211426\n",
      "Epoch: 417, Loss: 0.08125726133584976, Acc: 0.9187716245651245\n",
      "Epoch: 418, Loss: 0.08468373864889145, Acc: 0.9189444780349731\n",
      "Epoch: 419, Loss: 0.08653292059898376, Acc: 0.9192553162574768\n",
      "Epoch: 420, Loss: 0.08025949448347092, Acc: 0.918915867805481\n",
      "Epoch: 421, Loss: 0.08245307952165604, Acc: 0.9190018177032471\n",
      "Epoch: 422, Loss: 0.08225449174642563, Acc: 0.9192206859588623\n",
      "Epoch: 423, Loss: 0.08357882499694824, Acc: 0.9193862676620483\n",
      "Epoch: 424, Loss: 0.08116354048252106, Acc: 0.9195823073387146\n",
      "Epoch: 425, Loss: 0.08188153803348541, Acc: 0.9200150370597839\n",
      "Epoch: 426, Loss: 0.08193252980709076, Acc: 0.9202250242233276\n",
      "Epoch: 427, Loss: 0.07787490636110306, Acc: 0.9203093647956848\n",
      "Epoch: 428, Loss: 0.07791457325220108, Acc: 0.9202938079833984\n",
      "Epoch: 429, Loss: 0.07954563200473785, Acc: 0.9205357432365417\n",
      "Epoch: 430, Loss: 0.07979622483253479, Acc: 0.9209532141685486\n",
      "Epoch: 431, Loss: 0.07856124639511108, Acc: 0.9206759929656982\n",
      "Epoch: 432, Loss: 0.08187206089496613, Acc: 0.921112060546875\n",
      "Epoch: 433, Loss: 0.07893183827400208, Acc: 0.9215781688690186\n",
      "Epoch: 434, Loss: 0.07698798179626465, Acc: 0.9214916825294495\n",
      "Epoch: 435, Loss: 0.07838615030050278, Acc: 0.9219104051589966\n",
      "Epoch: 436, Loss: 0.07909490913152695, Acc: 0.9216300845146179\n",
      "Epoch: 437, Loss: 0.08250382542610168, Acc: 0.9218848943710327\n",
      "Epoch: 438, Loss: 0.0778677687048912, Acc: 0.9218623042106628\n",
      "Epoch: 439, Loss: 0.07583960890769958, Acc: 0.9220098853111267\n",
      "Epoch: 440, Loss: 0.08037716895341873, Acc: 0.9228655099868774\n",
      "Epoch: 441, Loss: 0.0770251452922821, Acc: 0.922682523727417\n",
      "Epoch: 442, Loss: 0.07836776226758957, Acc: 0.9227162003517151\n",
      "Epoch: 443, Loss: 0.08092927932739258, Acc: 0.9233508110046387\n",
      "Epoch: 444, Loss: 0.0784548968076706, Acc: 0.9229586124420166\n",
      "Epoch: 445, Loss: 0.07852672785520554, Acc: 0.9230207204818726\n",
      "Epoch: 446, Loss: 0.07827267795801163, Acc: 0.9231100082397461\n",
      "Epoch: 447, Loss: 0.0764753520488739, Acc: 0.9236725568771362\n",
      "Epoch: 448, Loss: 0.07737719267606735, Acc: 0.923449695110321\n",
      "Epoch: 449, Loss: 0.07651039212942123, Acc: 0.9237957000732422\n",
      "Epoch: 450, Loss: 0.0788855105638504, Acc: 0.923764169216156\n",
      "Epoch: 451, Loss: 0.07587652653455734, Acc: 0.9240176677703857\n",
      "Epoch: 452, Loss: 0.07459225505590439, Acc: 0.9248661994934082\n",
      "Epoch: 453, Loss: 0.0784003809094429, Acc: 0.924485445022583\n",
      "Epoch: 454, Loss: 0.07512776553630829, Acc: 0.9246035218238831\n",
      "Epoch: 455, Loss: 0.07579521089792252, Acc: 0.9249897599220276\n",
      "Epoch: 456, Loss: 0.07898592948913574, Acc: 0.9248647093772888\n",
      "Epoch: 457, Loss: 0.07700265944004059, Acc: 0.9249719381332397\n",
      "Epoch: 458, Loss: 0.072208933532238, Acc: 0.9249681830406189\n",
      "Epoch: 459, Loss: 0.07581409811973572, Acc: 0.9253513813018799\n",
      "Epoch: 460, Loss: 0.07361482828855515, Acc: 0.9253772497177124\n",
      "Epoch: 461, Loss: 0.07109936326742172, Acc: 0.9252810478210449\n",
      "Epoch: 462, Loss: 0.07616658508777618, Acc: 0.9255711436271667\n",
      "Epoch: 463, Loss: 0.07365354150533676, Acc: 0.9257314205169678\n",
      "Epoch: 464, Loss: 0.07276874780654907, Acc: 0.9259704351425171\n",
      "Epoch: 465, Loss: 0.07273658365011215, Acc: 0.9260615110397339\n",
      "Epoch: 466, Loss: 0.07235415279865265, Acc: 0.926036536693573\n",
      "Epoch: 467, Loss: 0.07535851746797562, Acc: 0.9270309209823608\n",
      "Epoch: 468, Loss: 0.07382132858037949, Acc: 0.9263417720794678\n",
      "Epoch: 469, Loss: 0.07185298204421997, Acc: 0.9265937209129333\n",
      "Epoch: 470, Loss: 0.0735650509595871, Acc: 0.9266820549964905\n",
      "Epoch: 471, Loss: 0.07440587878227234, Acc: 0.9268312454223633\n",
      "Epoch: 472, Loss: 0.0701499953866005, Acc: 0.926882803440094\n",
      "Epoch: 473, Loss: 0.07334651798009872, Acc: 0.9271705746650696\n",
      "Epoch: 474, Loss: 0.07513079047203064, Acc: 0.9273192882537842\n",
      "Epoch: 475, Loss: 0.07346632331609726, Acc: 0.9278468489646912\n",
      "Epoch: 476, Loss: 0.07103875279426575, Acc: 0.9275001287460327\n",
      "Epoch: 477, Loss: 0.07609500735998154, Acc: 0.9277176260948181\n",
      "Epoch: 478, Loss: 0.07269435375928879, Acc: 0.9279899001121521\n",
      "Epoch: 479, Loss: 0.07657930254936218, Acc: 0.9284003973007202\n",
      "Epoch: 480, Loss: 0.07000230252742767, Acc: 0.9279762506484985\n",
      "Epoch: 481, Loss: 0.07162856310606003, Acc: 0.9281512498855591\n",
      "Epoch: 482, Loss: 0.07502847909927368, Acc: 0.9281764626502991\n",
      "Epoch: 483, Loss: 0.07327822595834732, Acc: 0.9284781217575073\n",
      "Epoch: 484, Loss: 0.07834280282258987, Acc: 0.9288896322250366\n",
      "Epoch: 485, Loss: 0.07067820429801941, Acc: 0.9285753965377808\n",
      "Epoch: 486, Loss: 0.07058636844158173, Acc: 0.9289929866790771\n",
      "Epoch: 487, Loss: 0.07085136324167252, Acc: 0.9291698932647705\n",
      "Epoch: 488, Loss: 0.07287942618131638, Acc: 0.9289229512214661\n",
      "Epoch: 489, Loss: 0.07036811858415604, Acc: 0.9293035268783569\n",
      "Epoch: 490, Loss: 0.06804005056619644, Acc: 0.9292159080505371\n",
      "Epoch: 491, Loss: 0.07156427949666977, Acc: 0.929347038269043\n",
      "Epoch: 492, Loss: 0.06902723759412766, Acc: 0.9295440912246704\n",
      "Epoch: 493, Loss: 0.07371000945568085, Acc: 0.9297922253608704\n",
      "Epoch: 494, Loss: 0.06969581544399261, Acc: 0.9297176003456116\n",
      "Epoch: 495, Loss: 0.07204139977693558, Acc: 0.930316686630249\n",
      "Epoch: 496, Loss: 0.067594975233078, Acc: 0.9299606084823608\n",
      "Epoch: 497, Loss: 0.0727517306804657, Acc: 0.9302278757095337\n",
      "Epoch: 498, Loss: 0.0672835037112236, Acc: 0.9303187131881714\n",
      "Epoch: 499, Loss: 0.06999363005161285, Acc: 0.930329442024231\n",
      "Epoch: 500, Loss: 0.06712430715560913, Acc: 0.9305131435394287\n",
      "Epoch: 501, Loss: 0.06806480139493942, Acc: 0.9305974245071411\n",
      "Epoch: 502, Loss: 0.06864595413208008, Acc: 0.9311832785606384\n",
      "Epoch: 503, Loss: 0.07067026197910309, Acc: 0.9308549165725708\n",
      "Epoch: 504, Loss: 0.06692767143249512, Acc: 0.9311298131942749\n",
      "Epoch: 505, Loss: 0.0693432092666626, Acc: 0.9311825633049011\n",
      "Epoch: 506, Loss: 0.06886716932058334, Acc: 0.9310261011123657\n",
      "Epoch: 507, Loss: 0.07124599069356918, Acc: 0.9311851263046265\n",
      "Epoch: 508, Loss: 0.07095186412334442, Acc: 0.931437075138092\n",
      "Epoch: 509, Loss: 0.06714385747909546, Acc: 0.9315218925476074\n",
      "Epoch: 510, Loss: 0.06874977052211761, Acc: 0.9319910407066345\n",
      "Epoch: 511, Loss: 0.06615805625915527, Acc: 0.9316326379776001\n",
      "Epoch: 512, Loss: 0.06669522821903229, Acc: 0.9317597150802612\n",
      "Epoch: 513, Loss: 0.06981199979782104, Acc: 0.931756317615509\n",
      "Epoch: 514, Loss: 0.0704818069934845, Acc: 0.932193398475647\n",
      "Epoch: 515, Loss: 0.06789559870958328, Acc: 0.9320257306098938\n",
      "Epoch: 516, Loss: 0.07039174437522888, Acc: 0.9325991868972778\n",
      "Epoch: 517, Loss: 0.06637760996818542, Acc: 0.9324969053268433\n",
      "Epoch: 518, Loss: 0.0696163922548294, Acc: 0.9326040744781494\n",
      "Epoch: 519, Loss: 0.06570083647966385, Acc: 0.9325516819953918\n",
      "Epoch: 520, Loss: 0.06565993279218674, Acc: 0.9329354166984558\n",
      "Epoch: 521, Loss: 0.06877369433641434, Acc: 0.9327350854873657\n",
      "Epoch: 522, Loss: 0.07007907330989838, Acc: 0.9328864812850952\n",
      "Epoch: 523, Loss: 0.06667733192443848, Acc: 0.9331389665603638\n",
      "Epoch: 524, Loss: 0.06441683322191238, Acc: 0.9330743551254272\n",
      "Epoch: 525, Loss: 0.06475546211004257, Acc: 0.9331477880477905\n",
      "Epoch: 526, Loss: 0.06646270304918289, Acc: 0.9336230754852295\n",
      "Epoch: 527, Loss: 0.06733590364456177, Acc: 0.9334679245948792\n",
      "Epoch: 528, Loss: 0.06922499090433121, Acc: 0.9338400363922119\n",
      "Epoch: 529, Loss: 0.06742425262928009, Acc: 0.9335506558418274\n",
      "Epoch: 530, Loss: 0.06591418385505676, Acc: 0.9339056015014648\n",
      "Epoch: 531, Loss: 0.06375861912965775, Acc: 0.9339108467102051\n",
      "Epoch: 532, Loss: 0.06810969859361649, Acc: 0.9337373971939087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 533, Loss: 0.06382302939891815, Acc: 0.9339364767074585\n",
      "Epoch: 534, Loss: 0.06473378092050552, Acc: 0.9341889023780823\n",
      "Epoch: 535, Loss: 0.07060284912586212, Acc: 0.9342769384384155\n",
      "Epoch: 536, Loss: 0.06314326077699661, Acc: 0.9342191219329834\n",
      "Epoch: 537, Loss: 0.06407587975263596, Acc: 0.9348191022872925\n",
      "Epoch: 538, Loss: 0.06356791406869888, Acc: 0.9345099329948425\n",
      "Epoch: 539, Loss: 0.06712900847196579, Acc: 0.935076117515564\n",
      "Epoch: 540, Loss: 0.06299886107444763, Acc: 0.9347116947174072\n",
      "Epoch: 541, Loss: 0.07126247882843018, Acc: 0.9347881078720093\n",
      "Epoch: 542, Loss: 0.06400861591100693, Acc: 0.9347689151763916\n",
      "Epoch: 543, Loss: 0.06567400693893433, Acc: 0.9356257915496826\n",
      "Epoch: 544, Loss: 0.06562497466802597, Acc: 0.9348294734954834\n",
      "Epoch: 545, Loss: 0.06495223939418793, Acc: 0.9357224106788635\n",
      "Epoch: 546, Loss: 0.06520947813987732, Acc: 0.9351329803466797\n",
      "Epoch: 547, Loss: 0.06278015673160553, Acc: 0.9352043271064758\n",
      "Epoch: 548, Loss: 0.06368283927440643, Acc: 0.9355216026306152\n",
      "Epoch: 549, Loss: 0.06357268989086151, Acc: 0.9355107545852661\n",
      "Epoch: 550, Loss: 0.06549198925495148, Acc: 0.9354250431060791\n",
      "Epoch: 551, Loss: 0.06464284658432007, Acc: 0.9356522560119629\n",
      "Epoch: 552, Loss: 0.06302689015865326, Acc: 0.9356242418289185\n",
      "Epoch: 553, Loss: 0.06716642528772354, Acc: 0.9359352588653564\n",
      "Epoch: 554, Loss: 0.06984540820121765, Acc: 0.936142086982727\n",
      "Epoch: 555, Loss: 0.0679057314991951, Acc: 0.9368782043457031\n",
      "Epoch: 556, Loss: 0.06419550627470016, Acc: 0.9361951947212219\n",
      "Epoch: 557, Loss: 0.0625116229057312, Acc: 0.9362896084785461\n",
      "Epoch: 558, Loss: 0.06478901207447052, Acc: 0.9362598657608032\n",
      "Epoch: 559, Loss: 0.06877103447914124, Acc: 0.9367111921310425\n",
      "Epoch: 560, Loss: 0.06731100380420685, Acc: 0.9366446733474731\n",
      "Epoch: 561, Loss: 0.06462040543556213, Acc: 0.936538815498352\n",
      "Epoch: 562, Loss: 0.06600717455148697, Acc: 0.9367319941520691\n",
      "Epoch: 563, Loss: 0.06692462414503098, Acc: 0.9365230798721313\n",
      "Epoch: 564, Loss: 0.06907573342323303, Acc: 0.9367817640304565\n",
      "Epoch: 565, Loss: 0.06508275866508484, Acc: 0.9370014071464539\n",
      "Epoch: 566, Loss: 0.062298353761434555, Acc: 0.9369767904281616\n",
      "Epoch: 567, Loss: 0.06093408912420273, Acc: 0.9373231530189514\n",
      "Epoch: 568, Loss: 0.061427321285009384, Acc: 0.9372054934501648\n",
      "Epoch: 569, Loss: 0.0611993782222271, Acc: 0.9370882511138916\n",
      "Epoch: 570, Loss: 0.06424939632415771, Acc: 0.9372859001159668\n",
      "Epoch: 571, Loss: 0.06911253929138184, Acc: 0.9381898045539856\n",
      "Epoch: 572, Loss: 0.06252238154411316, Acc: 0.9373282194137573\n",
      "Epoch: 573, Loss: 0.06006137654185295, Acc: 0.9374950528144836\n",
      "Epoch: 574, Loss: 0.06300771981477737, Acc: 0.9381725788116455\n",
      "Epoch: 575, Loss: 0.05996096879243851, Acc: 0.9374791979789734\n",
      "Epoch: 576, Loss: 0.06065665930509567, Acc: 0.9377992749214172\n",
      "Epoch: 577, Loss: 0.06444889307022095, Acc: 0.9379969239234924\n",
      "Epoch: 578, Loss: 0.060945477336645126, Acc: 0.9384127259254456\n",
      "Epoch: 579, Loss: 0.06419149041175842, Acc: 0.9383834600448608\n",
      "Epoch: 580, Loss: 0.062229227274656296, Acc: 0.9383801817893982\n",
      "Epoch: 581, Loss: 0.06127617880702019, Acc: 0.9383299350738525\n",
      "Epoch: 582, Loss: 0.0596376433968544, Acc: 0.9382279515266418\n",
      "Epoch: 583, Loss: 0.061144594103097916, Acc: 0.9383466839790344\n",
      "Epoch: 584, Loss: 0.062459804117679596, Acc: 0.9383262991905212\n",
      "Epoch: 585, Loss: 0.05913133546710014, Acc: 0.9386873841285706\n",
      "Epoch: 586, Loss: 0.060497645288705826, Acc: 0.9387208819389343\n",
      "Epoch: 587, Loss: 0.0603751577436924, Acc: 0.9386571645736694\n",
      "Epoch: 588, Loss: 0.062120139598846436, Acc: 0.9387943744659424\n",
      "Epoch: 589, Loss: 0.06012788042426109, Acc: 0.93909752368927\n",
      "Epoch: 590, Loss: 0.06414173543453217, Acc: 0.9391090273857117\n",
      "Epoch: 591, Loss: 0.06282345950603485, Acc: 0.9393826723098755\n",
      "Epoch: 592, Loss: 0.061072178184986115, Acc: 0.9389786124229431\n",
      "Epoch: 593, Loss: 0.06186868995428085, Acc: 0.9391617774963379\n",
      "Epoch: 594, Loss: 0.0614219605922699, Acc: 0.9391964673995972\n",
      "Epoch: 595, Loss: 0.060156773775815964, Acc: 0.9396369457244873\n",
      "Epoch: 596, Loss: 0.06193586438894272, Acc: 0.9397671818733215\n",
      "Epoch: 597, Loss: 0.05996235832571983, Acc: 0.9395973682403564\n",
      "Epoch: 598, Loss: 0.060619913041591644, Acc: 0.9394790530204773\n",
      "Epoch: 599, Loss: 0.06200544536113739, Acc: 0.9398549199104309\n",
      "Epoch: 600, Loss: 0.059901248663663864, Acc: 0.939617395401001\n",
      "Epoch: 601, Loss: 0.06085832417011261, Acc: 0.9398952722549438\n",
      "Epoch: 602, Loss: 0.06279170513153076, Acc: 0.939919114112854\n",
      "Epoch: 603, Loss: 0.0645938366651535, Acc: 0.9402647614479065\n",
      "Epoch: 604, Loss: 0.06691040843725204, Acc: 0.9403663277626038\n",
      "Epoch: 605, Loss: 0.05804431438446045, Acc: 0.9399243593215942\n",
      "Epoch: 606, Loss: 0.05923156067728996, Acc: 0.9403350353240967\n",
      "Epoch: 607, Loss: 0.05936134234070778, Acc: 0.940243124961853\n",
      "Epoch: 608, Loss: 0.06313305348157883, Acc: 0.9404237270355225\n",
      "Epoch: 609, Loss: 0.0590689517557621, Acc: 0.9403378963470459\n",
      "Epoch: 610, Loss: 0.060036759823560715, Acc: 0.9405102729797363\n",
      "Epoch: 611, Loss: 0.05726844072341919, Acc: 0.9403448700904846\n",
      "Epoch: 612, Loss: 0.06042103469371796, Acc: 0.9403799772262573\n",
      "Epoch: 613, Loss: 0.06030900776386261, Acc: 0.9406918287277222\n",
      "Epoch: 614, Loss: 0.06108635663986206, Acc: 0.9410053491592407\n",
      "Epoch: 615, Loss: 0.06008574366569519, Acc: 0.9405664205551147\n",
      "Epoch: 616, Loss: 0.059775762259960175, Acc: 0.9408518075942993\n",
      "Epoch: 617, Loss: 0.058067530393600464, Acc: 0.9407565593719482\n",
      "Epoch: 618, Loss: 0.058456163853406906, Acc: 0.9407827258110046\n",
      "Epoch: 619, Loss: 0.059511348605155945, Acc: 0.941185712814331\n",
      "Epoch: 620, Loss: 0.05742316320538521, Acc: 0.9409173727035522\n",
      "Epoch: 621, Loss: 0.05814601480960846, Acc: 0.9411023855209351\n",
      "Epoch: 622, Loss: 0.061710111796855927, Acc: 0.9412597417831421\n",
      "Epoch: 623, Loss: 0.05978246405720711, Acc: 0.9411786198616028\n",
      "Epoch: 624, Loss: 0.05785605311393738, Acc: 0.9415942430496216\n",
      "Epoch: 625, Loss: 0.061400800943374634, Acc: 0.9417078495025635\n",
      "Epoch: 626, Loss: 0.06450171768665314, Acc: 0.9414522051811218\n",
      "Epoch: 627, Loss: 0.05860432609915733, Acc: 0.9418946504592896\n",
      "Epoch: 628, Loss: 0.05788124352693558, Acc: 0.9417746067047119\n",
      "Epoch: 629, Loss: 0.058007411658763885, Acc: 0.9419825673103333\n",
      "Epoch: 630, Loss: 0.05808287486433983, Acc: 0.9417158961296082\n",
      "Epoch: 631, Loss: 0.05724450573325157, Acc: 0.9422410726547241\n",
      "Epoch: 632, Loss: 0.05674400553107262, Acc: 0.9422141909599304\n",
      "Epoch: 633, Loss: 0.05982246249914169, Acc: 0.9421239495277405\n",
      "Epoch: 634, Loss: 0.057967450469732285, Acc: 0.9424006938934326\n",
      "Epoch: 635, Loss: 0.05618489533662796, Acc: 0.9421538710594177\n",
      "Epoch: 636, Loss: 0.05729973688721657, Acc: 0.9421085715293884\n",
      "Epoch: 637, Loss: 0.06277766823768616, Acc: 0.9428257942199707\n",
      "Epoch: 638, Loss: 0.05896305292844772, Acc: 0.942545473575592\n",
      "Epoch: 639, Loss: 0.06242065131664276, Acc: 0.9424328804016113\n",
      "Epoch: 640, Loss: 0.05902840569615364, Acc: 0.9426344037055969\n",
      "Epoch: 641, Loss: 0.05682676285505295, Acc: 0.9425027370452881\n",
      "Epoch: 642, Loss: 0.062476105988025665, Acc: 0.9431710243225098\n",
      "Epoch: 643, Loss: 0.05701247975230217, Acc: 0.9425873756408691\n",
      "Epoch: 644, Loss: 0.05579323694109917, Acc: 0.9426007866859436\n",
      "Epoch: 645, Loss: 0.0547700971364975, Acc: 0.942611813545227\n",
      "Epoch: 646, Loss: 0.05667731538414955, Acc: 0.9425473213195801\n",
      "Epoch: 647, Loss: 0.05577117204666138, Acc: 0.9432988166809082\n",
      "Epoch: 648, Loss: 0.061619170010089874, Acc: 0.9428413510322571\n",
      "Epoch: 649, Loss: 0.05811565741896629, Acc: 0.9430743455886841\n",
      "Epoch: 650, Loss: 0.05659954622387886, Acc: 0.9433053731918335\n",
      "Epoch: 651, Loss: 0.06016787886619568, Acc: 0.9436317086219788\n",
      "Epoch: 652, Loss: 0.056085988879203796, Acc: 0.9431769251823425\n",
      "Epoch: 653, Loss: 0.05626971274614334, Acc: 0.9430950880050659\n",
      "Epoch: 654, Loss: 0.0637926310300827, Acc: 0.943406879901886\n",
      "Epoch: 655, Loss: 0.055982451885938644, Acc: 0.9432840943336487\n",
      "Epoch: 656, Loss: 0.054766695946455, Acc: 0.9433772563934326\n",
      "Epoch: 657, Loss: 0.056894510984420776, Acc: 0.9436636567115784\n",
      "Epoch: 658, Loss: 0.054594509303569794, Acc: 0.9433505535125732\n",
      "Epoch: 659, Loss: 0.06184562295675278, Acc: 0.9435114860534668\n",
      "Epoch: 660, Loss: 0.05405731871724129, Acc: 0.9436127543449402\n",
      "Epoch: 661, Loss: 0.055002521723508835, Acc: 0.9435816407203674\n",
      "Epoch: 662, Loss: 0.05860806256532669, Acc: 0.9436638951301575\n",
      "Epoch: 663, Loss: 0.055595897138118744, Acc: 0.943962812423706\n",
      "Epoch: 664, Loss: 0.05621260032057762, Acc: 0.9436900615692139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 665, Loss: 0.05674665793776512, Acc: 0.9441694617271423\n",
      "Epoch: 666, Loss: 0.05922481417655945, Acc: 0.9439395070075989\n",
      "Epoch: 667, Loss: 0.05824744701385498, Acc: 0.9441211819648743\n",
      "Epoch: 668, Loss: 0.05734671652317047, Acc: 0.9440871477127075\n",
      "Epoch: 669, Loss: 0.055814098566770554, Acc: 0.9440049529075623\n",
      "Epoch: 670, Loss: 0.05472322180867195, Acc: 0.9441719055175781\n",
      "Epoch: 671, Loss: 0.05347197875380516, Acc: 0.9441102743148804\n",
      "Epoch: 672, Loss: 0.05970486253499985, Acc: 0.9445708394050598\n",
      "Epoch: 673, Loss: 0.05446884408593178, Acc: 0.944560170173645\n",
      "Epoch: 674, Loss: 0.054408274590969086, Acc: 0.9445551633834839\n",
      "Epoch: 675, Loss: 0.05356895551085472, Acc: 0.9443528652191162\n",
      "Epoch: 676, Loss: 0.05427231267094612, Acc: 0.9445979595184326\n",
      "Epoch: 677, Loss: 0.058212146162986755, Acc: 0.9446650147438049\n",
      "Epoch: 678, Loss: 0.05707681179046631, Acc: 0.9445685744285583\n",
      "Epoch: 679, Loss: 0.05471542850136757, Acc: 0.9451181888580322\n",
      "Epoch: 680, Loss: 0.0540221706032753, Acc: 0.9446274042129517\n",
      "Epoch: 681, Loss: 0.05621815472841263, Acc: 0.9446467757225037\n",
      "Epoch: 682, Loss: 0.05526028573513031, Acc: 0.944680392742157\n",
      "Epoch: 683, Loss: 0.05421065166592598, Acc: 0.9453824758529663\n",
      "Epoch: 684, Loss: 0.055692385882139206, Acc: 0.9448268413543701\n",
      "Epoch: 685, Loss: 0.05842195078730583, Acc: 0.9451700448989868\n",
      "Epoch: 686, Loss: 0.05294913426041603, Acc: 0.9452149868011475\n",
      "Epoch: 687, Loss: 0.05326433852314949, Acc: 0.944956362247467\n",
      "Epoch: 688, Loss: 0.05387641116976738, Acc: 0.9452341794967651\n",
      "Epoch: 689, Loss: 0.05321543663740158, Acc: 0.9451647996902466\n",
      "Epoch: 690, Loss: 0.055294763296842575, Acc: 0.9450517296791077\n",
      "Epoch: 691, Loss: 0.05610787123441696, Acc: 0.9454054236412048\n",
      "Epoch: 692, Loss: 0.058514948934316635, Acc: 0.9458207488059998\n",
      "Epoch: 693, Loss: 0.06023480370640755, Acc: 0.9453710913658142\n",
      "Epoch: 694, Loss: 0.05453799292445183, Acc: 0.9455276727676392\n",
      "Epoch: 695, Loss: 0.06277241557836533, Acc: 0.9465398788452148\n",
      "Epoch: 696, Loss: 0.058643221855163574, Acc: 0.9462939500808716\n",
      "Epoch: 697, Loss: 0.05415363609790802, Acc: 0.945783257484436\n",
      "Epoch: 698, Loss: 0.053759440779685974, Acc: 0.9458171129226685\n",
      "Epoch: 699, Loss: 0.0526900589466095, Acc: 0.9455698132514954\n",
      "Epoch: 700, Loss: 0.05302383005619049, Acc: 0.9458370208740234\n",
      "Epoch: 701, Loss: 0.05418796092271805, Acc: 0.9466379880905151\n",
      "Epoch: 702, Loss: 0.05421094968914986, Acc: 0.9461530447006226\n",
      "Epoch: 703, Loss: 0.059505268931388855, Acc: 0.9459792971611023\n",
      "Epoch: 704, Loss: 0.057293668389320374, Acc: 0.9458878040313721\n",
      "Epoch: 705, Loss: 0.05477624014019966, Acc: 0.9460588693618774\n",
      "Epoch: 706, Loss: 0.05876430496573448, Acc: 0.9462407827377319\n",
      "Epoch: 707, Loss: 0.05498802661895752, Acc: 0.9461856484413147\n",
      "Epoch: 708, Loss: 0.05532393604516983, Acc: 0.9463875889778137\n",
      "Epoch: 709, Loss: 0.05395103245973587, Acc: 0.9463397860527039\n",
      "Epoch: 710, Loss: 0.05491890385746956, Acc: 0.9468276500701904\n",
      "Epoch: 711, Loss: 0.05331790819764137, Acc: 0.9464367628097534\n",
      "Epoch: 712, Loss: 0.054818131029605865, Acc: 0.9465020895004272\n",
      "Epoch: 713, Loss: 0.053093504160642624, Acc: 0.9473934173583984\n",
      "Epoch: 714, Loss: 0.05328507721424103, Acc: 0.9465968012809753\n",
      "Epoch: 715, Loss: 0.05329374969005585, Acc: 0.946463406085968\n",
      "Epoch: 716, Loss: 0.056142643094062805, Acc: 0.9468427300453186\n",
      "Epoch: 717, Loss: 0.0519048236310482, Acc: 0.9466041922569275\n",
      "Epoch: 718, Loss: 0.054090384393930435, Acc: 0.946587860584259\n",
      "Epoch: 719, Loss: 0.05291762948036194, Acc: 0.9471772313117981\n",
      "Epoch: 720, Loss: 0.05715034157037735, Acc: 0.9472000598907471\n",
      "Epoch: 721, Loss: 0.057421959936618805, Acc: 0.947002649307251\n",
      "Epoch: 722, Loss: 0.05706919729709625, Acc: 0.9467133283615112\n",
      "Epoch: 723, Loss: 0.054947733879089355, Acc: 0.9469287991523743\n",
      "Epoch: 724, Loss: 0.06015734747052193, Acc: 0.9471112489700317\n",
      "Epoch: 725, Loss: 0.0544925257563591, Acc: 0.946862518787384\n",
      "Epoch: 726, Loss: 0.052608851343393326, Acc: 0.9470503926277161\n",
      "Epoch: 727, Loss: 0.052169933915138245, Acc: 0.9478060007095337\n",
      "Epoch: 728, Loss: 0.05382731929421425, Acc: 0.947375476360321\n",
      "Epoch: 729, Loss: 0.051563411951065063, Acc: 0.9471552968025208\n",
      "Epoch: 730, Loss: 0.05246324837207794, Acc: 0.947693943977356\n",
      "Epoch: 731, Loss: 0.05502341687679291, Acc: 0.9473680853843689\n",
      "Epoch: 732, Loss: 0.055641405284404755, Acc: 0.9479445219039917\n",
      "Epoch: 733, Loss: 0.051147133111953735, Acc: 0.9471850395202637\n",
      "Epoch: 734, Loss: 0.0586833730340004, Acc: 0.9473519325256348\n",
      "Epoch: 735, Loss: 0.055130861699581146, Acc: 0.9474127888679504\n",
      "Epoch: 736, Loss: 0.051831070333719254, Acc: 0.9473091959953308\n",
      "Epoch: 737, Loss: 0.05478888750076294, Acc: 0.9477439522743225\n",
      "Epoch: 738, Loss: 0.05241893231868744, Acc: 0.9477521181106567\n",
      "Epoch: 739, Loss: 0.053817279636859894, Acc: 0.947766900062561\n",
      "Epoch: 740, Loss: 0.05388588458299637, Acc: 0.9475646615028381\n",
      "Epoch: 741, Loss: 0.05127784609794617, Acc: 0.9476026892662048\n",
      "Epoch: 742, Loss: 0.05722611024975777, Acc: 0.9479434490203857\n",
      "Epoch: 743, Loss: 0.05696246027946472, Acc: 0.9481319189071655\n",
      "Epoch: 744, Loss: 0.056110769510269165, Acc: 0.9478613138198853\n",
      "Epoch: 745, Loss: 0.05061691999435425, Acc: 0.9478854537010193\n",
      "Epoch: 746, Loss: 0.051815811544656754, Acc: 0.9479652643203735\n",
      "Epoch: 747, Loss: 0.052893541753292084, Acc: 0.9481414556503296\n",
      "Epoch: 748, Loss: 0.052578315138816833, Acc: 0.9482831954956055\n",
      "Epoch: 749, Loss: 0.051850177347660065, Acc: 0.9484858512878418\n",
      "Epoch: 750, Loss: 0.051883336156606674, Acc: 0.9479944109916687\n",
      "Epoch: 751, Loss: 0.05512518435716629, Acc: 0.9481069445610046\n",
      "Epoch: 752, Loss: 0.0531432069838047, Acc: 0.9485380053520203\n",
      "Epoch: 753, Loss: 0.05506456643342972, Acc: 0.9486314654350281\n",
      "Epoch: 754, Loss: 0.055330440402030945, Acc: 0.9486019611358643\n",
      "Epoch: 755, Loss: 0.05293654650449753, Acc: 0.948266863822937\n",
      "Epoch: 756, Loss: 0.051847100257873535, Acc: 0.9483087062835693\n",
      "Epoch: 757, Loss: 0.05177833139896393, Acc: 0.9483842849731445\n",
      "Epoch: 758, Loss: 0.053017083555459976, Acc: 0.948344349861145\n",
      "Epoch: 759, Loss: 0.05026472732424736, Acc: 0.9485827684402466\n",
      "Epoch: 760, Loss: 0.04982190951704979, Acc: 0.9486083388328552\n",
      "Epoch: 761, Loss: 0.0532638281583786, Acc: 0.9486778378486633\n",
      "Epoch: 762, Loss: 0.050125978887081146, Acc: 0.9489893913269043\n",
      "Epoch: 763, Loss: 0.053736165165901184, Acc: 0.9487297534942627\n",
      "Epoch: 764, Loss: 0.050961099565029144, Acc: 0.9486859440803528\n",
      "Epoch: 765, Loss: 0.05278535187244415, Acc: 0.9488913416862488\n",
      "Epoch: 766, Loss: 0.05378486216068268, Acc: 0.9487297534942627\n",
      "Epoch: 767, Loss: 0.05319538712501526, Acc: 0.9490033388137817\n",
      "Epoch: 768, Loss: 0.052198562771081924, Acc: 0.9491085410118103\n",
      "Epoch: 769, Loss: 0.05346057564020157, Acc: 0.9499830007553101\n",
      "Epoch: 770, Loss: 0.049951765686273575, Acc: 0.9489139318466187\n",
      "Epoch: 771, Loss: 0.04900548979640007, Acc: 0.9488361477851868\n",
      "Epoch: 772, Loss: 0.052282530814409256, Acc: 0.949114978313446\n",
      "Epoch: 773, Loss: 0.05342275649309158, Acc: 0.9491279125213623\n",
      "Epoch: 774, Loss: 0.05434251204133034, Acc: 0.9489316940307617\n",
      "Epoch: 775, Loss: 0.055284012109041214, Acc: 0.9491099119186401\n",
      "Epoch: 776, Loss: 0.05410389602184296, Acc: 0.9493865370750427\n",
      "Epoch: 777, Loss: 0.05082029476761818, Acc: 0.9494303464889526\n",
      "Epoch: 778, Loss: 0.050249166786670685, Acc: 0.9495550394058228\n",
      "Epoch: 779, Loss: 0.04894042760133743, Acc: 0.9493881464004517\n",
      "Epoch: 780, Loss: 0.05125124752521515, Acc: 0.9494256973266602\n",
      "Epoch: 781, Loss: 0.04886892810463905, Acc: 0.9498720765113831\n",
      "Epoch: 782, Loss: 0.051399633288383484, Acc: 0.9499884247779846\n",
      "Epoch: 783, Loss: 0.05366813391447067, Acc: 0.9493356943130493\n",
      "Epoch: 784, Loss: 0.05035156011581421, Acc: 0.9499093890190125\n",
      "Epoch: 785, Loss: 0.04958300292491913, Acc: 0.9493666291236877\n",
      "Epoch: 786, Loss: 0.049253545701503754, Acc: 0.949691653251648\n",
      "Epoch: 787, Loss: 0.052842602133750916, Acc: 0.9497832655906677\n",
      "Epoch: 788, Loss: 0.05201219022274017, Acc: 0.9501080513000488\n",
      "Epoch: 789, Loss: 0.052314113825559616, Acc: 0.9501225352287292\n",
      "Epoch: 790, Loss: 0.04936506226658821, Acc: 0.9496986865997314\n",
      "Epoch: 791, Loss: 0.05018213763833046, Acc: 0.9499406218528748\n",
      "Epoch: 792, Loss: 0.05041315034031868, Acc: 0.9496433138847351\n",
      "Epoch: 793, Loss: 0.04967711493372917, Acc: 0.9500488638877869\n",
      "Epoch: 794, Loss: 0.05015081912279129, Acc: 0.9505457282066345\n",
      "Epoch: 795, Loss: 0.04956619814038277, Acc: 0.9501243233680725\n",
      "Epoch: 796, Loss: 0.04801514372229576, Acc: 0.9501286149024963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 797, Loss: 0.048744700849056244, Acc: 0.9499186277389526\n",
      "Epoch: 798, Loss: 0.04932372272014618, Acc: 0.9499800801277161\n",
      "Epoch: 799, Loss: 0.052062518894672394, Acc: 0.950975775718689\n",
      "Epoch: 800, Loss: 0.050705477595329285, Acc: 0.9503576755523682\n",
      "Epoch: 801, Loss: 0.049564532935619354, Acc: 0.9499832391738892\n",
      "Epoch: 802, Loss: 0.04979686811566353, Acc: 0.9510863423347473\n",
      "Epoch: 803, Loss: 0.05024643987417221, Acc: 0.9508743286132812\n",
      "Epoch: 804, Loss: 0.05202683061361313, Acc: 0.9502168893814087\n",
      "Epoch: 805, Loss: 0.049476731568574905, Acc: 0.9507651329040527\n",
      "Epoch: 806, Loss: 0.05099417641758919, Acc: 0.9505273103713989\n",
      "Epoch: 807, Loss: 0.04913914203643799, Acc: 0.9503045082092285\n",
      "Epoch: 808, Loss: 0.04930766671895981, Acc: 0.9504245519638062\n",
      "Epoch: 809, Loss: 0.05032163858413696, Acc: 0.9503446221351624\n",
      "Epoch: 810, Loss: 0.04889924079179764, Acc: 0.9510545134544373\n",
      "Epoch: 811, Loss: 0.048201773315668106, Acc: 0.950576663017273\n",
      "Epoch: 812, Loss: 0.047747060656547546, Acc: 0.9504712820053101\n",
      "Epoch: 813, Loss: 0.05025411397218704, Acc: 0.9506004452705383\n",
      "Epoch: 814, Loss: 0.04801858961582184, Acc: 0.9505261182785034\n",
      "Epoch: 815, Loss: 0.04803906008601189, Acc: 0.9505981802940369\n",
      "Epoch: 816, Loss: 0.047366105020046234, Acc: 0.9509672522544861\n",
      "Epoch: 817, Loss: 0.04785126447677612, Acc: 0.9508397579193115\n",
      "Epoch: 818, Loss: 0.048798054456710815, Acc: 0.9508119821548462\n",
      "Epoch: 819, Loss: 0.04808073863387108, Acc: 0.9510753154754639\n",
      "Epoch: 820, Loss: 0.05291371047496796, Acc: 0.9511857032775879\n",
      "Epoch: 821, Loss: 0.049206722527742386, Acc: 0.9511122703552246\n",
      "Epoch: 822, Loss: 0.04802728071808815, Acc: 0.9511931538581848\n",
      "Epoch: 823, Loss: 0.05333157628774643, Acc: 0.9518031477928162\n",
      "Epoch: 824, Loss: 0.047851692885160446, Acc: 0.9512004852294922\n",
      "Epoch: 825, Loss: 0.04973011091351509, Acc: 0.9509298205375671\n",
      "Epoch: 826, Loss: 0.05130843445658684, Acc: 0.9510876536369324\n",
      "Epoch: 827, Loss: 0.05069936439394951, Acc: 0.9512581825256348\n",
      "Epoch: 828, Loss: 0.051227912306785583, Acc: 0.9513498544692993\n",
      "Epoch: 829, Loss: 0.04760541021823883, Acc: 0.9514898061752319\n",
      "Epoch: 830, Loss: 0.04986706003546715, Acc: 0.9513864517211914\n",
      "Epoch: 831, Loss: 0.05449467897415161, Acc: 0.9521150588989258\n",
      "Epoch: 832, Loss: 0.04941006749868393, Acc: 0.9514895677566528\n",
      "Epoch: 833, Loss: 0.0471164807677269, Acc: 0.9514880180358887\n",
      "Epoch: 834, Loss: 0.04867181554436684, Acc: 0.9515452980995178\n",
      "Epoch: 835, Loss: 0.04910087585449219, Acc: 0.9513394832611084\n",
      "Epoch: 836, Loss: 0.047112755477428436, Acc: 0.9513193964958191\n",
      "Epoch: 837, Loss: 0.0480426624417305, Acc: 0.9513450264930725\n",
      "Epoch: 838, Loss: 0.05005296319723129, Acc: 0.9514625072479248\n",
      "Epoch: 839, Loss: 0.047700922936201096, Acc: 0.9514669179916382\n",
      "Epoch: 840, Loss: 0.04845454916357994, Acc: 0.951479434967041\n",
      "Epoch: 841, Loss: 0.048803459852933884, Acc: 0.9516298174858093\n",
      "Epoch: 842, Loss: 0.051341794431209564, Acc: 0.9517242312431335\n",
      "Epoch: 843, Loss: 0.048583537340164185, Acc: 0.9515693187713623\n",
      "Epoch: 844, Loss: 0.0485406331717968, Acc: 0.9521730542182922\n",
      "Epoch: 845, Loss: 0.04902864620089531, Acc: 0.9516929388046265\n",
      "Epoch: 846, Loss: 0.04793332517147064, Acc: 0.9518367648124695\n",
      "Epoch: 847, Loss: 0.050582535564899445, Acc: 0.9518246650695801\n",
      "Epoch: 848, Loss: 0.05210193619132042, Acc: 0.9521027207374573\n",
      "Epoch: 849, Loss: 0.0471959114074707, Acc: 0.952238917350769\n",
      "Epoch: 850, Loss: 0.046883437782526016, Acc: 0.9523109197616577\n",
      "Epoch: 851, Loss: 0.05043067783117294, Acc: 0.9519622921943665\n",
      "Epoch: 852, Loss: 0.04584336653351784, Acc: 0.9519856572151184\n",
      "Epoch: 853, Loss: 0.04705854877829552, Acc: 0.9520545601844788\n",
      "Epoch: 854, Loss: 0.045501165091991425, Acc: 0.9518681764602661\n",
      "Epoch: 855, Loss: 0.05241964012384415, Acc: 0.9522901177406311\n",
      "Epoch: 856, Loss: 0.04734627902507782, Acc: 0.952174961566925\n",
      "Epoch: 857, Loss: 0.05044538527727127, Acc: 0.9524050354957581\n",
      "Epoch: 858, Loss: 0.04678509384393692, Acc: 0.9523164629936218\n",
      "Epoch: 859, Loss: 0.048320330679416656, Acc: 0.9523464441299438\n",
      "Epoch: 860, Loss: 0.053828030824661255, Acc: 0.9534085988998413\n",
      "Epoch: 861, Loss: 0.04905491694808006, Acc: 0.9526034593582153\n",
      "Epoch: 862, Loss: 0.04622571915388107, Acc: 0.9527260661125183\n",
      "Epoch: 863, Loss: 0.048669833689928055, Acc: 0.9521982073783875\n",
      "Epoch: 864, Loss: 0.04838870093226433, Acc: 0.9526499509811401\n",
      "Epoch: 865, Loss: 0.04717309772968292, Acc: 0.9525933265686035\n",
      "Epoch: 866, Loss: 0.04702259227633476, Acc: 0.952440083026886\n",
      "Epoch: 867, Loss: 0.048093609511852264, Acc: 0.9524040222167969\n",
      "Epoch: 868, Loss: 0.04622240364551544, Acc: 0.9529299139976501\n",
      "Epoch: 869, Loss: 0.04782247915863991, Acc: 0.9529308676719666\n",
      "Epoch: 870, Loss: 0.04597367346286774, Acc: 0.9534367918968201\n",
      "Epoch: 871, Loss: 0.046235065907239914, Acc: 0.952796459197998\n",
      "Epoch: 872, Loss: 0.04611479118466377, Acc: 0.9524926543235779\n",
      "Epoch: 873, Loss: 0.045532599091529846, Acc: 0.9525617361068726\n",
      "Epoch: 874, Loss: 0.046219561249017715, Acc: 0.9527139663696289\n",
      "Epoch: 875, Loss: 0.047116268426179886, Acc: 0.9526920318603516\n",
      "Epoch: 876, Loss: 0.047389041632413864, Acc: 0.9528880715370178\n",
      "Epoch: 877, Loss: 0.04602097347378731, Acc: 0.9527688026428223\n",
      "Epoch: 878, Loss: 0.049667734652757645, Acc: 0.9527938961982727\n",
      "Epoch: 879, Loss: 0.0471460297703743, Acc: 0.9527108669281006\n",
      "Epoch: 880, Loss: 0.04961083084344864, Acc: 0.9532673954963684\n",
      "Epoch: 881, Loss: 0.04568572714924812, Acc: 0.9528385996818542\n",
      "Epoch: 882, Loss: 0.04758083075284958, Acc: 0.9529919028282166\n",
      "Epoch: 883, Loss: 0.046306829899549484, Acc: 0.9533212184906006\n",
      "Epoch: 884, Loss: 0.0460929349064827, Acc: 0.9531052112579346\n",
      "Epoch: 885, Loss: 0.046007633209228516, Acc: 0.9532505869865417\n",
      "Epoch: 886, Loss: 0.047054946422576904, Acc: 0.9531992077827454\n",
      "Epoch: 887, Loss: 0.04720798507332802, Acc: 0.9534388184547424\n",
      "Epoch: 888, Loss: 0.04891376197338104, Acc: 0.9533753395080566\n",
      "Epoch: 889, Loss: 0.052522677928209305, Acc: 0.9532871842384338\n",
      "Epoch: 890, Loss: 0.04571985453367233, Acc: 0.9531565308570862\n",
      "Epoch: 891, Loss: 0.046194665133953094, Acc: 0.9535135626792908\n",
      "Epoch: 892, Loss: 0.04972337931394577, Acc: 0.9532025456428528\n",
      "Epoch: 893, Loss: 0.049079883843660355, Acc: 0.9533301591873169\n",
      "Epoch: 894, Loss: 0.04691262170672417, Acc: 0.9537169337272644\n",
      "Epoch: 895, Loss: 0.04578946530818939, Acc: 0.9533087611198425\n",
      "Epoch: 896, Loss: 0.04603419452905655, Acc: 0.9537344574928284\n",
      "Epoch: 897, Loss: 0.0451330840587616, Acc: 0.9534832835197449\n",
      "Epoch: 898, Loss: 0.045724183320999146, Acc: 0.9533452987670898\n",
      "Epoch: 899, Loss: 0.04914773255586624, Acc: 0.9539160132408142\n",
      "Epoch: 900, Loss: 0.04727334901690483, Acc: 0.9535338878631592\n",
      "Epoch: 901, Loss: 0.045504018664360046, Acc: 0.953502357006073\n",
      "Epoch: 902, Loss: 0.04696134477853775, Acc: 0.9535138010978699\n",
      "Epoch: 903, Loss: 0.046846598386764526, Acc: 0.9536569118499756\n",
      "Epoch: 904, Loss: 0.045720990747213364, Acc: 0.9537438750267029\n",
      "Epoch: 905, Loss: 0.04965434968471527, Acc: 0.9538344740867615\n",
      "Epoch: 906, Loss: 0.04633406177163124, Acc: 0.9537634253501892\n",
      "Epoch: 907, Loss: 0.04597104713320732, Acc: 0.9537519216537476\n",
      "Epoch: 908, Loss: 0.049179330468177795, Acc: 0.9537203907966614\n",
      "Epoch: 909, Loss: 0.04671315848827362, Acc: 0.953589916229248\n",
      "Epoch: 910, Loss: 0.044728826731443405, Acc: 0.9539279341697693\n",
      "Epoch: 911, Loss: 0.04498439282178879, Acc: 0.9536756873130798\n",
      "Epoch: 912, Loss: 0.04957129806280136, Acc: 0.9541301131248474\n",
      "Epoch: 913, Loss: 0.05220116674900055, Acc: 0.9550548791885376\n",
      "Epoch: 914, Loss: 0.0452154166996479, Acc: 0.9540659189224243\n",
      "Epoch: 915, Loss: 0.045496683567762375, Acc: 0.9538809061050415\n",
      "Epoch: 916, Loss: 0.045255620032548904, Acc: 0.9539617896080017\n",
      "Epoch: 917, Loss: 0.04837255924940109, Acc: 0.9539808034896851\n",
      "Epoch: 918, Loss: 0.04792223870754242, Acc: 0.9541361331939697\n",
      "Epoch: 919, Loss: 0.04773779213428497, Acc: 0.9542347192764282\n",
      "Epoch: 920, Loss: 0.044740982353687286, Acc: 0.9546852707862854\n",
      "Epoch: 921, Loss: 0.045114513486623764, Acc: 0.9541335105895996\n",
      "Epoch: 922, Loss: 0.04518810287117958, Acc: 0.9541502594947815\n",
      "Epoch: 923, Loss: 0.048376649618148804, Acc: 0.954251229763031\n",
      "Epoch: 924, Loss: 0.04563458636403084, Acc: 0.9543159604072571\n",
      "Epoch: 925, Loss: 0.047134850174188614, Acc: 0.9543055295944214\n",
      "Epoch: 926, Loss: 0.04561398923397064, Acc: 0.9542994499206543\n",
      "Epoch: 927, Loss: 0.045624975115060806, Acc: 0.9546878337860107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 928, Loss: 0.049582820385694504, Acc: 0.954723060131073\n",
      "Epoch: 929, Loss: 0.04815281182527542, Acc: 0.9543078541755676\n",
      "Epoch: 930, Loss: 0.046931471675634384, Acc: 0.9544374346733093\n",
      "Epoch: 931, Loss: 0.046012360602617264, Acc: 0.9543457627296448\n",
      "Epoch: 932, Loss: 0.04483450576663017, Acc: 0.9546314477920532\n",
      "Epoch: 933, Loss: 0.045332614332437515, Acc: 0.9546685218811035\n",
      "Epoch: 934, Loss: 0.04529143124818802, Acc: 0.9548842906951904\n",
      "Epoch: 935, Loss: 0.045945871621370316, Acc: 0.9547639489173889\n",
      "Epoch: 936, Loss: 0.04351077228784561, Acc: 0.9544704556465149\n",
      "Epoch: 937, Loss: 0.04398595541715622, Acc: 0.9544357657432556\n",
      "Epoch: 938, Loss: 0.045206218957901, Acc: 0.95491623878479\n",
      "Epoch: 939, Loss: 0.04424717277288437, Acc: 0.9545133709907532\n",
      "Epoch: 940, Loss: 0.0445111021399498, Acc: 0.9547798037528992\n",
      "Epoch: 941, Loss: 0.04900689795613289, Acc: 0.9545283913612366\n",
      "Epoch: 942, Loss: 0.045679252594709396, Acc: 0.954677164554596\n",
      "Epoch: 943, Loss: 0.04596559330821037, Acc: 0.9551575183868408\n",
      "Epoch: 944, Loss: 0.044616345316171646, Acc: 0.9546093344688416\n",
      "Epoch: 945, Loss: 0.04377239942550659, Acc: 0.9545882940292358\n",
      "Epoch: 946, Loss: 0.04593044891953468, Acc: 0.9548318386077881\n",
      "Epoch: 947, Loss: 0.044405609369277954, Acc: 0.9548757076263428\n",
      "Epoch: 948, Loss: 0.04455694556236267, Acc: 0.9551002979278564\n",
      "Epoch: 949, Loss: 0.04400113597512245, Acc: 0.954860508441925\n",
      "Epoch: 950, Loss: 0.04583040624856949, Acc: 0.9549998641014099\n",
      "Epoch: 951, Loss: 0.049844905734062195, Acc: 0.9549784064292908\n",
      "Epoch: 952, Loss: 0.04738815873861313, Acc: 0.9550853967666626\n",
      "Epoch: 953, Loss: 0.04583480581641197, Acc: 0.9548612236976624\n",
      "Epoch: 954, Loss: 0.04551742970943451, Acc: 0.9550985097885132\n",
      "Epoch: 955, Loss: 0.04363667592406273, Acc: 0.9550065398216248\n",
      "Epoch: 956, Loss: 0.045310452580451965, Acc: 0.9553430080413818\n",
      "Epoch: 957, Loss: 0.04470635578036308, Acc: 0.9548998475074768\n",
      "Epoch: 958, Loss: 0.04433119669556618, Acc: 0.9551804661750793\n",
      "Epoch: 959, Loss: 0.04420856758952141, Acc: 0.9552567601203918\n",
      "Epoch: 960, Loss: 0.04416919872164726, Acc: 0.9550624489784241\n",
      "Epoch: 961, Loss: 0.04507581889629364, Acc: 0.955072820186615\n",
      "Epoch: 962, Loss: 0.0479179322719574, Acc: 0.9551168084144592\n",
      "Epoch: 963, Loss: 0.044570114463567734, Acc: 0.9551669955253601\n",
      "Epoch: 964, Loss: 0.04488281533122063, Acc: 0.9551548957824707\n",
      "Epoch: 965, Loss: 0.042859286069869995, Acc: 0.9552413821220398\n",
      "Epoch: 966, Loss: 0.045780401676893234, Acc: 0.9555267691612244\n",
      "Epoch: 967, Loss: 0.04394051432609558, Acc: 0.9560697078704834\n",
      "Epoch: 968, Loss: 0.04470950365066528, Acc: 0.9557219743728638\n",
      "Epoch: 969, Loss: 0.04441716521978378, Acc: 0.9555633664131165\n",
      "Epoch: 970, Loss: 0.044018641114234924, Acc: 0.9553999900817871\n",
      "Epoch: 971, Loss: 0.04309053346514702, Acc: 0.9555698037147522\n",
      "Epoch: 972, Loss: 0.04584621638059616, Acc: 0.9553338289260864\n",
      "Epoch: 973, Loss: 0.04448949545621872, Acc: 0.9555279612541199\n",
      "Epoch: 974, Loss: 0.04424189776182175, Acc: 0.9556316137313843\n",
      "Epoch: 975, Loss: 0.049148447811603546, Acc: 0.9557965993881226\n",
      "Epoch: 976, Loss: 0.04489718750119209, Acc: 0.9560290575027466\n",
      "Epoch: 977, Loss: 0.04454562067985535, Acc: 0.9558013677597046\n",
      "Epoch: 978, Loss: 0.04395050182938576, Acc: 0.9554818868637085\n",
      "Epoch: 979, Loss: 0.04401753097772598, Acc: 0.955772340297699\n",
      "Epoch: 980, Loss: 0.04443415254354477, Acc: 0.9558449387550354\n",
      "Epoch: 981, Loss: 0.04665618762373924, Acc: 0.9560033679008484\n",
      "Epoch: 982, Loss: 0.04573140665888786, Acc: 0.9558409452438354\n",
      "Epoch: 983, Loss: 0.04370546340942383, Acc: 0.955781102180481\n",
      "Epoch: 984, Loss: 0.048717107623815536, Acc: 0.9559447765350342\n",
      "Epoch: 985, Loss: 0.04454047232866287, Acc: 0.9563526511192322\n",
      "Epoch: 986, Loss: 0.043439362198114395, Acc: 0.9557753801345825\n",
      "Epoch: 987, Loss: 0.04896292835474014, Acc: 0.9558793306350708\n",
      "Epoch: 988, Loss: 0.04302915930747986, Acc: 0.955767810344696\n",
      "Epoch: 989, Loss: 0.04441659152507782, Acc: 0.9560401439666748\n",
      "Epoch: 990, Loss: 0.04305514320731163, Acc: 0.9559054374694824\n",
      "Epoch: 991, Loss: 0.042289864271879196, Acc: 0.9561155438423157\n",
      "Epoch: 992, Loss: 0.04261741414666176, Acc: 0.956031322479248\n",
      "Epoch: 993, Loss: 0.043857917189598083, Acc: 0.9558974504470825\n",
      "Epoch: 994, Loss: 0.043298766016960144, Acc: 0.9561012387275696\n",
      "Epoch: 995, Loss: 0.045380428433418274, Acc: 0.9564197063446045\n",
      "Epoch: 996, Loss: 0.043885812163352966, Acc: 0.9561763405799866\n",
      "Epoch: 997, Loss: 0.04429430514574051, Acc: 0.9560315012931824\n",
      "Epoch: 998, Loss: 0.051145754754543304, Acc: 0.9572799205780029\n",
      "Epoch: 999, Loss: 0.043681759387254715, Acc: 0.9563727974891663\n",
      "Epoch: 1000, Loss: 0.04454343020915985, Acc: 0.9565932750701904\n",
      "Epoch: 1001, Loss: 0.044321559369564056, Acc: 0.9560979604721069\n",
      "Epoch: 1002, Loss: 0.043352749198675156, Acc: 0.9564773440361023\n",
      "Epoch: 1003, Loss: 0.04172859340906143, Acc: 0.9560925960540771\n",
      "Epoch: 1004, Loss: 0.043080415576696396, Acc: 0.9565964937210083\n",
      "Epoch: 1005, Loss: 0.04394764453172684, Acc: 0.9563624858856201\n",
      "Epoch: 1006, Loss: 0.04326250031590462, Acc: 0.9561286568641663\n",
      "Epoch: 1007, Loss: 0.04841555655002594, Acc: 0.9564651846885681\n",
      "Epoch: 1008, Loss: 0.04983575642108917, Acc: 0.9568429589271545\n",
      "Epoch: 1009, Loss: 0.04521697759628296, Acc: 0.9567434191703796\n",
      "Epoch: 1010, Loss: 0.04370611906051636, Acc: 0.9568213820457458\n",
      "Epoch: 1011, Loss: 0.04271809756755829, Acc: 0.9564204812049866\n",
      "Epoch: 1012, Loss: 0.046701230108737946, Acc: 0.9570510387420654\n",
      "Epoch: 1013, Loss: 0.043140292167663574, Acc: 0.9566401839256287\n",
      "Epoch: 1014, Loss: 0.044264864176511765, Acc: 0.9566394686698914\n",
      "Epoch: 1015, Loss: 0.042282529175281525, Acc: 0.956541121006012\n",
      "Epoch: 1016, Loss: 0.04322327673435211, Acc: 0.9565030932426453\n",
      "Epoch: 1017, Loss: 0.04254455491900444, Acc: 0.9564420580863953\n",
      "Epoch: 1018, Loss: 0.04378226399421692, Acc: 0.9564241766929626\n",
      "Epoch: 1019, Loss: 0.04608608037233353, Acc: 0.9566899538040161\n",
      "Epoch: 1020, Loss: 0.042965441942214966, Acc: 0.9567915201187134\n",
      "Epoch: 1021, Loss: 0.04362257942557335, Acc: 0.9566381573677063\n",
      "Epoch: 1022, Loss: 0.04352385178208351, Acc: 0.9572792053222656\n",
      "Epoch: 1023, Loss: 0.04724584519863129, Acc: 0.9570179581642151\n",
      "Epoch: 1024, Loss: 0.04826899617910385, Acc: 0.9567784667015076\n",
      "Epoch: 1025, Loss: 0.04364670813083649, Acc: 0.9568315744400024\n",
      "Epoch: 1026, Loss: 0.04493214935064316, Acc: 0.9570114016532898\n",
      "Epoch: 1027, Loss: 0.042779237031936646, Acc: 0.957015335559845\n",
      "Epoch: 1028, Loss: 0.0436922125518322, Acc: 0.9570432305335999\n",
      "Epoch: 1029, Loss: 0.046004801988601685, Acc: 0.9567375779151917\n",
      "Epoch: 1030, Loss: 0.047207027673721313, Acc: 0.9568579792976379\n",
      "Epoch: 1031, Loss: 0.04532269388437271, Acc: 0.9571300745010376\n",
      "Epoch: 1032, Loss: 0.04456093907356262, Acc: 0.957354724407196\n",
      "Epoch: 1033, Loss: 0.04768715053796768, Acc: 0.9573312401771545\n",
      "Epoch: 1034, Loss: 0.04148952290415764, Acc: 0.9568101167678833\n",
      "Epoch: 1035, Loss: 0.045117150992155075, Acc: 0.9571229219436646\n",
      "Epoch: 1036, Loss: 0.04311412572860718, Acc: 0.958017885684967\n",
      "Epoch: 1037, Loss: 0.046370021998882294, Acc: 0.9570167064666748\n",
      "Epoch: 1038, Loss: 0.04662582278251648, Acc: 0.9572523832321167\n",
      "Epoch: 1039, Loss: 0.04287469759583473, Acc: 0.9572733640670776\n",
      "Epoch: 1040, Loss: 0.04291225224733353, Acc: 0.9572372436523438\n",
      "Epoch: 1041, Loss: 0.041886910796165466, Acc: 0.9570512175559998\n",
      "Epoch: 1042, Loss: 0.047808222472667694, Acc: 0.9581111669540405\n",
      "Epoch: 1043, Loss: 0.0504811629652977, Acc: 0.9575007557868958\n",
      "Epoch: 1044, Loss: 0.047651082277297974, Acc: 0.9570944905281067\n",
      "Epoch: 1045, Loss: 0.047034069895744324, Acc: 0.9575449228286743\n",
      "Epoch: 1046, Loss: 0.04242090508341789, Acc: 0.9575104117393494\n",
      "Epoch: 1047, Loss: 0.04137266054749489, Acc: 0.9572595953941345\n",
      "Epoch: 1048, Loss: 0.04252893105149269, Acc: 0.9571849703788757\n",
      "Epoch: 1049, Loss: 0.04417423531413078, Acc: 0.957641065120697\n",
      "Epoch: 1050, Loss: 0.04240588843822479, Acc: 0.9579347372055054\n",
      "Epoch: 1051, Loss: 0.04544021561741829, Acc: 0.9574379920959473\n",
      "Epoch: 1052, Loss: 0.04107247665524483, Acc: 0.957767903804779\n",
      "Epoch: 1053, Loss: 0.045420870184898376, Acc: 0.9573594927787781\n",
      "Epoch: 1054, Loss: 0.0437207892537117, Acc: 0.9576430916786194\n",
      "Epoch: 1055, Loss: 0.0431831032037735, Acc: 0.958053708076477\n",
      "Epoch: 1056, Loss: 0.042951442301273346, Acc: 0.957281768321991\n",
      "Epoch: 1057, Loss: 0.04279450699687004, Acc: 0.9576088190078735\n",
      "Epoch: 1058, Loss: 0.0413191057741642, Acc: 0.9576330184936523\n",
      "Epoch: 1059, Loss: 0.04111562669277191, Acc: 0.9576552510261536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1060, Loss: 0.04197647050023079, Acc: 0.9577915668487549\n",
      "Epoch: 1061, Loss: 0.04320617392659187, Acc: 0.9576926827430725\n",
      "Epoch: 1062, Loss: 0.04276007041335106, Acc: 0.9579102396965027\n",
      "Epoch: 1063, Loss: 0.046360548585653305, Acc: 0.958138108253479\n",
      "Epoch: 1064, Loss: 0.042787764221429825, Acc: 0.9576410055160522\n",
      "Epoch: 1065, Loss: 0.043788544833660126, Acc: 0.9575226902961731\n",
      "Epoch: 1066, Loss: 0.040714479982852936, Acc: 0.9575331807136536\n",
      "Epoch: 1067, Loss: 0.04637376219034195, Acc: 0.9577609896659851\n",
      "Epoch: 1068, Loss: 0.041677866131067276, Acc: 0.9580589532852173\n",
      "Epoch: 1069, Loss: 0.0473497100174427, Acc: 0.9585845470428467\n",
      "Epoch: 1070, Loss: 0.04294339567422867, Acc: 0.958307147026062\n",
      "Epoch: 1071, Loss: 0.04619130119681358, Acc: 0.957909345626831\n",
      "Epoch: 1072, Loss: 0.042222797870635986, Acc: 0.9579765796661377\n",
      "Epoch: 1073, Loss: 0.04774715006351471, Acc: 0.9577188491821289\n",
      "Epoch: 1074, Loss: 0.04869195818901062, Acc: 0.9581979513168335\n",
      "Epoch: 1075, Loss: 0.04184877127408981, Acc: 0.9578437805175781\n",
      "Epoch: 1076, Loss: 0.041393205523490906, Acc: 0.9577516913414001\n",
      "Epoch: 1077, Loss: 0.04231468588113785, Acc: 0.9584113955497742\n",
      "Epoch: 1078, Loss: 0.04290996119379997, Acc: 0.957960844039917\n",
      "Epoch: 1079, Loss: 0.04364849999547005, Acc: 0.9578427076339722\n",
      "Epoch: 1080, Loss: 0.04515194520354271, Acc: 0.9581286311149597\n",
      "Epoch: 1081, Loss: 0.044228918850421906, Acc: 0.9584521055221558\n",
      "Epoch: 1082, Loss: 0.04485487937927246, Acc: 0.9581287503242493\n",
      "Epoch: 1083, Loss: 0.04162997007369995, Acc: 0.9579325914382935\n",
      "Epoch: 1084, Loss: 0.04138243943452835, Acc: 0.9582688808441162\n",
      "Epoch: 1085, Loss: 0.043196678161621094, Acc: 0.9582964181900024\n",
      "Epoch: 1086, Loss: 0.042939551174640656, Acc: 0.9581255912780762\n",
      "Epoch: 1087, Loss: 0.04070472717285156, Acc: 0.9579859375953674\n",
      "Epoch: 1088, Loss: 0.04056183248758316, Acc: 0.9579812288284302\n",
      "Epoch: 1089, Loss: 0.04530125856399536, Acc: 0.9579635858535767\n",
      "Epoch: 1090, Loss: 0.040259018540382385, Acc: 0.9580432772636414\n",
      "Epoch: 1091, Loss: 0.04606591910123825, Acc: 0.9585886597633362\n",
      "Epoch: 1092, Loss: 0.04273774474859238, Acc: 0.9582152366638184\n",
      "Epoch: 1093, Loss: 0.04527082294225693, Acc: 0.9583536386489868\n",
      "Epoch: 1094, Loss: 0.039801858365535736, Acc: 0.9581101536750793\n",
      "Epoch: 1095, Loss: 0.04074524715542793, Acc: 0.9580618143081665\n",
      "Epoch: 1096, Loss: 0.046262480318546295, Acc: 0.9584549069404602\n",
      "Epoch: 1097, Loss: 0.03996364399790764, Acc: 0.9583185315132141\n",
      "Epoch: 1098, Loss: 0.044517118483781815, Acc: 0.9584319591522217\n",
      "Epoch: 1099, Loss: 0.04375750571489334, Acc: 0.9584465622901917\n",
      "Epoch: 1100, Loss: 0.0398794561624527, Acc: 0.9581590294837952\n",
      "Epoch: 1101, Loss: 0.045943744480609894, Acc: 0.9587809443473816\n",
      "Epoch: 1102, Loss: 0.04038412868976593, Acc: 0.958229124546051\n",
      "Epoch: 1103, Loss: 0.04139173403382301, Acc: 0.958450973033905\n",
      "Epoch: 1104, Loss: 0.04081019014120102, Acc: 0.9584951996803284\n",
      "Epoch: 1105, Loss: 0.0412566177546978, Acc: 0.9584400057792664\n",
      "Epoch: 1106, Loss: 0.04120400920510292, Acc: 0.9587313532829285\n",
      "Epoch: 1107, Loss: 0.04146546125411987, Acc: 0.9584724307060242\n",
      "Epoch: 1108, Loss: 0.04023892059922218, Acc: 0.9586119651794434\n",
      "Epoch: 1109, Loss: 0.046507883816957474, Acc: 0.9592263698577881\n",
      "Epoch: 1110, Loss: 0.041149236261844635, Acc: 0.9587070345878601\n",
      "Epoch: 1111, Loss: 0.041474323719739914, Acc: 0.9587640166282654\n",
      "Epoch: 1112, Loss: 0.04142970219254494, Acc: 0.9586634635925293\n",
      "Epoch: 1113, Loss: 0.0472695417702198, Acc: 0.9587560892105103\n",
      "Epoch: 1114, Loss: 0.04132222384214401, Acc: 0.9587731957435608\n",
      "Epoch: 1115, Loss: 0.04509645700454712, Acc: 0.9591547846794128\n",
      "Epoch: 1116, Loss: 0.042338863015174866, Acc: 0.9587047696113586\n",
      "Epoch: 1117, Loss: 0.04008644446730614, Acc: 0.9585639238357544\n",
      "Epoch: 1118, Loss: 0.04011911898851395, Acc: 0.9587268233299255\n",
      "Epoch: 1119, Loss: 0.041156187653541565, Acc: 0.9590668082237244\n",
      "Epoch: 1120, Loss: 0.04075723886489868, Acc: 0.9588165283203125\n",
      "Epoch: 1121, Loss: 0.04205531254410744, Acc: 0.9588689208030701\n",
      "Epoch: 1122, Loss: 0.04023012891411781, Acc: 0.958864152431488\n",
      "Epoch: 1123, Loss: 0.04397955536842346, Acc: 0.9589487314224243\n",
      "Epoch: 1124, Loss: 0.0407199002802372, Acc: 0.9589017629623413\n",
      "Epoch: 1125, Loss: 0.03997402265667915, Acc: 0.9586834907531738\n",
      "Epoch: 1126, Loss: 0.04110516980290413, Acc: 0.9590551257133484\n",
      "Epoch: 1127, Loss: 0.04143587872385979, Acc: 0.9587282538414001\n",
      "Epoch: 1128, Loss: 0.043377652764320374, Acc: 0.9590924382209778\n",
      "Epoch: 1129, Loss: 0.04135938361287117, Acc: 0.959186315536499\n",
      "Epoch: 1130, Loss: 0.04010382667183876, Acc: 0.9587981104850769\n",
      "Epoch: 1131, Loss: 0.04518720507621765, Acc: 0.9595215320587158\n",
      "Epoch: 1132, Loss: 0.04143979772925377, Acc: 0.9588873982429504\n",
      "Epoch: 1133, Loss: 0.040908083319664, Acc: 0.958890438079834\n",
      "Epoch: 1134, Loss: 0.04119095206260681, Acc: 0.9588939547538757\n",
      "Epoch: 1135, Loss: 0.04336508736014366, Acc: 0.9589674472808838\n",
      "Epoch: 1136, Loss: 0.040706880390644073, Acc: 0.9593547582626343\n",
      "Epoch: 1137, Loss: 0.042456407099962234, Acc: 0.9593319296836853\n",
      "Epoch: 1138, Loss: 0.04074705392122269, Acc: 0.9588814377784729\n",
      "Epoch: 1139, Loss: 0.04058999940752983, Acc: 0.9590439796447754\n",
      "Epoch: 1140, Loss: 0.04332370311021805, Acc: 0.9592031836509705\n",
      "Epoch: 1141, Loss: 0.038953281939029694, Acc: 0.9589669108390808\n",
      "Epoch: 1142, Loss: 0.04429705813527107, Acc: 0.95924973487854\n",
      "Epoch: 1143, Loss: 0.04139355197548866, Acc: 0.9591622352600098\n",
      "Epoch: 1144, Loss: 0.040420517325401306, Acc: 0.9592183828353882\n",
      "Epoch: 1145, Loss: 0.04233381524682045, Acc: 0.9591874480247498\n",
      "Epoch: 1146, Loss: 0.03887880593538284, Acc: 0.9591519832611084\n",
      "Epoch: 1147, Loss: 0.04071125388145447, Acc: 0.9596861004829407\n",
      "Epoch: 1148, Loss: 0.041743189096450806, Acc: 0.9594036936759949\n",
      "Epoch: 1149, Loss: 0.03931659832596779, Acc: 0.9592776894569397\n",
      "Epoch: 1150, Loss: 0.044335730373859406, Acc: 0.9593661427497864\n",
      "Epoch: 1151, Loss: 0.042411964386701584, Acc: 0.9593525528907776\n",
      "Epoch: 1152, Loss: 0.040225327014923096, Acc: 0.9593290090560913\n",
      "Epoch: 1153, Loss: 0.04510203003883362, Acc: 0.9596593976020813\n",
      "Epoch: 1154, Loss: 0.038792166858911514, Acc: 0.9591169357299805\n",
      "Epoch: 1155, Loss: 0.043899912387132645, Acc: 0.9594348073005676\n",
      "Epoch: 1156, Loss: 0.04002098739147186, Acc: 0.9600788354873657\n",
      "Epoch: 1157, Loss: 0.04190988093614578, Acc: 0.9596297144889832\n",
      "Epoch: 1158, Loss: 0.042275507003068924, Acc: 0.9596023559570312\n",
      "Epoch: 1159, Loss: 0.043348148465156555, Acc: 0.9595118761062622\n",
      "Epoch: 1160, Loss: 0.04293593764305115, Acc: 0.9596149921417236\n",
      "Epoch: 1161, Loss: 0.039665136486291885, Acc: 0.9593820571899414\n",
      "Epoch: 1162, Loss: 0.04314344376325607, Acc: 0.9594572186470032\n",
      "Epoch: 1163, Loss: 0.04151887074112892, Acc: 0.9593935012817383\n",
      "Epoch: 1164, Loss: 0.04194032773375511, Acc: 0.9596701264381409\n",
      "Epoch: 1165, Loss: 0.04057972878217697, Acc: 0.9597046375274658\n",
      "Epoch: 1166, Loss: 0.04059750214219093, Acc: 0.9598771929740906\n",
      "Epoch: 1167, Loss: 0.03968155011534691, Acc: 0.9595679044723511\n",
      "Epoch: 1168, Loss: 0.0412704274058342, Acc: 0.959434986114502\n",
      "Epoch: 1169, Loss: 0.04019393399357796, Acc: 0.9599990844726562\n",
      "Epoch: 1170, Loss: 0.04320429265499115, Acc: 0.9595531225204468\n",
      "Epoch: 1171, Loss: 0.03964225575327873, Acc: 0.9596853256225586\n",
      "Epoch: 1172, Loss: 0.03994840383529663, Acc: 0.9595025181770325\n",
      "Epoch: 1173, Loss: 0.03966199979186058, Acc: 0.9598383903503418\n",
      "Epoch: 1174, Loss: 0.04151463881134987, Acc: 0.9598930478096008\n",
      "Epoch: 1175, Loss: 0.039267681539058685, Acc: 0.9596418738365173\n",
      "Epoch: 1176, Loss: 0.03998693823814392, Acc: 0.9598986506462097\n",
      "Epoch: 1177, Loss: 0.04506690055131912, Acc: 0.9598406553268433\n",
      "Epoch: 1178, Loss: 0.04048534110188484, Acc: 0.9596688747406006\n",
      "Epoch: 1179, Loss: 0.041758883744478226, Acc: 0.9601612091064453\n",
      "Epoch: 1180, Loss: 0.040005628019571304, Acc: 0.959945023059845\n",
      "Epoch: 1181, Loss: 0.03875875473022461, Acc: 0.9601598381996155\n",
      "Epoch: 1182, Loss: 0.041014693677425385, Acc: 0.95972740650177\n",
      "Epoch: 1183, Loss: 0.043096475303173065, Acc: 0.960186243057251\n",
      "Epoch: 1184, Loss: 0.04011785238981247, Acc: 0.9600491523742676\n",
      "Epoch: 1185, Loss: 0.03879781812429428, Acc: 0.9597700834274292\n",
      "Epoch: 1186, Loss: 0.04242180287837982, Acc: 0.9605364799499512\n",
      "Epoch: 1187, Loss: 0.040125779807567596, Acc: 0.9602616429328918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1188, Loss: 0.04231022670865059, Acc: 0.9600300788879395\n",
      "Epoch: 1189, Loss: 0.039463698863983154, Acc: 0.9599317908287048\n",
      "Epoch: 1190, Loss: 0.04089133068919182, Acc: 0.960344672203064\n",
      "Epoch: 1191, Loss: 0.0410623736679554, Acc: 0.9600252509117126\n",
      "Epoch: 1192, Loss: 0.038919635117053986, Acc: 0.9603235721588135\n",
      "Epoch: 1193, Loss: 0.04051685333251953, Acc: 0.9598429203033447\n",
      "Epoch: 1194, Loss: 0.04033074527978897, Acc: 0.9600397348403931\n",
      "Epoch: 1195, Loss: 0.04072115570306778, Acc: 0.9605698585510254\n",
      "Epoch: 1196, Loss: 0.03823085129261017, Acc: 0.9601638317108154\n",
      "Epoch: 1197, Loss: 0.04060857743024826, Acc: 0.9604958891868591\n",
      "Epoch: 1198, Loss: 0.039621978998184204, Acc: 0.9601490497589111\n",
      "Epoch: 1199, Loss: 0.03994259983301163, Acc: 0.9603786468505859\n",
      "Epoch: 1200, Loss: 0.03821659833192825, Acc: 0.9601097702980042\n",
      "Epoch: 1201, Loss: 0.038959354162216187, Acc: 0.960352897644043\n",
      "Epoch: 1202, Loss: 0.04106931760907173, Acc: 0.9602228999137878\n",
      "Epoch: 1203, Loss: 0.03965378552675247, Acc: 0.9602081179618835\n",
      "Epoch: 1204, Loss: 0.03938614949584007, Acc: 0.9603992104530334\n",
      "Epoch: 1205, Loss: 0.0395081490278244, Acc: 0.9603031873703003\n",
      "Epoch: 1206, Loss: 0.041125375777482986, Acc: 0.9602677226066589\n",
      "Epoch: 1207, Loss: 0.039764102548360825, Acc: 0.9603140950202942\n",
      "Epoch: 1208, Loss: 0.03956681489944458, Acc: 0.9604261517524719\n",
      "Epoch: 1209, Loss: 0.038942087441682816, Acc: 0.9604337811470032\n",
      "Epoch: 1210, Loss: 0.04004935920238495, Acc: 0.9604318141937256\n",
      "Epoch: 1211, Loss: 0.03993922844529152, Acc: 0.9602200984954834\n",
      "Epoch: 1212, Loss: 0.03919830918312073, Acc: 0.9606793522834778\n",
      "Epoch: 1213, Loss: 0.039679624140262604, Acc: 0.9603806138038635\n",
      "Epoch: 1214, Loss: 0.03926031291484833, Acc: 0.9604055881500244\n",
      "Epoch: 1215, Loss: 0.03975493460893631, Acc: 0.9606175422668457\n",
      "Epoch: 1216, Loss: 0.038014523684978485, Acc: 0.9602673649787903\n",
      "Epoch: 1217, Loss: 0.0381576269865036, Acc: 0.9603932499885559\n",
      "Epoch: 1218, Loss: 0.03901827335357666, Acc: 0.9608715176582336\n",
      "Epoch: 1219, Loss: 0.04018358513712883, Acc: 0.9602971076965332\n",
      "Epoch: 1220, Loss: 0.0402412973344326, Acc: 0.9603349566459656\n",
      "Epoch: 1221, Loss: 0.03970136120915413, Acc: 0.9607980251312256\n",
      "Epoch: 1222, Loss: 0.03983808308839798, Acc: 0.960503876209259\n",
      "Epoch: 1223, Loss: 0.039098843932151794, Acc: 0.9605804085731506\n",
      "Epoch: 1224, Loss: 0.04112263396382332, Acc: 0.9607415199279785\n",
      "Epoch: 1225, Loss: 0.03943660110235214, Acc: 0.9605851769447327\n",
      "Epoch: 1226, Loss: 0.03867483511567116, Acc: 0.9606743454933167\n",
      "Epoch: 1227, Loss: 0.037590038031339645, Acc: 0.9603342413902283\n",
      "Epoch: 1228, Loss: 0.03903914615511894, Acc: 0.9608527421951294\n",
      "Epoch: 1229, Loss: 0.044102415442466736, Acc: 0.9608898758888245\n",
      "Epoch: 1230, Loss: 0.040094662457704544, Acc: 0.9606766104698181\n",
      "Epoch: 1231, Loss: 0.03905516490340233, Acc: 0.9607043266296387\n",
      "Epoch: 1232, Loss: 0.04032518342137337, Acc: 0.9607887864112854\n",
      "Epoch: 1233, Loss: 0.03802166134119034, Acc: 0.9606888890266418\n",
      "Epoch: 1234, Loss: 0.039109375327825546, Acc: 0.960800290107727\n",
      "Epoch: 1235, Loss: 0.03944580629467964, Acc: 0.9608555436134338\n",
      "Epoch: 1236, Loss: 0.03896410018205643, Acc: 0.9609054923057556\n",
      "Epoch: 1237, Loss: 0.0393223911523819, Acc: 0.9606896042823792\n",
      "Epoch: 1238, Loss: 0.042850054800510406, Acc: 0.9609931111335754\n",
      "Epoch: 1239, Loss: 0.042925428599119186, Acc: 0.9607881903648376\n",
      "Epoch: 1240, Loss: 0.03870821371674538, Acc: 0.9605584740638733\n",
      "Epoch: 1241, Loss: 0.03908524662256241, Acc: 0.9608857035636902\n",
      "Epoch: 1242, Loss: 0.03980991989374161, Acc: 0.9610651135444641\n",
      "Epoch: 1243, Loss: 0.04321561008691788, Acc: 0.9610308408737183\n",
      "Epoch: 1244, Loss: 0.04212305694818497, Acc: 0.9607393145561218\n",
      "Epoch: 1245, Loss: 0.04400409013032913, Acc: 0.9615615606307983\n",
      "Epoch: 1246, Loss: 0.040529947727918625, Acc: 0.9612941741943359\n",
      "Epoch: 1247, Loss: 0.04020242020487785, Acc: 0.9607148766517639\n",
      "Epoch: 1248, Loss: 0.03927081823348999, Acc: 0.9613902568817139\n",
      "Epoch: 1249, Loss: 0.03743534907698631, Acc: 0.9608716368675232\n",
      "Epoch: 1250, Loss: 0.03827719762921333, Acc: 0.9608331918716431\n",
      "Epoch: 1251, Loss: 0.03866845741868019, Acc: 0.9611611366271973\n",
      "Epoch: 1252, Loss: 0.04169482737779617, Acc: 0.9609037041664124\n",
      "Epoch: 1253, Loss: 0.03872394561767578, Acc: 0.9607871174812317\n",
      "Epoch: 1254, Loss: 0.03881962224841118, Acc: 0.9611606001853943\n",
      "Epoch: 1255, Loss: 0.03897382318973541, Acc: 0.9611358642578125\n",
      "Epoch: 1256, Loss: 0.042568057775497437, Acc: 0.9609214663505554\n",
      "Epoch: 1257, Loss: 0.0403553768992424, Acc: 0.9611740112304688\n",
      "Epoch: 1258, Loss: 0.03845546394586563, Acc: 0.9618431329727173\n",
      "Epoch: 1259, Loss: 0.038499534130096436, Acc: 0.96097731590271\n",
      "Epoch: 1260, Loss: 0.04317209869623184, Acc: 0.9611915946006775\n",
      "Epoch: 1261, Loss: 0.03973861038684845, Acc: 0.9609020352363586\n",
      "Epoch: 1262, Loss: 0.039660293608903885, Acc: 0.9612947702407837\n",
      "Epoch: 1263, Loss: 0.043466947972774506, Acc: 0.9613308906555176\n",
      "Epoch: 1264, Loss: 0.041575830429792404, Acc: 0.9611555337905884\n",
      "Epoch: 1265, Loss: 0.03739127144217491, Acc: 0.9612473249435425\n",
      "Epoch: 1266, Loss: 0.04334046691656113, Acc: 0.9612732529640198\n",
      "Epoch: 1267, Loss: 0.03994237631559372, Acc: 0.9615777134895325\n",
      "Epoch: 1268, Loss: 0.04369409382343292, Acc: 0.9612328410148621\n",
      "Epoch: 1269, Loss: 0.04204072430729866, Acc: 0.9612875580787659\n",
      "Epoch: 1270, Loss: 0.03747659921646118, Acc: 0.961054265499115\n",
      "Epoch: 1271, Loss: 0.03796231746673584, Acc: 0.9611520767211914\n",
      "Epoch: 1272, Loss: 0.0417914092540741, Acc: 0.9614514112472534\n",
      "Epoch: 1273, Loss: 0.038157276809215546, Acc: 0.9618427753448486\n",
      "Epoch: 1274, Loss: 0.042865484952926636, Acc: 0.9616937041282654\n",
      "Epoch: 1275, Loss: 0.03977683186531067, Acc: 0.9614377021789551\n",
      "Epoch: 1276, Loss: 0.03780877962708473, Acc: 0.9615087509155273\n",
      "Epoch: 1277, Loss: 0.03794035315513611, Acc: 0.9620192050933838\n",
      "Epoch: 1278, Loss: 0.039315156638622284, Acc: 0.9613760709762573\n",
      "Epoch: 1279, Loss: 0.04269351810216904, Acc: 0.9616134762763977\n",
      "Epoch: 1280, Loss: 0.0399957075715065, Acc: 0.9612399935722351\n",
      "Epoch: 1281, Loss: 0.038232456892728806, Acc: 0.961393415927887\n",
      "Epoch: 1282, Loss: 0.04018666222691536, Acc: 0.9613789319992065\n",
      "Epoch: 1283, Loss: 0.041377581655979156, Acc: 0.9614568948745728\n",
      "Epoch: 1284, Loss: 0.03731657937169075, Acc: 0.9618644714355469\n",
      "Epoch: 1285, Loss: 0.03734028339385986, Acc: 0.9612838625907898\n",
      "Epoch: 1286, Loss: 0.03682852163910866, Acc: 0.9614022374153137\n",
      "Epoch: 1287, Loss: 0.04094475507736206, Acc: 0.9618465304374695\n",
      "Epoch: 1288, Loss: 0.03869912773370743, Acc: 0.9616951942443848\n",
      "Epoch: 1289, Loss: 0.03775184974074364, Acc: 0.9613806009292603\n",
      "Epoch: 1290, Loss: 0.04035348445177078, Acc: 0.9615570306777954\n",
      "Epoch: 1291, Loss: 0.0368071049451828, Acc: 0.9614684581756592\n",
      "Epoch: 1292, Loss: 0.03780335560441017, Acc: 0.9621214866638184\n",
      "Epoch: 1293, Loss: 0.04009757936000824, Acc: 0.9616173505783081\n",
      "Epoch: 1294, Loss: 0.0383344367146492, Acc: 0.9616470336914062\n",
      "Epoch: 1295, Loss: 0.04134926572442055, Acc: 0.9616329669952393\n",
      "Epoch: 1296, Loss: 0.037573929876089096, Acc: 0.9618735909461975\n",
      "Epoch: 1297, Loss: 0.036625709384679794, Acc: 0.9614390134811401\n",
      "Epoch: 1298, Loss: 0.04289430379867554, Acc: 0.961887001991272\n",
      "Epoch: 1299, Loss: 0.03679100424051285, Acc: 0.9617385268211365\n",
      "Epoch: 1300, Loss: 0.03826887160539627, Acc: 0.9618498086929321\n",
      "Epoch: 1301, Loss: 0.03900310769677162, Acc: 0.9615004062652588\n",
      "Epoch: 1302, Loss: 0.03884159028530121, Acc: 0.9615898132324219\n",
      "Epoch: 1303, Loss: 0.037083689123392105, Acc: 0.9615719318389893\n",
      "Epoch: 1304, Loss: 0.03731125220656395, Acc: 0.9618017673492432\n",
      "Epoch: 1305, Loss: 0.03733978047966957, Acc: 0.9618963599205017\n",
      "Epoch: 1306, Loss: 0.03815509006381035, Acc: 0.9621223211288452\n",
      "Epoch: 1307, Loss: 0.040436215698719025, Acc: 0.9619850516319275\n",
      "Epoch: 1308, Loss: 0.041364118456840515, Acc: 0.9616249799728394\n",
      "Epoch: 1309, Loss: 0.038338690996170044, Acc: 0.9620978236198425\n",
      "Epoch: 1310, Loss: 0.036850396543741226, Acc: 0.9615956544876099\n",
      "Epoch: 1311, Loss: 0.04224418103694916, Acc: 0.962192714214325\n",
      "Epoch: 1312, Loss: 0.0385948121547699, Acc: 0.9616867899894714\n",
      "Epoch: 1313, Loss: 0.04059385508298874, Acc: 0.961981475353241\n",
      "Epoch: 1314, Loss: 0.03823203593492508, Acc: 0.9617831707000732\n",
      "Epoch: 1315, Loss: 0.03835991024971008, Acc: 0.9627893567085266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1316, Loss: 0.03867260366678238, Acc: 0.9621997475624084\n",
      "Epoch: 1317, Loss: 0.03894815593957901, Acc: 0.9618508219718933\n",
      "Epoch: 1318, Loss: 0.038558658212423325, Acc: 0.9625681042671204\n",
      "Epoch: 1319, Loss: 0.0411248579621315, Acc: 0.9619047045707703\n",
      "Epoch: 1320, Loss: 0.04185827076435089, Acc: 0.9621927738189697\n",
      "Epoch: 1321, Loss: 0.036365609616041183, Acc: 0.9619978666305542\n",
      "Epoch: 1322, Loss: 0.041270479559898376, Acc: 0.9619336724281311\n",
      "Epoch: 1323, Loss: 0.03836054727435112, Acc: 0.961994469165802\n",
      "Epoch: 1324, Loss: 0.039248719811439514, Acc: 0.9626588821411133\n",
      "Epoch: 1325, Loss: 0.03773452341556549, Acc: 0.9619327783584595\n",
      "Epoch: 1326, Loss: 0.03797797113656998, Acc: 0.9619006514549255\n",
      "Epoch: 1327, Loss: 0.03731004148721695, Acc: 0.9622682332992554\n",
      "Epoch: 1328, Loss: 0.03740378096699715, Acc: 0.9622653126716614\n",
      "Epoch: 1329, Loss: 0.037600528448820114, Acc: 0.9620897173881531\n",
      "Epoch: 1330, Loss: 0.0387168787419796, Acc: 0.9622297286987305\n",
      "Epoch: 1331, Loss: 0.036832939833402634, Acc: 0.9619209170341492\n",
      "Epoch: 1332, Loss: 0.04306358844041824, Acc: 0.9623044729232788\n",
      "Epoch: 1333, Loss: 0.04100068658590317, Acc: 0.9626761078834534\n",
      "Epoch: 1334, Loss: 0.03741268068552017, Acc: 0.9620885848999023\n",
      "Epoch: 1335, Loss: 0.03707849606871605, Acc: 0.962555468082428\n",
      "Epoch: 1336, Loss: 0.0402614027261734, Acc: 0.9623388051986694\n",
      "Epoch: 1337, Loss: 0.0388193242251873, Acc: 0.9623609185218811\n",
      "Epoch: 1338, Loss: 0.03715413063764572, Acc: 0.9620566964149475\n",
      "Epoch: 1339, Loss: 0.03781609237194061, Acc: 0.962618350982666\n",
      "Epoch: 1340, Loss: 0.03794819116592407, Acc: 0.962638795375824\n",
      "Epoch: 1341, Loss: 0.03831813484430313, Acc: 0.9623036980628967\n",
      "Epoch: 1342, Loss: 0.03673947602510452, Acc: 0.9622876048088074\n",
      "Epoch: 1343, Loss: 0.037040628492832184, Acc: 0.9619806408882141\n",
      "Epoch: 1344, Loss: 0.03771248832345009, Acc: 0.9623132944107056\n",
      "Epoch: 1345, Loss: 0.03880412131547928, Acc: 0.9622055888175964\n",
      "Epoch: 1346, Loss: 0.0414402075111866, Acc: 0.962237536907196\n",
      "Epoch: 1347, Loss: 0.03763432428240776, Acc: 0.9628076553344727\n",
      "Epoch: 1348, Loss: 0.04073011130094528, Acc: 0.9622566103935242\n",
      "Epoch: 1349, Loss: 0.0379880890250206, Acc: 0.9625112414360046\n",
      "Epoch: 1350, Loss: 0.03699508309364319, Acc: 0.9622286558151245\n",
      "Epoch: 1351, Loss: 0.036461737006902695, Acc: 0.9624258875846863\n",
      "Epoch: 1352, Loss: 0.036745261400938034, Acc: 0.9626531600952148\n",
      "Epoch: 1353, Loss: 0.03826681897044182, Acc: 0.962462842464447\n",
      "Epoch: 1354, Loss: 0.040233924984931946, Acc: 0.9624713063240051\n",
      "Epoch: 1355, Loss: 0.04298483580350876, Acc: 0.9629285335540771\n",
      "Epoch: 1356, Loss: 0.03987546265125275, Acc: 0.962585985660553\n",
      "Epoch: 1357, Loss: 0.037377238273620605, Acc: 0.9624530673027039\n",
      "Epoch: 1358, Loss: 0.03647793456912041, Acc: 0.9624579548835754\n",
      "Epoch: 1359, Loss: 0.04276596009731293, Acc: 0.9630233645439148\n",
      "Epoch: 1360, Loss: 0.042798448354005814, Acc: 0.9624552726745605\n",
      "Epoch: 1361, Loss: 0.03716246783733368, Acc: 0.9624225497245789\n",
      "Epoch: 1362, Loss: 0.03993458300828934, Acc: 0.9628637433052063\n",
      "Epoch: 1363, Loss: 0.03651375323534012, Acc: 0.9625594615936279\n",
      "Epoch: 1364, Loss: 0.041297584772109985, Acc: 0.9626932740211487\n",
      "Epoch: 1365, Loss: 0.03858720138669014, Acc: 0.9625274538993835\n",
      "Epoch: 1366, Loss: 0.03730075806379318, Acc: 0.9625381231307983\n",
      "Epoch: 1367, Loss: 0.04044948145747185, Acc: 0.9629862308502197\n",
      "Epoch: 1368, Loss: 0.03869382664561272, Acc: 0.9628528356552124\n",
      "Epoch: 1369, Loss: 0.04232802987098694, Acc: 0.9628089666366577\n",
      "Epoch: 1370, Loss: 0.037559837102890015, Acc: 0.9627320170402527\n",
      "Epoch: 1371, Loss: 0.03860647603869438, Acc: 0.9636319875717163\n",
      "Epoch: 1372, Loss: 0.038066890090703964, Acc: 0.963575005531311\n",
      "Epoch: 1373, Loss: 0.03938005492091179, Acc: 0.9631869792938232\n",
      "Epoch: 1374, Loss: 0.035919349640607834, Acc: 0.9624999761581421\n",
      "Epoch: 1375, Loss: 0.03641989827156067, Acc: 0.962868869304657\n",
      "Epoch: 1376, Loss: 0.036069031804800034, Acc: 0.9627397060394287\n",
      "Epoch: 1377, Loss: 0.037498075515031815, Acc: 0.9625775814056396\n",
      "Epoch: 1378, Loss: 0.04115619510412216, Acc: 0.963481068611145\n",
      "Epoch: 1379, Loss: 0.036613088101148605, Acc: 0.9628298878669739\n",
      "Epoch: 1380, Loss: 0.041466787457466125, Acc: 0.9630656242370605\n",
      "Epoch: 1381, Loss: 0.03706808388233185, Acc: 0.9627279043197632\n",
      "Epoch: 1382, Loss: 0.03713103383779526, Acc: 0.9627702236175537\n",
      "Epoch: 1383, Loss: 0.03627592697739601, Acc: 0.9628560543060303\n",
      "Epoch: 1384, Loss: 0.03624042868614197, Acc: 0.9629513025283813\n",
      "Epoch: 1385, Loss: 0.036516088992357254, Acc: 0.9631978273391724\n",
      "Epoch: 1386, Loss: 0.0402764230966568, Acc: 0.9627445936203003\n",
      "Epoch: 1387, Loss: 0.03740200400352478, Acc: 0.9627537727355957\n",
      "Epoch: 1388, Loss: 0.036689549684524536, Acc: 0.9628551602363586\n",
      "Epoch: 1389, Loss: 0.04169215261936188, Acc: 0.963117241859436\n",
      "Epoch: 1390, Loss: 0.03682700917124748, Acc: 0.9632183313369751\n",
      "Epoch: 1391, Loss: 0.037260059267282486, Acc: 0.9629663228988647\n",
      "Epoch: 1392, Loss: 0.03686505928635597, Acc: 0.9634031057357788\n",
      "Epoch: 1393, Loss: 0.035888027399778366, Acc: 0.9628668427467346\n",
      "Epoch: 1394, Loss: 0.03693387657403946, Acc: 0.9629004597663879\n",
      "Epoch: 1395, Loss: 0.04024164006114006, Acc: 0.9632294774055481\n",
      "Epoch: 1396, Loss: 0.03793963044881821, Acc: 0.9632022380828857\n",
      "Epoch: 1397, Loss: 0.03726957365870476, Acc: 0.9635176658630371\n",
      "Epoch: 1398, Loss: 0.03705174848437309, Acc: 0.9632829427719116\n",
      "Epoch: 1399, Loss: 0.04211168736219406, Acc: 0.9634372591972351\n",
      "Epoch: 1400, Loss: 0.03888737037777901, Acc: 0.9628380537033081\n",
      "Epoch: 1401, Loss: 0.04269278049468994, Acc: 0.9638130068778992\n",
      "Epoch: 1402, Loss: 0.0352429561316967, Acc: 0.9628559947013855\n",
      "Epoch: 1403, Loss: 0.04021338373422623, Acc: 0.9631214737892151\n",
      "Epoch: 1404, Loss: 0.0351557619869709, Acc: 0.9628065228462219\n",
      "Epoch: 1405, Loss: 0.038169313222169876, Acc: 0.9629733562469482\n",
      "Epoch: 1406, Loss: 0.036933448165655136, Acc: 0.9629011750221252\n",
      "Epoch: 1407, Loss: 0.03783238306641579, Acc: 0.9633506536483765\n",
      "Epoch: 1408, Loss: 0.039441559463739395, Acc: 0.9631627202033997\n",
      "Epoch: 1409, Loss: 0.03637833893299103, Acc: 0.9632858633995056\n",
      "Epoch: 1410, Loss: 0.0396309569478035, Acc: 0.9631631374359131\n",
      "Epoch: 1411, Loss: 0.03566298633813858, Acc: 0.962995171546936\n",
      "Epoch: 1412, Loss: 0.03962159901857376, Acc: 0.9632879495620728\n",
      "Epoch: 1413, Loss: 0.037018921226263046, Acc: 0.963158905506134\n",
      "Epoch: 1414, Loss: 0.03655263036489487, Acc: 0.9630077481269836\n",
      "Epoch: 1415, Loss: 0.035956911742687225, Acc: 0.9633596539497375\n",
      "Epoch: 1416, Loss: 0.03592957928776741, Acc: 0.9633626341819763\n",
      "Epoch: 1417, Loss: 0.03643976151943207, Acc: 0.9631364941596985\n",
      "Epoch: 1418, Loss: 0.03807578980922699, Acc: 0.9635640382766724\n",
      "Epoch: 1419, Loss: 0.03702061250805855, Acc: 0.9632105231285095\n",
      "Epoch: 1420, Loss: 0.036098796874284744, Acc: 0.9631443023681641\n",
      "Epoch: 1421, Loss: 0.03513215854763985, Acc: 0.9632394313812256\n",
      "Epoch: 1422, Loss: 0.03704901784658432, Acc: 0.9631308913230896\n",
      "Epoch: 1423, Loss: 0.037011899054050446, Acc: 0.9632557034492493\n",
      "Epoch: 1424, Loss: 0.03976874798536301, Acc: 0.9633850455284119\n",
      "Epoch: 1425, Loss: 0.04036012664437294, Acc: 0.9640758633613586\n",
      "Epoch: 1426, Loss: 0.03583240881562233, Acc: 0.9631114602088928\n",
      "Epoch: 1427, Loss: 0.035755496472120285, Acc: 0.9634097218513489\n",
      "Epoch: 1428, Loss: 0.03602306544780731, Acc: 0.9633743762969971\n",
      "Epoch: 1429, Loss: 0.039941444993019104, Acc: 0.9635328650474548\n",
      "Epoch: 1430, Loss: 0.03720485419034958, Acc: 0.9632968306541443\n",
      "Epoch: 1431, Loss: 0.03584941476583481, Acc: 0.963333249092102\n",
      "Epoch: 1432, Loss: 0.04083728790283203, Acc: 0.9636577367782593\n",
      "Epoch: 1433, Loss: 0.037265848368406296, Acc: 0.9632293581962585\n",
      "Epoch: 1434, Loss: 0.041756413877010345, Acc: 0.9633282423019409\n",
      "Epoch: 1435, Loss: 0.040896736085414886, Acc: 0.9638493657112122\n",
      "Epoch: 1436, Loss: 0.03627851605415344, Acc: 0.9635765552520752\n",
      "Epoch: 1437, Loss: 0.04007191210985184, Acc: 0.9641521573066711\n",
      "Epoch: 1438, Loss: 0.04018557071685791, Acc: 0.9635492563247681\n",
      "Epoch: 1439, Loss: 0.03898013383150101, Acc: 0.9636371731758118\n",
      "Epoch: 1440, Loss: 0.036380644887685776, Acc: 0.9633939266204834\n",
      "Epoch: 1441, Loss: 0.03496374562382698, Acc: 0.9633253812789917\n",
      "Epoch: 1442, Loss: 0.03772883117198944, Acc: 0.9636748433113098\n",
      "Epoch: 1443, Loss: 0.03618178889155388, Acc: 0.963517427444458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1444, Loss: 0.03735978528857231, Acc: 0.9637331366539001\n",
      "Epoch: 1445, Loss: 0.03816636651754379, Acc: 0.9636300206184387\n",
      "Epoch: 1446, Loss: 0.03492366895079613, Acc: 0.9636985063552856\n",
      "Epoch: 1447, Loss: 0.036744728684425354, Acc: 0.9646150469779968\n",
      "Epoch: 1448, Loss: 0.036650151014328, Acc: 0.9636473655700684\n",
      "Epoch: 1449, Loss: 0.03665651008486748, Acc: 0.9639632701873779\n",
      "Epoch: 1450, Loss: 0.03586127609014511, Acc: 0.9636449217796326\n",
      "Epoch: 1451, Loss: 0.03910357505083084, Acc: 0.9636911153793335\n",
      "Epoch: 1452, Loss: 0.04151838272809982, Acc: 0.9637733697891235\n",
      "Epoch: 1453, Loss: 0.040453605353832245, Acc: 0.9635223150253296\n",
      "Epoch: 1454, Loss: 0.03688650205731392, Acc: 0.964036762714386\n",
      "Epoch: 1455, Loss: 0.037442728877067566, Acc: 0.9642236232757568\n",
      "Epoch: 1456, Loss: 0.03678596764802933, Acc: 0.9642439484596252\n",
      "Epoch: 1457, Loss: 0.03915431350469589, Acc: 0.9639459848403931\n",
      "Epoch: 1458, Loss: 0.036934979259967804, Acc: 0.9639638066291809\n",
      "Epoch: 1459, Loss: 0.03676239401102066, Acc: 0.9639932513237\n",
      "Epoch: 1460, Loss: 0.03629763424396515, Acc: 0.9638570547103882\n",
      "Epoch: 1461, Loss: 0.03700161725282669, Acc: 0.9637138247489929\n",
      "Epoch: 1462, Loss: 0.040127746760845184, Acc: 0.9640859365463257\n",
      "Epoch: 1463, Loss: 0.038631193339824677, Acc: 0.9639652371406555\n",
      "Epoch: 1464, Loss: 0.036095183342695236, Acc: 0.9642759561538696\n",
      "Epoch: 1465, Loss: 0.03486953303217888, Acc: 0.9637238383293152\n",
      "Epoch: 1466, Loss: 0.035967618227005005, Acc: 0.9643487334251404\n",
      "Epoch: 1467, Loss: 0.03511715307831764, Acc: 0.9637579917907715\n",
      "Epoch: 1468, Loss: 0.03696547821164131, Acc: 0.9639224410057068\n",
      "Epoch: 1469, Loss: 0.0366361066699028, Acc: 0.9640522599220276\n",
      "Epoch: 1470, Loss: 0.03552824631333351, Acc: 0.9638440012931824\n",
      "Epoch: 1471, Loss: 0.039382241666316986, Acc: 0.9645086526870728\n",
      "Epoch: 1472, Loss: 0.03554198145866394, Acc: 0.9641208052635193\n",
      "Epoch: 1473, Loss: 0.0403546504676342, Acc: 0.964215099811554\n",
      "Epoch: 1474, Loss: 0.0348723903298378, Acc: 0.9639437198638916\n",
      "Epoch: 1475, Loss: 0.035824503749608994, Acc: 0.9638747572898865\n",
      "Epoch: 1476, Loss: 0.03910115361213684, Acc: 0.9642846584320068\n",
      "Epoch: 1477, Loss: 0.03710663691163063, Acc: 0.9639748930931091\n",
      "Epoch: 1478, Loss: 0.03469947353005409, Acc: 0.9638282060623169\n",
      "Epoch: 1479, Loss: 0.03642602637410164, Acc: 0.9640359282493591\n",
      "Epoch: 1480, Loss: 0.036812346428632736, Acc: 0.96413254737854\n",
      "Epoch: 1481, Loss: 0.03551476448774338, Acc: 0.9640811681747437\n",
      "Epoch: 1482, Loss: 0.039130497723817825, Acc: 0.9647253751754761\n",
      "Epoch: 1483, Loss: 0.03472136706113815, Acc: 0.9642560482025146\n",
      "Epoch: 1484, Loss: 0.03665058687329292, Acc: 0.9638909101486206\n",
      "Epoch: 1485, Loss: 0.035576432943344116, Acc: 0.9644319415092468\n",
      "Epoch: 1486, Loss: 0.040179185569286346, Acc: 0.9644229412078857\n",
      "Epoch: 1487, Loss: 0.03544969484210014, Acc: 0.9641960859298706\n",
      "Epoch: 1488, Loss: 0.03883162513375282, Acc: 0.9640420079231262\n",
      "Epoch: 1489, Loss: 0.034978900104761124, Acc: 0.9639012217521667\n",
      "Epoch: 1490, Loss: 0.03526762127876282, Acc: 0.9643548130989075\n",
      "Epoch: 1491, Loss: 0.035966381430625916, Acc: 0.9641976952552795\n",
      "Epoch: 1492, Loss: 0.03513879328966141, Acc: 0.9642277359962463\n",
      "Epoch: 1493, Loss: 0.03571941331028938, Acc: 0.9642627239227295\n",
      "Epoch: 1494, Loss: 0.035595908761024475, Acc: 0.9643920063972473\n",
      "Epoch: 1495, Loss: 0.03497077524662018, Acc: 0.964525580406189\n",
      "Epoch: 1496, Loss: 0.034175265580415726, Acc: 0.9641436338424683\n",
      "Epoch: 1497, Loss: 0.034936510026454926, Acc: 0.9640745520591736\n",
      "Epoch: 1498, Loss: 0.038066565990448, Acc: 0.9644522666931152\n",
      "Epoch: 1499, Loss: 0.03947576880455017, Acc: 0.9641110897064209\n",
      "Epoch: 1500, Loss: 0.03493985906243324, Acc: 0.9642029404640198\n",
      "Epoch: 1501, Loss: 0.03630229830741882, Acc: 0.9641681909561157\n",
      "Epoch: 1502, Loss: 0.036599040031433105, Acc: 0.9641746282577515\n",
      "Epoch: 1503, Loss: 0.03829081356525421, Acc: 0.9645257592201233\n",
      "Epoch: 1504, Loss: 0.03625917434692383, Acc: 0.9642894268035889\n",
      "Epoch: 1505, Loss: 0.035875629633665085, Acc: 0.9645983576774597\n",
      "Epoch: 1506, Loss: 0.03487130254507065, Acc: 0.9643251299858093\n",
      "Epoch: 1507, Loss: 0.03493344411253929, Acc: 0.9645209312438965\n",
      "Epoch: 1508, Loss: 0.035338014364242554, Acc: 0.9644020199775696\n",
      "Epoch: 1509, Loss: 0.03428978472948074, Acc: 0.9641927480697632\n",
      "Epoch: 1510, Loss: 0.03505007177591324, Acc: 0.9645110964775085\n",
      "Epoch: 1511, Loss: 0.035133812576532364, Acc: 0.9647520184516907\n",
      "Epoch: 1512, Loss: 0.034407977014780045, Acc: 0.9644509553909302\n",
      "Epoch: 1513, Loss: 0.03615884482860565, Acc: 0.9647294878959656\n",
      "Epoch: 1514, Loss: 0.03999163582921028, Acc: 0.9646096229553223\n",
      "Epoch: 1515, Loss: 0.03981339931488037, Acc: 0.9646181464195251\n",
      "Epoch: 1516, Loss: 0.03469017520546913, Acc: 0.9644461870193481\n",
      "Epoch: 1517, Loss: 0.03438320383429527, Acc: 0.9643098711967468\n",
      "Epoch: 1518, Loss: 0.03672570735216141, Acc: 0.9646146893501282\n",
      "Epoch: 1519, Loss: 0.034934550523757935, Acc: 0.964698076248169\n",
      "Epoch: 1520, Loss: 0.03686997666954994, Acc: 0.965088427066803\n",
      "Epoch: 1521, Loss: 0.03453084081411362, Acc: 0.9644612669944763\n",
      "Epoch: 1522, Loss: 0.03501616045832634, Acc: 0.964583694934845\n",
      "Epoch: 1523, Loss: 0.03423105552792549, Acc: 0.9648715853691101\n",
      "Epoch: 1524, Loss: 0.03513544797897339, Acc: 0.9643775224685669\n",
      "Epoch: 1525, Loss: 0.03793783113360405, Acc: 0.9646619558334351\n",
      "Epoch: 1526, Loss: 0.038996372371912, Acc: 0.9643701314926147\n",
      "Epoch: 1527, Loss: 0.03471177816390991, Acc: 0.9648597240447998\n",
      "Epoch: 1528, Loss: 0.035724516957998276, Acc: 0.9645310640335083\n",
      "Epoch: 1529, Loss: 0.034123484045267105, Acc: 0.964732825756073\n",
      "Epoch: 1530, Loss: 0.03463310748338699, Acc: 0.9648491740226746\n",
      "Epoch: 1531, Loss: 0.0388953760266304, Acc: 0.9645397067070007\n",
      "Epoch: 1532, Loss: 0.033595021814107895, Acc: 0.9644847512245178\n",
      "Epoch: 1533, Loss: 0.0347052626311779, Acc: 0.9646437764167786\n",
      "Epoch: 1534, Loss: 0.03780345618724823, Acc: 0.9649671912193298\n",
      "Epoch: 1535, Loss: 0.038705285638570786, Acc: 0.9648876786231995\n",
      "Epoch: 1536, Loss: 0.03567753732204437, Acc: 0.9651472568511963\n",
      "Epoch: 1537, Loss: 0.0354740284383297, Acc: 0.9644892811775208\n",
      "Epoch: 1538, Loss: 0.03789115324616432, Acc: 0.9647030830383301\n",
      "Epoch: 1539, Loss: 0.03626374155282974, Acc: 0.9646783471107483\n",
      "Epoch: 1540, Loss: 0.03474044427275658, Acc: 0.964842677116394\n",
      "Epoch: 1541, Loss: 0.03985859081149101, Acc: 0.9647209048271179\n",
      "Epoch: 1542, Loss: 0.03471939265727997, Acc: 0.9650605320930481\n",
      "Epoch: 1543, Loss: 0.03840956091880798, Acc: 0.9648510813713074\n",
      "Epoch: 1544, Loss: 0.034961238503456116, Acc: 0.964907705783844\n",
      "Epoch: 1545, Loss: 0.03835074603557587, Acc: 0.964885413646698\n",
      "Epoch: 1546, Loss: 0.03995756804943085, Acc: 0.964632511138916\n",
      "Epoch: 1547, Loss: 0.035637468099594116, Acc: 0.9651198983192444\n",
      "Epoch: 1548, Loss: 0.03918376564979553, Acc: 0.9651066660881042\n",
      "Epoch: 1549, Loss: 0.03404771909117699, Acc: 0.9647103548049927\n",
      "Epoch: 1550, Loss: 0.03602006658911705, Acc: 0.9650766253471375\n",
      "Epoch: 1551, Loss: 0.03338039293885231, Acc: 0.9646605849266052\n",
      "Epoch: 1552, Loss: 0.03544710576534271, Acc: 0.9650849103927612\n",
      "Epoch: 1553, Loss: 0.03509174659848213, Acc: 0.964855432510376\n",
      "Epoch: 1554, Loss: 0.03442041575908661, Acc: 0.9650239944458008\n",
      "Epoch: 1555, Loss: 0.03462495282292366, Acc: 0.9648030996322632\n",
      "Epoch: 1556, Loss: 0.03556917980313301, Acc: 0.9649207592010498\n",
      "Epoch: 1557, Loss: 0.038834188133478165, Acc: 0.96470707654953\n",
      "Epoch: 1558, Loss: 0.033268123865127563, Acc: 0.9647021293640137\n",
      "Epoch: 1559, Loss: 0.035114530473947525, Acc: 0.9652160406112671\n",
      "Epoch: 1560, Loss: 0.034220337867736816, Acc: 0.9648522138595581\n",
      "Epoch: 1561, Loss: 0.035437654703855515, Acc: 0.965249240398407\n",
      "Epoch: 1562, Loss: 0.03374839574098587, Acc: 0.9650114178657532\n",
      "Epoch: 1563, Loss: 0.03450455516576767, Acc: 0.9647910594940186\n",
      "Epoch: 1564, Loss: 0.035610999912023544, Acc: 0.9652230739593506\n",
      "Epoch: 1565, Loss: 0.03496203571557999, Acc: 0.9647864699363708\n",
      "Epoch: 1566, Loss: 0.0341038815677166, Acc: 0.9649785757064819\n",
      "Epoch: 1567, Loss: 0.03466765582561493, Acc: 0.9651362895965576\n",
      "Epoch: 1568, Loss: 0.03649939224123955, Acc: 0.9652242660522461\n",
      "Epoch: 1569, Loss: 0.033337824046611786, Acc: 0.9649428129196167\n",
      "Epoch: 1570, Loss: 0.034462280571460724, Acc: 0.9650585651397705\n",
      "Epoch: 1571, Loss: 0.03567584231495857, Acc: 0.9651527404785156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1572, Loss: 0.039484910666942596, Acc: 0.9654375910758972\n",
      "Epoch: 1573, Loss: 0.03471151739358902, Acc: 0.9651487469673157\n",
      "Epoch: 1574, Loss: 0.03531291335821152, Acc: 0.9657282829284668\n",
      "Epoch: 1575, Loss: 0.03593268245458603, Acc: 0.9652243256568909\n",
      "Epoch: 1576, Loss: 0.039546940475702286, Acc: 0.965268611907959\n",
      "Epoch: 1577, Loss: 0.03571462631225586, Acc: 0.9657254815101624\n",
      "Epoch: 1578, Loss: 0.037520430982112885, Acc: 0.9653319120407104\n",
      "Epoch: 1579, Loss: 0.0358227863907814, Acc: 0.9654011130332947\n",
      "Epoch: 1580, Loss: 0.03412361070513725, Acc: 0.9651638269424438\n",
      "Epoch: 1581, Loss: 0.03506889194250107, Acc: 0.9653465747833252\n",
      "Epoch: 1582, Loss: 0.03483499586582184, Acc: 0.9651838541030884\n",
      "Epoch: 1583, Loss: 0.03329433873295784, Acc: 0.9653268456459045\n",
      "Epoch: 1584, Loss: 0.03737204149365425, Acc: 0.9655446410179138\n",
      "Epoch: 1585, Loss: 0.03730859234929085, Acc: 0.9654902219772339\n",
      "Epoch: 1586, Loss: 0.03414763882756233, Acc: 0.965399444103241\n",
      "Epoch: 1587, Loss: 0.034535739570856094, Acc: 0.9655743837356567\n",
      "Epoch: 1588, Loss: 0.03497492894530296, Acc: 0.9657698273658752\n",
      "Epoch: 1589, Loss: 0.03616427630186081, Acc: 0.9656220078468323\n",
      "Epoch: 1590, Loss: 0.03419990465044975, Acc: 0.9654602408409119\n",
      "Epoch: 1591, Loss: 0.03758034482598305, Acc: 0.965302050113678\n",
      "Epoch: 1592, Loss: 0.03538786619901657, Acc: 0.9658988118171692\n",
      "Epoch: 1593, Loss: 0.035674307495355606, Acc: 0.9657291769981384\n",
      "Epoch: 1594, Loss: 0.034328483045101166, Acc: 0.9657778143882751\n",
      "Epoch: 1595, Loss: 0.034401386976242065, Acc: 0.965349555015564\n",
      "Epoch: 1596, Loss: 0.03735431656241417, Acc: 0.9657081961631775\n",
      "Epoch: 1597, Loss: 0.03748764842748642, Acc: 0.965390145778656\n",
      "Epoch: 1598, Loss: 0.03533874452114105, Acc: 0.9656038284301758\n",
      "Epoch: 1599, Loss: 0.03744044154882431, Acc: 0.9655695557594299\n",
      "Epoch: 1600, Loss: 0.03402688354253769, Acc: 0.9654165506362915\n",
      "Epoch: 1601, Loss: 0.03581136837601662, Acc: 0.9658177495002747\n",
      "Epoch: 1602, Loss: 0.03332744538784027, Acc: 0.9653774499893188\n",
      "Epoch: 1603, Loss: 0.033291496336460114, Acc: 0.9653722643852234\n",
      "Epoch: 1604, Loss: 0.039548784494400024, Acc: 0.9655895829200745\n",
      "Epoch: 1605, Loss: 0.03568408638238907, Acc: 0.9653746485710144\n",
      "Epoch: 1606, Loss: 0.034433044493198395, Acc: 0.9657463431358337\n",
      "Epoch: 1607, Loss: 0.03398885950446129, Acc: 0.9656343460083008\n",
      "Epoch: 1608, Loss: 0.040157515555620193, Acc: 0.9662485718727112\n",
      "Epoch: 1609, Loss: 0.034954942762851715, Acc: 0.9657256603240967\n",
      "Epoch: 1610, Loss: 0.03359212726354599, Acc: 0.9654044508934021\n",
      "Epoch: 1611, Loss: 0.03361421823501587, Acc: 0.9657484292984009\n",
      "Epoch: 1612, Loss: 0.034191928803920746, Acc: 0.9655497074127197\n",
      "Epoch: 1613, Loss: 0.03357187286019325, Acc: 0.9656977653503418\n",
      "Epoch: 1614, Loss: 0.033572420477867126, Acc: 0.9654825925827026\n",
      "Epoch: 1615, Loss: 0.03745377063751221, Acc: 0.9654775261878967\n",
      "Epoch: 1616, Loss: 0.03364706039428711, Acc: 0.9655008316040039\n",
      "Epoch: 1617, Loss: 0.03331392630934715, Acc: 0.9657214879989624\n",
      "Epoch: 1618, Loss: 0.03730172663927078, Acc: 0.96579509973526\n",
      "Epoch: 1619, Loss: 0.03352734446525574, Acc: 0.9654974937438965\n",
      "Epoch: 1620, Loss: 0.033096566796302795, Acc: 0.9657185673713684\n",
      "Epoch: 1621, Loss: 0.03675390034914017, Acc: 0.9659423828125\n",
      "Epoch: 1622, Loss: 0.03309568762779236, Acc: 0.9659632444381714\n",
      "Epoch: 1623, Loss: 0.035161104053258896, Acc: 0.9657601714134216\n",
      "Epoch: 1624, Loss: 0.033896975219249725, Acc: 0.9657185673713684\n",
      "Epoch: 1625, Loss: 0.03395635634660721, Acc: 0.9659193158149719\n",
      "Epoch: 1626, Loss: 0.037318743765354156, Acc: 0.9655978083610535\n",
      "Epoch: 1627, Loss: 0.03371488302946091, Acc: 0.965549111366272\n",
      "Epoch: 1628, Loss: 0.03393446281552315, Acc: 0.965781033039093\n",
      "Epoch: 1629, Loss: 0.03427053615450859, Acc: 0.9659322500228882\n",
      "Epoch: 1630, Loss: 0.037247978150844574, Acc: 0.9661491513252258\n",
      "Epoch: 1631, Loss: 0.03466951847076416, Acc: 0.9660278558731079\n",
      "Epoch: 1632, Loss: 0.03442545235157013, Acc: 0.9657309651374817\n",
      "Epoch: 1633, Loss: 0.037272486835718155, Acc: 0.9657278060913086\n",
      "Epoch: 1634, Loss: 0.035736266523599625, Acc: 0.9659552574157715\n",
      "Epoch: 1635, Loss: 0.03496432304382324, Acc: 0.9660792350769043\n",
      "Epoch: 1636, Loss: 0.03420335426926613, Acc: 0.9659463167190552\n",
      "Epoch: 1637, Loss: 0.034424275159835815, Acc: 0.9660068154335022\n",
      "Epoch: 1638, Loss: 0.032716523855924606, Acc: 0.9657074809074402\n",
      "Epoch: 1639, Loss: 0.03747595101594925, Acc: 0.9660323858261108\n",
      "Epoch: 1640, Loss: 0.03521338105201721, Acc: 0.9663780927658081\n",
      "Epoch: 1641, Loss: 0.03739957883954048, Acc: 0.9664162397384644\n",
      "Epoch: 1642, Loss: 0.036967143416404724, Acc: 0.9660677909851074\n",
      "Epoch: 1643, Loss: 0.032452914863824844, Acc: 0.9656664133071899\n",
      "Epoch: 1644, Loss: 0.03366534784436226, Acc: 0.9663880467414856\n",
      "Epoch: 1645, Loss: 0.03405259922146797, Acc: 0.9660221934318542\n",
      "Epoch: 1646, Loss: 0.03302128240466118, Acc: 0.9658677577972412\n",
      "Epoch: 1647, Loss: 0.03365137428045273, Acc: 0.9659954309463501\n",
      "Epoch: 1648, Loss: 0.033231522887945175, Acc: 0.9659006595611572\n",
      "Epoch: 1649, Loss: 0.03654048591852188, Acc: 0.9662700891494751\n",
      "Epoch: 1650, Loss: 0.032975513488054276, Acc: 0.9658517241477966\n",
      "Epoch: 1651, Loss: 0.04010827839374542, Acc: 0.9671229124069214\n",
      "Epoch: 1652, Loss: 0.03383758291602135, Acc: 0.9658369421958923\n",
      "Epoch: 1653, Loss: 0.033313117921352386, Acc: 0.9658568501472473\n",
      "Epoch: 1654, Loss: 0.03635209798812866, Acc: 0.966254472732544\n",
      "Epoch: 1655, Loss: 0.03395553305745125, Acc: 0.9663139581680298\n",
      "Epoch: 1656, Loss: 0.03385423123836517, Acc: 0.9662780165672302\n",
      "Epoch: 1657, Loss: 0.03334319591522217, Acc: 0.9662312269210815\n",
      "Epoch: 1658, Loss: 0.0346698984503746, Acc: 0.9665499329566956\n",
      "Epoch: 1659, Loss: 0.03833513706922531, Acc: 0.9658949375152588\n",
      "Epoch: 1660, Loss: 0.03844562917947769, Acc: 0.9661956429481506\n",
      "Epoch: 1661, Loss: 0.03508610650897026, Acc: 0.9663019180297852\n",
      "Epoch: 1662, Loss: 0.03376990184187889, Acc: 0.9661362767219543\n",
      "Epoch: 1663, Loss: 0.03231127932667732, Acc: 0.9659674167633057\n",
      "Epoch: 1664, Loss: 0.03863015025854111, Acc: 0.9667049050331116\n",
      "Epoch: 1665, Loss: 0.03344494104385376, Acc: 0.9664773941040039\n",
      "Epoch: 1666, Loss: 0.032684359699487686, Acc: 0.9660804271697998\n",
      "Epoch: 1667, Loss: 0.03401126712560654, Acc: 0.9661960601806641\n",
      "Epoch: 1668, Loss: 0.033575233072042465, Acc: 0.9660073518753052\n",
      "Epoch: 1669, Loss: 0.03432203829288483, Acc: 0.9663025736808777\n",
      "Epoch: 1670, Loss: 0.03380356729030609, Acc: 0.9663918018341064\n",
      "Epoch: 1671, Loss: 0.033806607127189636, Acc: 0.9660810232162476\n",
      "Epoch: 1672, Loss: 0.03393068537116051, Acc: 0.9663931131362915\n",
      "Epoch: 1673, Loss: 0.0337936095893383, Acc: 0.9663098454475403\n",
      "Epoch: 1674, Loss: 0.036601707339286804, Acc: 0.9662563800811768\n",
      "Epoch: 1675, Loss: 0.0347113162279129, Acc: 0.9662718772888184\n",
      "Epoch: 1676, Loss: 0.03267827257514, Acc: 0.9660373330116272\n",
      "Epoch: 1677, Loss: 0.03374779596924782, Acc: 0.9662383198738098\n",
      "Epoch: 1678, Loss: 0.033956363797187805, Acc: 0.9664706587791443\n",
      "Epoch: 1679, Loss: 0.0368565171957016, Acc: 0.9663371443748474\n",
      "Epoch: 1680, Loss: 0.03374641016125679, Acc: 0.9663720726966858\n",
      "Epoch: 1681, Loss: 0.03383254632353783, Acc: 0.966440737247467\n",
      "Epoch: 1682, Loss: 0.03253810480237007, Acc: 0.9662529230117798\n",
      "Epoch: 1683, Loss: 0.03311804682016373, Acc: 0.9663360714912415\n",
      "Epoch: 1684, Loss: 0.033986806869506836, Acc: 0.9663164019584656\n",
      "Epoch: 1685, Loss: 0.038982294499874115, Acc: 0.9673723578453064\n",
      "Epoch: 1686, Loss: 0.03534100204706192, Acc: 0.9671023488044739\n",
      "Epoch: 1687, Loss: 0.03337591141462326, Acc: 0.966425895690918\n",
      "Epoch: 1688, Loss: 0.03490254655480385, Acc: 0.9663898944854736\n",
      "Epoch: 1689, Loss: 0.03365422785282135, Acc: 0.9663790464401245\n",
      "Epoch: 1690, Loss: 0.033115215599536896, Acc: 0.9662628769874573\n",
      "Epoch: 1691, Loss: 0.032725006341934204, Acc: 0.9665670990943909\n",
      "Epoch: 1692, Loss: 0.03274156153202057, Acc: 0.9663905501365662\n",
      "Epoch: 1693, Loss: 0.03421967104077339, Acc: 0.9663925766944885\n",
      "Epoch: 1694, Loss: 0.036457717418670654, Acc: 0.966404139995575\n",
      "Epoch: 1695, Loss: 0.033827077597379684, Acc: 0.9667867422103882\n",
      "Epoch: 1696, Loss: 0.033123888075351715, Acc: 0.9666279554367065\n",
      "Epoch: 1697, Loss: 0.032797813415527344, Acc: 0.966295599937439\n",
      "Epoch: 1698, Loss: 0.03325073793530464, Acc: 0.9667510986328125\n",
      "Epoch: 1699, Loss: 0.0338372141122818, Acc: 0.9664955139160156\n",
      "Epoch: 1700, Loss: 0.03734631463885307, Acc: 0.9663188457489014\n",
      "Epoch: 1701, Loss: 0.03503044694662094, Acc: 0.9666662812232971\n",
      "Epoch: 1702, Loss: 0.03446493297815323, Acc: 0.9664061069488525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1703, Loss: 0.03317217901349068, Acc: 0.9663910269737244\n",
      "Epoch: 1704, Loss: 0.03422693908214569, Acc: 0.9670119285583496\n",
      "Epoch: 1705, Loss: 0.03316250815987587, Acc: 0.9666394591331482\n",
      "Epoch: 1706, Loss: 0.033407408744096756, Acc: 0.9666862487792969\n",
      "Epoch: 1707, Loss: 0.03412821888923645, Acc: 0.9668906927108765\n",
      "Epoch: 1708, Loss: 0.03801189363002777, Acc: 0.9669438600540161\n",
      "Epoch: 1709, Loss: 0.03247680142521858, Acc: 0.9665040373802185\n",
      "Epoch: 1710, Loss: 0.0346984826028347, Acc: 0.9666257500648499\n",
      "Epoch: 1711, Loss: 0.03355979174375534, Acc: 0.9665165543556213\n",
      "Epoch: 1712, Loss: 0.034456755965948105, Acc: 0.967126727104187\n",
      "Epoch: 1713, Loss: 0.0326516218483448, Acc: 0.9667568206787109\n",
      "Epoch: 1714, Loss: 0.03267785906791687, Acc: 0.9666419625282288\n",
      "Epoch: 1715, Loss: 0.03448053076863289, Acc: 0.9667038321495056\n",
      "Epoch: 1716, Loss: 0.03321051597595215, Acc: 0.9665220379829407\n",
      "Epoch: 1717, Loss: 0.03231799602508545, Acc: 0.9667004346847534\n",
      "Epoch: 1718, Loss: 0.034596819430589676, Acc: 0.9667324423789978\n",
      "Epoch: 1719, Loss: 0.03228094428777695, Acc: 0.9665998220443726\n",
      "Epoch: 1720, Loss: 0.03442622721195221, Acc: 0.9667560458183289\n",
      "Epoch: 1721, Loss: 0.03251223266124725, Acc: 0.9665736556053162\n",
      "Epoch: 1722, Loss: 0.03670120611786842, Acc: 0.9666192531585693\n",
      "Epoch: 1723, Loss: 0.03216613456606865, Acc: 0.9667999744415283\n",
      "Epoch: 1724, Loss: 0.03411490097641945, Acc: 0.9666057229042053\n",
      "Epoch: 1725, Loss: 0.03417884185910225, Acc: 0.9669003486633301\n",
      "Epoch: 1726, Loss: 0.031649164855480194, Acc: 0.9665539264678955\n",
      "Epoch: 1727, Loss: 0.03222740441560745, Acc: 0.9667438864707947\n",
      "Epoch: 1728, Loss: 0.03548712283372879, Acc: 0.9666575789451599\n",
      "Epoch: 1729, Loss: 0.036807313561439514, Acc: 0.9666585922241211\n",
      "Epoch: 1730, Loss: 0.034438300877809525, Acc: 0.96745365858078\n",
      "Epoch: 1731, Loss: 0.03279649466276169, Acc: 0.9667626619338989\n",
      "Epoch: 1732, Loss: 0.035938091576099396, Acc: 0.9668589234352112\n",
      "Epoch: 1733, Loss: 0.033900272101163864, Acc: 0.9670810103416443\n",
      "Epoch: 1734, Loss: 0.03264911472797394, Acc: 0.9668205976486206\n",
      "Epoch: 1735, Loss: 0.03601865470409393, Acc: 0.9669815301895142\n",
      "Epoch: 1736, Loss: 0.036553382873535156, Acc: 0.9666939973831177\n",
      "Epoch: 1737, Loss: 0.03388288617134094, Acc: 0.9667755961418152\n",
      "Epoch: 1738, Loss: 0.03242720291018486, Acc: 0.9667469263076782\n",
      "Epoch: 1739, Loss: 0.03717505931854248, Acc: 0.9674402475357056\n",
      "Epoch: 1740, Loss: 0.033840253949165344, Acc: 0.9668777585029602\n",
      "Epoch: 1741, Loss: 0.03287447988986969, Acc: 0.9668896198272705\n",
      "Epoch: 1742, Loss: 0.03670012950897217, Acc: 0.9675999879837036\n",
      "Epoch: 1743, Loss: 0.035655926913022995, Acc: 0.9672213196754456\n",
      "Epoch: 1744, Loss: 0.03257288038730621, Acc: 0.9668104648590088\n",
      "Epoch: 1745, Loss: 0.03183504939079285, Acc: 0.9668532013893127\n",
      "Epoch: 1746, Loss: 0.033175669610500336, Acc: 0.966736376285553\n",
      "Epoch: 1747, Loss: 0.033398792147636414, Acc: 0.9669451713562012\n",
      "Epoch: 1748, Loss: 0.03283874690532684, Acc: 0.9672755002975464\n",
      "Epoch: 1749, Loss: 0.036905884742736816, Acc: 0.9668505191802979\n",
      "Epoch: 1750, Loss: 0.032723914831876755, Acc: 0.9668723940849304\n",
      "Epoch: 1751, Loss: 0.037068095058202744, Acc: 0.9673543572425842\n",
      "Epoch: 1752, Loss: 0.03277309238910675, Acc: 0.9673215746879578\n",
      "Epoch: 1753, Loss: 0.03303852677345276, Acc: 0.9677420854568481\n",
      "Epoch: 1754, Loss: 0.034394312649965286, Acc: 0.9673153162002563\n",
      "Epoch: 1755, Loss: 0.03385777026414871, Acc: 0.9670113325119019\n",
      "Epoch: 1756, Loss: 0.032429009675979614, Acc: 0.9670965671539307\n",
      "Epoch: 1757, Loss: 0.03171319514513016, Acc: 0.9671992063522339\n",
      "Epoch: 1758, Loss: 0.03308764472603798, Acc: 0.9674978852272034\n",
      "Epoch: 1759, Loss: 0.0320013128221035, Acc: 0.9669294357299805\n",
      "Epoch: 1760, Loss: 0.03752085939049721, Acc: 0.9679079651832581\n",
      "Epoch: 1761, Loss: 0.035744160413742065, Acc: 0.9670634269714355\n",
      "Epoch: 1762, Loss: 0.03590173274278641, Acc: 0.9673773646354675\n",
      "Epoch: 1763, Loss: 0.035759374499320984, Acc: 0.9672843813896179\n",
      "Epoch: 1764, Loss: 0.03271898254752159, Acc: 0.9672244191169739\n",
      "Epoch: 1765, Loss: 0.033163413405418396, Acc: 0.9671463370323181\n",
      "Epoch: 1766, Loss: 0.031891919672489166, Acc: 0.9671425223350525\n",
      "Epoch: 1767, Loss: 0.03333716467022896, Acc: 0.9679762125015259\n",
      "Epoch: 1768, Loss: 0.03249206766486168, Acc: 0.9673221111297607\n",
      "Epoch: 1769, Loss: 0.03186601400375366, Acc: 0.9670434594154358\n",
      "Epoch: 1770, Loss: 0.038042739033699036, Acc: 0.9681351184844971\n",
      "Epoch: 1771, Loss: 0.03281895071268082, Acc: 0.9673346281051636\n",
      "Epoch: 1772, Loss: 0.03151606768369675, Acc: 0.9670568704605103\n",
      "Epoch: 1773, Loss: 0.03409106656908989, Acc: 0.9671186804771423\n",
      "Epoch: 1774, Loss: 0.03352891281247139, Acc: 0.9672737121582031\n",
      "Epoch: 1775, Loss: 0.03337559849023819, Acc: 0.9671513438224792\n",
      "Epoch: 1776, Loss: 0.03299683332443237, Acc: 0.9670811295509338\n",
      "Epoch: 1777, Loss: 0.033269818872213364, Acc: 0.9674281477928162\n",
      "Epoch: 1778, Loss: 0.031168121844530106, Acc: 0.9669898748397827\n",
      "Epoch: 1779, Loss: 0.03276364877820015, Acc: 0.9673770666122437\n",
      "Epoch: 1780, Loss: 0.03350810334086418, Acc: 0.9675611853599548\n",
      "Epoch: 1781, Loss: 0.03262946009635925, Acc: 0.9675669074058533\n",
      "Epoch: 1782, Loss: 0.031704939901828766, Acc: 0.9672275185585022\n",
      "Epoch: 1783, Loss: 0.03583763539791107, Acc: 0.9675708413124084\n",
      "Epoch: 1784, Loss: 0.03207072243094444, Acc: 0.9671754837036133\n",
      "Epoch: 1785, Loss: 0.03168405592441559, Acc: 0.9671874642372131\n",
      "Epoch: 1786, Loss: 0.03258741647005081, Acc: 0.9671661853790283\n",
      "Epoch: 1787, Loss: 0.03182869777083397, Acc: 0.9672664999961853\n",
      "Epoch: 1788, Loss: 0.032923128455877304, Acc: 0.9674641489982605\n",
      "Epoch: 1789, Loss: 0.03224237635731697, Acc: 0.9673959016799927\n",
      "Epoch: 1790, Loss: 0.032400742173194885, Acc: 0.9678938388824463\n",
      "Epoch: 1791, Loss: 0.03541230782866478, Acc: 0.9676681756973267\n",
      "Epoch: 1792, Loss: 0.03370780497789383, Acc: 0.9678587317466736\n",
      "Epoch: 1793, Loss: 0.03401953727006912, Acc: 0.9673469066619873\n",
      "Epoch: 1794, Loss: 0.03306608274579048, Acc: 0.967433750629425\n",
      "Epoch: 1795, Loss: 0.03247781842947006, Acc: 0.9674139022827148\n",
      "Epoch: 1796, Loss: 0.03663521260023117, Acc: 0.9675556421279907\n",
      "Epoch: 1797, Loss: 0.03172341361641884, Acc: 0.9675121307373047\n",
      "Epoch: 1798, Loss: 0.0353730246424675, Acc: 0.967522144317627\n",
      "Epoch: 1799, Loss: 0.03230847418308258, Acc: 0.9672783017158508\n",
      "Epoch: 1800, Loss: 0.03203801438212395, Acc: 0.9673134088516235\n",
      "Epoch: 1801, Loss: 0.03585272282361984, Acc: 0.9675849080085754\n",
      "Epoch: 1802, Loss: 0.03313879668712616, Acc: 0.9676241278648376\n",
      "Epoch: 1803, Loss: 0.031025690957903862, Acc: 0.9672929644584656\n",
      "Epoch: 1804, Loss: 0.03767010569572449, Acc: 0.9678701758384705\n",
      "Epoch: 1805, Loss: 0.03155679628252983, Acc: 0.9674641489982605\n",
      "Epoch: 1806, Loss: 0.0367993526160717, Acc: 0.9683553576469421\n",
      "Epoch: 1807, Loss: 0.03560726344585419, Acc: 0.9673919081687927\n",
      "Epoch: 1808, Loss: 0.03126409649848938, Acc: 0.9672388434410095\n",
      "Epoch: 1809, Loss: 0.03587302565574646, Acc: 0.9677695035934448\n",
      "Epoch: 1810, Loss: 0.03159124031662941, Acc: 0.9675252437591553\n",
      "Epoch: 1811, Loss: 0.03223991021513939, Acc: 0.9674381017684937\n",
      "Epoch: 1812, Loss: 0.03580637276172638, Acc: 0.9674620032310486\n",
      "Epoch: 1813, Loss: 0.035130925476551056, Acc: 0.9675570130348206\n",
      "Epoch: 1814, Loss: 0.03236057609319687, Acc: 0.9679734110832214\n",
      "Epoch: 1815, Loss: 0.03304717689752579, Acc: 0.9673948884010315\n",
      "Epoch: 1816, Loss: 0.03335461765527725, Acc: 0.9678515791893005\n",
      "Epoch: 1817, Loss: 0.032261427491903305, Acc: 0.9675889611244202\n",
      "Epoch: 1818, Loss: 0.033783551305532455, Acc: 0.9675236940383911\n",
      "Epoch: 1819, Loss: 0.03304802253842354, Acc: 0.9677955508232117\n",
      "Epoch: 1820, Loss: 0.03348293900489807, Acc: 0.9678080677986145\n",
      "Epoch: 1821, Loss: 0.031414538621902466, Acc: 0.967727541923523\n",
      "Epoch: 1822, Loss: 0.03266599029302597, Acc: 0.9673957824707031\n",
      "Epoch: 1823, Loss: 0.035207390785217285, Acc: 0.9676820039749146\n",
      "Epoch: 1824, Loss: 0.03196774050593376, Acc: 0.9677470922470093\n",
      "Epoch: 1825, Loss: 0.03159443289041519, Acc: 0.9673931002616882\n",
      "Epoch: 1826, Loss: 0.033667489886283875, Acc: 0.9678029417991638\n",
      "Epoch: 1827, Loss: 0.03392381593585014, Acc: 0.9677785038948059\n",
      "Epoch: 1828, Loss: 0.03279668465256691, Acc: 0.9680065512657166\n",
      "Epoch: 1829, Loss: 0.036054208874702454, Acc: 0.9675031900405884\n",
      "Epoch: 1830, Loss: 0.033014439046382904, Acc: 0.9675261974334717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1831, Loss: 0.03341915085911751, Acc: 0.9677146077156067\n",
      "Epoch: 1832, Loss: 0.031732723116874695, Acc: 0.9679718017578125\n",
      "Epoch: 1833, Loss: 0.03231719508767128, Acc: 0.9677526354789734\n",
      "Epoch: 1834, Loss: 0.03340775519609451, Acc: 0.9678950309753418\n",
      "Epoch: 1835, Loss: 0.03347373381257057, Acc: 0.9676305651664734\n",
      "Epoch: 1836, Loss: 0.03332929313182831, Acc: 0.9676722884178162\n",
      "Epoch: 1837, Loss: 0.0337052047252655, Acc: 0.9681603908538818\n",
      "Epoch: 1838, Loss: 0.031278785318136215, Acc: 0.9676445722579956\n",
      "Epoch: 1839, Loss: 0.03271989896893501, Acc: 0.9681060910224915\n",
      "Epoch: 1840, Loss: 0.03150347247719765, Acc: 0.968353807926178\n",
      "Epoch: 1841, Loss: 0.03239112347364426, Acc: 0.9677224159240723\n",
      "Epoch: 1842, Loss: 0.032270241528749466, Acc: 0.9675731658935547\n",
      "Epoch: 1843, Loss: 0.03865683078765869, Acc: 0.968416690826416\n",
      "Epoch: 1844, Loss: 0.03276745602488518, Acc: 0.9678612351417542\n",
      "Epoch: 1845, Loss: 0.03380706533789635, Acc: 0.9680790901184082\n",
      "Epoch: 1846, Loss: 0.03266780823469162, Acc: 0.9680030345916748\n",
      "Epoch: 1847, Loss: 0.03206144645810127, Acc: 0.9679633975028992\n",
      "Epoch: 1848, Loss: 0.03236488252878189, Acc: 0.9679686427116394\n",
      "Epoch: 1849, Loss: 0.03472299128770828, Acc: 0.9682848453521729\n",
      "Epoch: 1850, Loss: 0.031933028250932693, Acc: 0.9678457379341125\n",
      "Epoch: 1851, Loss: 0.03206820413470268, Acc: 0.9680253863334656\n",
      "Epoch: 1852, Loss: 0.03311147540807724, Acc: 0.9678236246109009\n",
      "Epoch: 1853, Loss: 0.032515205442905426, Acc: 0.9683326482772827\n",
      "Epoch: 1854, Loss: 0.03191959112882614, Acc: 0.9678651094436646\n",
      "Epoch: 1855, Loss: 0.031265776604413986, Acc: 0.9679635763168335\n",
      "Epoch: 1856, Loss: 0.030982039868831635, Acc: 0.9676957726478577\n",
      "Epoch: 1857, Loss: 0.030602846294641495, Acc: 0.9677013754844666\n",
      "Epoch: 1858, Loss: 0.031528539955616, Acc: 0.9682550430297852\n",
      "Epoch: 1859, Loss: 0.031323473900556564, Acc: 0.9677769541740417\n",
      "Epoch: 1860, Loss: 0.030725570395588875, Acc: 0.9677921533584595\n",
      "Epoch: 1861, Loss: 0.03142757713794708, Acc: 0.9680266976356506\n",
      "Epoch: 1862, Loss: 0.03527587652206421, Acc: 0.967808723449707\n",
      "Epoch: 1863, Loss: 0.03182392567396164, Acc: 0.9682244062423706\n",
      "Epoch: 1864, Loss: 0.030448144301772118, Acc: 0.9677242636680603\n",
      "Epoch: 1865, Loss: 0.03113333322107792, Acc: 0.9680211544036865\n",
      "Epoch: 1866, Loss: 0.03209635242819786, Acc: 0.9679731130599976\n",
      "Epoch: 1867, Loss: 0.03532171994447708, Acc: 0.9678205251693726\n",
      "Epoch: 1868, Loss: 0.03323747590184212, Acc: 0.9678874611854553\n",
      "Epoch: 1869, Loss: 0.03154199570417404, Acc: 0.9679970145225525\n",
      "Epoch: 1870, Loss: 0.03302503004670143, Acc: 0.9684074521064758\n",
      "Epoch: 1871, Loss: 0.03246096894145012, Acc: 0.9680882096290588\n",
      "Epoch: 1872, Loss: 0.034433260560035706, Acc: 0.9682865142822266\n",
      "Epoch: 1873, Loss: 0.03241688385605812, Acc: 0.9681593179702759\n",
      "Epoch: 1874, Loss: 0.033540207892656326, Acc: 0.9684041738510132\n",
      "Epoch: 1875, Loss: 0.03238406032323837, Acc: 0.9678879976272583\n",
      "Epoch: 1876, Loss: 0.03286086022853851, Acc: 0.9682389497756958\n",
      "Epoch: 1877, Loss: 0.032433342188596725, Acc: 0.9682201743125916\n",
      "Epoch: 1878, Loss: 0.031731609255075455, Acc: 0.9682742953300476\n",
      "Epoch: 1879, Loss: 0.03057943657040596, Acc: 0.9680676460266113\n",
      "Epoch: 1880, Loss: 0.03509654104709625, Acc: 0.9686498641967773\n",
      "Epoch: 1881, Loss: 0.03250521421432495, Acc: 0.9682806730270386\n",
      "Epoch: 1882, Loss: 0.03276889771223068, Acc: 0.9683743119239807\n",
      "Epoch: 1883, Loss: 0.03170381486415863, Acc: 0.9680436849594116\n",
      "Epoch: 1884, Loss: 0.035944633185863495, Acc: 0.9686450958251953\n",
      "Epoch: 1885, Loss: 0.030750997364521027, Acc: 0.9684673547744751\n",
      "Epoch: 1886, Loss: 0.0349530428647995, Acc: 0.9680506587028503\n",
      "Epoch: 1887, Loss: 0.031198492273688316, Acc: 0.96834796667099\n",
      "Epoch: 1888, Loss: 0.03503153473138809, Acc: 0.9681179523468018\n",
      "Epoch: 1889, Loss: 0.03497429937124252, Acc: 0.9681404829025269\n",
      "Epoch: 1890, Loss: 0.03153249993920326, Acc: 0.9680708646774292\n",
      "Epoch: 1891, Loss: 0.03306698426604271, Acc: 0.9681663513183594\n",
      "Epoch: 1892, Loss: 0.03491957113146782, Acc: 0.9685472846031189\n",
      "Epoch: 1893, Loss: 0.03241662681102753, Acc: 0.9687672257423401\n",
      "Epoch: 1894, Loss: 0.03183303028345108, Acc: 0.9681066274642944\n",
      "Epoch: 1895, Loss: 0.0347423329949379, Acc: 0.9682883620262146\n",
      "Epoch: 1896, Loss: 0.03102230653166771, Acc: 0.9683817028999329\n",
      "Epoch: 1897, Loss: 0.03341713175177574, Acc: 0.9686216711997986\n",
      "Epoch: 1898, Loss: 0.032271161675453186, Acc: 0.9688894152641296\n",
      "Epoch: 1899, Loss: 0.036092475056648254, Acc: 0.9685609340667725\n",
      "Epoch: 1900, Loss: 0.03346066176891327, Acc: 0.9683383703231812\n",
      "Epoch: 1901, Loss: 0.03241138905286789, Acc: 0.9685045480728149\n",
      "Epoch: 1902, Loss: 0.031597547233104706, Acc: 0.9683372378349304\n",
      "Epoch: 1903, Loss: 0.03255733475089073, Acc: 0.9681983590126038\n",
      "Epoch: 1904, Loss: 0.03251219540834427, Acc: 0.9684274196624756\n",
      "Epoch: 1905, Loss: 0.030560459941625595, Acc: 0.9681956768035889\n",
      "Epoch: 1906, Loss: 0.030487462878227234, Acc: 0.9682004451751709\n",
      "Epoch: 1907, Loss: 0.030586401000618935, Acc: 0.9684828519821167\n",
      "Epoch: 1908, Loss: 0.03137500584125519, Acc: 0.9682796001434326\n",
      "Epoch: 1909, Loss: 0.031943053007125854, Acc: 0.9684741497039795\n",
      "Epoch: 1910, Loss: 0.031795766204595566, Acc: 0.9681873321533203\n",
      "Epoch: 1911, Loss: 0.031821899116039276, Acc: 0.9684960842132568\n",
      "Epoch: 1912, Loss: 0.0315944142639637, Acc: 0.9689164757728577\n",
      "Epoch: 1913, Loss: 0.030926164239645004, Acc: 0.9686102271080017\n",
      "Epoch: 1914, Loss: 0.030816100537776947, Acc: 0.9685567021369934\n",
      "Epoch: 1915, Loss: 0.0344359427690506, Acc: 0.9685102701187134\n",
      "Epoch: 1916, Loss: 0.03195247799158096, Acc: 0.9686866998672485\n",
      "Epoch: 1917, Loss: 0.032072655856609344, Acc: 0.9684008359909058\n",
      "Epoch: 1918, Loss: 0.034708231687545776, Acc: 0.9690683484077454\n",
      "Epoch: 1919, Loss: 0.031856052577495575, Acc: 0.9684422612190247\n",
      "Epoch: 1920, Loss: 0.031047681346535683, Acc: 0.9683200120925903\n",
      "Epoch: 1921, Loss: 0.031481023877859116, Acc: 0.968390703201294\n",
      "Epoch: 1922, Loss: 0.031181612983345985, Acc: 0.9687384963035583\n",
      "Epoch: 1923, Loss: 0.030932029709219933, Acc: 0.9684059619903564\n",
      "Epoch: 1924, Loss: 0.03162691369652748, Acc: 0.9683747887611389\n",
      "Epoch: 1925, Loss: 0.030701972544193268, Acc: 0.9685026407241821\n",
      "Epoch: 1926, Loss: 0.034333236515522, Acc: 0.9685064554214478\n",
      "Epoch: 1927, Loss: 0.030836204066872597, Acc: 0.968695878982544\n",
      "Epoch: 1928, Loss: 0.035713762044906616, Acc: 0.9686617255210876\n",
      "Epoch: 1929, Loss: 0.031050831079483032, Acc: 0.9683513641357422\n",
      "Epoch: 1930, Loss: 0.030912863090634346, Acc: 0.9685307741165161\n",
      "Epoch: 1931, Loss: 0.030262604355812073, Acc: 0.9684422016143799\n",
      "Epoch: 1932, Loss: 0.032297853380441666, Acc: 0.9691573977470398\n",
      "Epoch: 1933, Loss: 0.031613826751708984, Acc: 0.9690744280815125\n",
      "Epoch: 1934, Loss: 0.0367339625954628, Acc: 0.9684702754020691\n",
      "Epoch: 1935, Loss: 0.031252432614564896, Acc: 0.9683955311775208\n",
      "Epoch: 1936, Loss: 0.031788669526576996, Acc: 0.9685834050178528\n",
      "Epoch: 1937, Loss: 0.030791787430644035, Acc: 0.9683776497840881\n",
      "Epoch: 1938, Loss: 0.03437129035592079, Acc: 0.9687492847442627\n",
      "Epoch: 1939, Loss: 0.031726814806461334, Acc: 0.9686992764472961\n",
      "Epoch: 1940, Loss: 0.030755382031202316, Acc: 0.9685453176498413\n",
      "Epoch: 1941, Loss: 0.03160564973950386, Acc: 0.968685507774353\n",
      "Epoch: 1942, Loss: 0.03092513419687748, Acc: 0.9685174226760864\n",
      "Epoch: 1943, Loss: 0.030237585306167603, Acc: 0.9684732556343079\n",
      "Epoch: 1944, Loss: 0.03245313838124275, Acc: 0.9686573147773743\n",
      "Epoch: 1945, Loss: 0.03459116816520691, Acc: 0.9688565731048584\n",
      "Epoch: 1946, Loss: 0.03145745396614075, Acc: 0.9685831665992737\n",
      "Epoch: 1947, Loss: 0.03379708528518677, Acc: 0.9687407612800598\n",
      "Epoch: 1948, Loss: 0.030612604692578316, Acc: 0.968564510345459\n",
      "Epoch: 1949, Loss: 0.03219054639339447, Acc: 0.9689233303070068\n",
      "Epoch: 1950, Loss: 0.03446079045534134, Acc: 0.9688674211502075\n",
      "Epoch: 1951, Loss: 0.03088241070508957, Acc: 0.9689980149269104\n",
      "Epoch: 1952, Loss: 0.0325632244348526, Acc: 0.9686176180839539\n",
      "Epoch: 1953, Loss: 0.03047667071223259, Acc: 0.9685577154159546\n",
      "Epoch: 1954, Loss: 0.03444390743970871, Acc: 0.969144344329834\n",
      "Epoch: 1955, Loss: 0.03045421652495861, Acc: 0.9686091542243958\n",
      "Epoch: 1956, Loss: 0.033110611140728, Acc: 0.9688416123390198\n",
      "Epoch: 1957, Loss: 0.030708562582731247, Acc: 0.9685879945755005\n",
      "Epoch: 1958, Loss: 0.03467525541782379, Acc: 0.9689739942550659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1959, Loss: 0.030932016670703888, Acc: 0.9689936637878418\n",
      "Epoch: 1960, Loss: 0.035077016800642014, Acc: 0.9693477153778076\n",
      "Epoch: 1961, Loss: 0.035555072128772736, Acc: 0.9688363671302795\n",
      "Epoch: 1962, Loss: 0.033868201076984406, Acc: 0.968859076499939\n",
      "Epoch: 1963, Loss: 0.030621321871876717, Acc: 0.9686176180839539\n",
      "Epoch: 1964, Loss: 0.030299991369247437, Acc: 0.9686391949653625\n",
      "Epoch: 1965, Loss: 0.030914926901459694, Acc: 0.9688608050346375\n",
      "Epoch: 1966, Loss: 0.02986340969800949, Acc: 0.9687703251838684\n",
      "Epoch: 1967, Loss: 0.0313265360891819, Acc: 0.9692903757095337\n",
      "Epoch: 1968, Loss: 0.033797331154346466, Acc: 0.9689666032791138\n",
      "Epoch: 1969, Loss: 0.03099244087934494, Acc: 0.9691258072853088\n",
      "Epoch: 1970, Loss: 0.03167008236050606, Acc: 0.9694086909294128\n",
      "Epoch: 1971, Loss: 0.030881544575095177, Acc: 0.9691154956817627\n",
      "Epoch: 1972, Loss: 0.03032376617193222, Acc: 0.9687437415122986\n",
      "Epoch: 1973, Loss: 0.03190787509083748, Acc: 0.9689048528671265\n",
      "Epoch: 1974, Loss: 0.033679571002721786, Acc: 0.9690714478492737\n",
      "Epoch: 1975, Loss: 0.03155703470110893, Acc: 0.9688388705253601\n",
      "Epoch: 1976, Loss: 0.03186313807964325, Acc: 0.9688087105751038\n",
      "Epoch: 1977, Loss: 0.03174661099910736, Acc: 0.9689199328422546\n",
      "Epoch: 1978, Loss: 0.03201887756586075, Acc: 0.9688761830329895\n",
      "Epoch: 1979, Loss: 0.03040046989917755, Acc: 0.9687556028366089\n",
      "Epoch: 1980, Loss: 0.03523768484592438, Acc: 0.9693564176559448\n",
      "Epoch: 1981, Loss: 0.03262074291706085, Acc: 0.9692120552062988\n",
      "Epoch: 1982, Loss: 0.030140450224280357, Acc: 0.9687302112579346\n",
      "Epoch: 1983, Loss: 0.03053012676537037, Acc: 0.9694552421569824\n",
      "Epoch: 1984, Loss: 0.035240065306425095, Acc: 0.968833327293396\n",
      "Epoch: 1985, Loss: 0.030019588768482208, Acc: 0.9690684676170349\n",
      "Epoch: 1986, Loss: 0.03413711488246918, Acc: 0.9688539505004883\n",
      "Epoch: 1987, Loss: 0.030924320220947266, Acc: 0.9688502550125122\n",
      "Epoch: 1988, Loss: 0.03173701837658882, Acc: 0.9691824913024902\n",
      "Epoch: 1989, Loss: 0.030946753919124603, Acc: 0.9691214561462402\n",
      "Epoch: 1990, Loss: 0.03400981426239014, Acc: 0.9690415263175964\n",
      "Epoch: 1991, Loss: 0.03040272928774357, Acc: 0.9689126014709473\n",
      "Epoch: 1992, Loss: 0.03216519206762314, Acc: 0.968927800655365\n",
      "Epoch: 1993, Loss: 0.034565363079309464, Acc: 0.9688395857810974\n",
      "Epoch: 1994, Loss: 0.03184863179922104, Acc: 0.9695038795471191\n",
      "Epoch: 1995, Loss: 0.03114956244826317, Acc: 0.9688517451286316\n",
      "Epoch: 1996, Loss: 0.030620047822594643, Acc: 0.969319224357605\n",
      "Epoch: 1997, Loss: 0.036120958626270294, Acc: 0.969316303730011\n",
      "Epoch: 1998, Loss: 0.029711371287703514, Acc: 0.9689332246780396\n",
      "Epoch: 1999, Loss: 0.031506795436143875, Acc: 0.9693000316619873\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "epochs = 2000\n",
    "history_acc, history_loss = train_model(epochs, model, packed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
    "fig.suptitle('Training Metrics')\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(history_loss)\n",
    "\n",
    "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "axes[1].plot(history_acc)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
